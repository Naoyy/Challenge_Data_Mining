{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from src.preprocessing import display_missing_values\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "sns.set_theme(style=\"ticks\", palette=\"pastel\")\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonctions import Dataset,group_fuel_types\n",
    "from src.preprocessing import display_missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=Dataset(\"data/train.csv\")\n",
    "data_train=train.load_data()\n",
    "test=Dataset(\"data/test.csv\")\n",
    "data_test=test.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonctions import Preprocessor,TrainPreprocessor,TestPreprocessor\n",
    "train_preprocessor=TrainPreprocessor(data_train)\n",
    "test_preprocessor=TestPreprocessor(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preprocessor.fill_fuel_consumption()\n",
    "train_preprocessor.fill_electric_range()\n",
    "train_preprocessor.fill_engine_capacity()\n",
    "train_preprocessor.fill_electric_consumption()\n",
    "train_preprocessor.fill_category_type()\n",
    "train_preprocessor.fill_wheel_base()\n",
    "train_preprocessor.fill_At_1()\n",
    "train_preprocessor.fill_At_2()\n",
    "train_preprocessor.fill_mass()\n",
    "train_preprocessor.fill_engine_power()\n",
    "train_preprocessor.last_step()\n",
    "\n",
    "test_preprocessor.fill_fuel_consumption()\n",
    "test_preprocessor.fill_electric_range()\n",
    "test_preprocessor.fill_engine_capacity()\n",
    "test_preprocessor.fill_electric_consumption()\n",
    "test_preprocessor.fill_category_type()\n",
    "test_preprocessor.fill_wheel_base()\n",
    "test_preprocessor.fill_At_1()\n",
    "test_preprocessor.fill_At_2()\n",
    "test_preprocessor.fill_mass()\n",
    "test_preprocessor.fill_engine_power()\n",
    "test_preprocessor.last_step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test de non-linéarité des variables quantitatives "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_continues=[\"Fuel consumption \",\"Electric range (km)\",\"ec (cm3)\",\"z (Wh/km)\",\"W (mm)\"]\n",
    "others=[col for col in data_train.columns if col not in variables_continues]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "# Application de la transformation aux variables continues\n",
    "variables_continues_transformees = poly.fit_transform(data_train[variables_continues])\n",
    "\n",
    "data_train_transforme = pd.DataFrame(variables_continues_transformees, columns=poly.get_feature_names_out(variables_continues))\n",
    "data_train= pd.concat([data_train[others],data_train_transforme], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_continues_transformees = poly.fit_transform(data_test[variables_continues])\n",
    "\n",
    "data_test_transforme = pd.DataFrame(variables_continues_transformees, columns=poly.get_feature_names_out(variables_continues))\n",
    "data_test= pd.concat([data_test[others.remove('Ewltp (g/km)')],data_train_transforme], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = train_preprocessor.encode_that_var(\"Ct\")\n",
    "data_train = train_preprocessor.encode_that_var(\"Cr\")\n",
    "data_test = test_preprocessor.encode_that_var(\"Ct\")\n",
    "data_test = test_preprocessor.encode_that_var(\"Cr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conforme(df):\n",
    "    df['conforme'] = df['Tan'].isna()\n",
    "    df['conforme'] = df['conforme'].apply(lambda x: 1 if x==False else 0)\n",
    "    df.drop(columns='Tan', inplace=True)\n",
    "    pass\n",
    "def compute_surface(obs):\n",
    "    max_largeur= max(obs['At1 (mm)'], obs['At2 (mm)'])\n",
    "    return obs['W (mm)']*obs['At1 (mm)'] if max_largeur == obs['At1 (mm)'] else obs['W (mm)'] * obs['At2 (mm)']\n",
    "\n",
    "def create_surface(df):\n",
    "    df['surface']= df.apply(compute_surface, axis=1)\n",
    "    pass\n",
    "\n",
    "def group_fuel_types(category: str):\n",
    "    if category in ['PETROL/ELECTRIC', 'DIESEL/ELECTRIC']:\n",
    "        return \"HYBRID\"\n",
    "    elif category in ['NG-BIOMETHANE', 'HYDROGEN', 'NG','E85']:\n",
    "        return \"BIO-FUEL\"\n",
    "    elif category in ['PETROL','LPG'] :\n",
    "        return 'PETROL'\n",
    "    else:\n",
    "        return category\n",
    "def create_carburant(df):\n",
    "    df['carburant']= df['Ft'].apply(group_fuel_types)\n",
    "    df.drop(columns='Ft',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_conforme(data_train)\n",
    "create_surface(data_train)\n",
    "create_carburant(data_train)\n",
    "\n",
    "create_conforme(data_test)\n",
    "create_surface(data_test)\n",
    "create_carburant(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = train_preprocessor.encode_that_var(\"carburant\")\n",
    "data_test = test_preprocessor.encode_that_var(\"carburant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_this=['VFN', 'Mp', 'Mh', 'Man', 'T', 'Mk', 'Cn','Mt','W (mm)', 'At1 (mm)', 'At2 (mm)','Fm','Erwltp (g/km)']\n",
    "\n",
    "data_train.drop(columns=drop_this, inplace=True)\n",
    "data_test.drop(columns=drop_this, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.drop(columns='ID',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_vars=[\"Ewltp (g/km)\", \"m (kg)\",\"ec (cm3)\", \"ep (KW)\", \"z (Wh/km)\", \"Fuel consumption \",\"Electric range (km)\", \"surface\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data_train[list_vars].corr(method='spearman'),annot=True)\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.drop(columns='z (Wh/km)',inplace=True)\n",
    "data_test.drop(columns='z (Wh/km)',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV, ElasticNetCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data_train, test_size=0.33, random_state=42)\n",
    "\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X_train, y_train = train.drop(columns=[\"Ewltp (g/km)\"]), train[\"Ewltp (g/km)\"]\n",
    "X_test, y_test = test.drop(columns=[\"Ewltp (g/km)\"]), test[\"Ewltp (g/km)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = LassoCV(cv=5,)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Affichez les coefficients sélectionnés\n",
    "selected_features = X_train.columns[lasso.coef_ != 0]\n",
    "print(\"Variables sélectionnées par Adaptive LASSO :\")\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_net = ElasticNetCV(cv=5, random_state=42, l1_ratio=0.8)\n",
    "elastic_net.fit(X_train, y_train)\n",
    "\n",
    "# Affichez les coefficients sélectionnés\n",
    "selected_features = X_train.columns[elastic_net.coef_ != 0]\n",
    "print(\"Variables sélectionnées par Adaptive LASSO :\")\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynome=PolynomialFeatures(degree=3, interaction_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_polynome=polynome.fit_transform(X_train)\n",
    "X_test_polynome=polynome.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space={'max_depth': hp.quniform(\"max_depth\", 3, 18, 1),\n",
    "        'gamma': hp.uniform ('gamma', 1,9),\n",
    "        'reg_alpha' : hp.quniform('reg_alpha', 40,180,1),\n",
    "        'reg_lambda' : hp.uniform('reg_lambda', 0,1),\n",
    "        'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),\n",
    "        'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n",
    "        'n_estimators': 1000,\n",
    "        'seed': 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_bis={'max_depth': hp.quniform(\"max_depth\", 3, 18, 1),\n",
    "           'n_estimators': 1500,\n",
    "           'learning_rate':hp.uniform(\"learning_rate\",0.1,0.30)\n",
    "           \n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit\n",
    "def objective(space):\n",
    "    clf=xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                    n_estimators =space['n_estimators'], max_depth = int(space['max_depth']), gamma = space['gamma'],\n",
    "                    reg_alpha = int(space['reg_alpha']),min_child_weight=int(space['min_child_weight']),\n",
    "                    colsample_bytree=int(space['colsample_bytree']), eval_metric=\"mae\")\n",
    "    \n",
    "    evaluation = [( X_train, y_train), ( X_test, y_test)]\n",
    "    \n",
    "    clf.fit(X_train, y_train,\n",
    "            eval_set=evaluation,\n",
    "            verbose=False)\n",
    "    \n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    print (\"SCORE:\", mae)\n",
    "    return {'loss': mae, 'status': STATUS_OK }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit\n",
    "def objective_bis(space):\n",
    "    clf=xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                    n_estimators =space['n_estimators'], max_depth = int(space['max_depth']),max_iter=3000, eval_metric=\"mae\")\n",
    "    \n",
    "    evaluation = [( X_train, y_train), ( X_test, y_test)]\n",
    "    \n",
    "    clf.fit(X_train, y_train,\n",
    "            eval_set=evaluation,\n",
    "            verbose=False)\n",
    "    \n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    print (\"SCORE:\", mae)\n",
    "    return {'loss': mae, 'status': STATUS_OK }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = Trials()\n",
    "\n",
    "best_hyperparams = fmin(fn = objective_bis,\n",
    "                        space = space_bis,\n",
    "                        algo = tpe.suggest,\n",
    "                        max_evals = 100,\n",
    "                        trials = trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit\n",
    "def train_xgboost(n_estimators=1000 ,objective='reg:squarederror',max_depth=10):\n",
    "    xgboost_model=xgb.XGBRegressor(n_estimators=n_estimators ,objective=objective,max_depth=max_depth,subsample=0.75,eval_metric='mae')\n",
    "    xgboost_model.fit(X_train, y_train, )\n",
    "    return xgboost_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_model=train_xgboost()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae=evaluate_model(xgboost_model,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_model = xgb.XGBRegressor(n_estimators=10000, objective='reg:squarederror', random_state=0,max_depth=10)\n",
    "\n",
    "# Entraînez le modèle sur les données d'entraînement\n",
    "xgboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Prédisez sur les données de test\n",
    "y_pred = xgboost_model.predict(X_test)\n",
    "\n",
    "# Évaluez le modèle en utilisant la MAE (Mean Absolute Error)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error:\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit(nopython=True)\n",
    "def train_random_forest(n_estimators=1000,max_samples=10000 ,criterion='absolute_error',max_depth=10):\n",
    "    rf_model=RandomForestRegressor(n_estimators=n_estimators, max_samples=max_samples ,criterion=criterion,max_depth=max_depth)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    return rf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model=train_random_forest(n_estimators=1000, max_samples=10000,criterion='absolute_error',max_depth=10)\n",
    "\n",
    "mae=evaluate_model(rf_model,X_test,y_test)\n",
    "print(\"Mean Absolute Error:\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit\n",
    "def train_hist_gradient_boosting(loss=\"absolute_error\",max_iter=3000, learning_rate=0.22,max_depth=10):\n",
    "    hgbgb_model=HistGradientBoostingRegressor(loss=loss,max_iter=max_iter, learning_rate=learning_rate, max_depth=max_depth,interaction_cst=\"pairwise\")\n",
    "    hgbgb_model.fit(X_train,y_train)\n",
    "    return hgbgb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgbgb_model=train_hist_gradient_boosting()\n",
    "\n",
    "mae=evaluate_model(hgbgb_model,X_test,y_test)\n",
    "print(f\"Mean Absolute Error : {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgbgb_model.n_iter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créez un modèle de régression de base (par exemple, un arbre de régression)\n",
    "@numba.jit\n",
    "def train_adaboost_regressor(max_depth=7,n_estimators=1000, learning_rate=.2, random_state=42) -> AdaBoostRegressor:\n",
    "    base_regressor = DecisionTreeRegressor(max_depth=max_depth)\n",
    "\n",
    "    # Créez un modèle AdaBoostRegressor en utilisant le modèle de base\n",
    "    adaboost_regressor = AdaBoostRegressor(base_regressor, n_estimators=n_estimators, learning_rate=learning_rate, random_state=42)\n",
    "\n",
    "    # Entraînez le modèle AdaBoost sur l'ensemble d'entraînement\n",
    "    adaboost_regressor.fit(X_train, y_train)\n",
    "    return adaboost_regressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost_model=train_adaboost_regressor()\n",
    "\n",
    "mae=evaluate_model(adaboost_model,X_test,y_test)\n",
    "print(f\"Mean Absolute Error : {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_true_test=data_test.drop(columns=['ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[\"ID\"]=data_test[\"ID\"].apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[\"Ewltp (g/km)\"] = xgboost_model.predict(X_true_test)\n",
    "data_test[[\"ID\",\"Ewltp (g/km)\"]].to_csv(\"data/xgboost_results3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "hgboost_model=HistGradientBoostingRegressor(max_iter=1000,learning_rate=0.23,max_depth=7,loss=\"absolute_error\",random_state=42,l2_regularization=0.5, validation_fraction=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînez le modèle sur les données d'entraînement\n",
    "hgboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Prédisez sur les données de test\n",
    "y_pred = hgboost_model.predict(X_test)\n",
    "\n",
    "# Évaluez le modèle en utilisant la MAE (Mean Absolute Error)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error:\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtregressor_model=DecisionTreeRegressor(criterion='absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtregressor_model.fit(X_train, y_train)\n",
    "\n",
    "# Prédisez sur les données de test\n",
    "y_pred = dtregressor_model.predict(X_test)\n",
    "\n",
    "# Évaluez le modèle en utilisant la MAE (Mean Absolute Error)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error:\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Créez un ensemble de données LightGBM\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "\n",
    "# Définissez les paramètres du modèle\n",
    "params = {\n",
    "    \"objective\": \"regression\",  # Régression\n",
    "    \"metric\": \"mae\",  # Métrique d'erreur : Mean Absolute Error\n",
    "    \"boosting_type\": \"gbdt\",  # Type de boosting (Gradient Boosting Decision Tree)\n",
    "    \"num_leaves\": 31,  # Nombre maximum de feuilles dans un arbre\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"feature_fraction\": 0.9,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\": 10,\n",
    "    \"verbose\": -1\n",
    "}\n",
    "\n",
    "# Entraînez le modèle\n",
    "num_round = 1000  # Nombre d'itérations (vous pouvez ajuster selon vos besoins)\n",
    "bst = lgb.train(params, train_data, num_round)\n",
    "\n",
    "# Faites des prédictions sur l'ensemble de test\n",
    "y_pred = bst.predict(X_test, num_iteration=bst.best_iteration)\n",
    "\n",
    "# Évaluez le modèle\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Error (mae): {mae}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_mining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
