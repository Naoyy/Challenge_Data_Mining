{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import + Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonctions import Dataset\n",
    "from src.preprocessing import display_missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=Dataset(\"data/train.csv\")\n",
    "data_train=train.load_data()\n",
    "test=Dataset(\"data/test.csv\")\n",
    "data_test=test.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonctions import Preprocessor,TrainPreprocessor,TestPreprocessor\n",
    "train_preprocessor=TrainPreprocessor(data_train)\n",
    "test_preprocessor=TestPreprocessor(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_continues=[\"Fuel consumption \",\"Electric range (km)\",\"ec (cm3)\",\"z (Wh/km)\",\"W (mm)\"]\n",
    "\n",
    "for col in variables_continues:\n",
    "    train_preprocessor.outlier_detection(col)\n",
    "    test_preprocessor.outlier_detection(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preprocessor.fill_fuel_consumption()\n",
    "train_preprocessor.fill_electric_range()\n",
    "train_preprocessor.fill_engine_capacity()\n",
    "train_preprocessor.fill_electric_consumption()\n",
    "train_preprocessor.fill_category_type()\n",
    "train_preprocessor.fill_wheel_base()\n",
    "train_preprocessor.fill_At_1()\n",
    "train_preprocessor.fill_At_2()\n",
    "train_preprocessor.fill_mass()\n",
    "train_preprocessor.fill_engine_power()\n",
    "train_preprocessor.encode_country()\n",
    "train_preprocessor.encode_manufacture_pooling()\n",
    "train_preprocessor.last_step()\n",
    "\n",
    "test_preprocessor.fill_fuel_consumption()\n",
    "test_preprocessor.fill_electric_range()\n",
    "test_preprocessor.fill_engine_capacity()\n",
    "test_preprocessor.fill_electric_consumption()\n",
    "test_preprocessor.fill_category_type()\n",
    "test_preprocessor.fill_wheel_base()\n",
    "test_preprocessor.fill_At_1()\n",
    "test_preprocessor.fill_At_2()\n",
    "test_preprocessor.fill_mass()\n",
    "test_preprocessor.fill_engine_power()\n",
    "test_preprocessor.encode_country()\n",
    "test_preprocessor.encode_manufacture_pooling()\n",
    "test_preprocessor.last_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = train_preprocessor.encode_that_var(\"Ct\")\n",
    "data_train = train_preprocessor.encode_that_var(\"Cr\")\n",
    "data_test = test_preprocessor.encode_that_var(\"Ct\")\n",
    "data_test = test_preprocessor.encode_that_var(\"Cr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conforme(df):\n",
    "    df['conforme'] = df['Tan'].isna()\n",
    "    df['conforme'] = df['conforme'].apply(lambda x: 1 if x==False else 0)\n",
    "    df.drop(columns='Tan', inplace=True)\n",
    "    pass\n",
    "def compute_surface(obs):\n",
    "    max_largeur= max(obs['At1 (mm)'], obs['At2 (mm)'])\n",
    "    return obs['W (mm)']*obs['At1 (mm)'] if max_largeur == obs['At1 (mm)'] else obs['W (mm)'] * obs['At2 (mm)']\n",
    "\n",
    "def create_surface(df):\n",
    "    df['surface']= df.apply(compute_surface, axis=1)\n",
    "    pass\n",
    "\n",
    "def group_fuel_types(category: str):\n",
    "    if category in ['PETROL/ELECTRIC', 'DIESEL/ELECTRIC']:\n",
    "        return \"HYBRID\"\n",
    "    elif category in ['NG-BIOMETHANE', 'HYDROGEN', 'NG','E85']:\n",
    "        return \"BIO-FUEL\"\n",
    "    elif category in ['PETROL','LPG'] :\n",
    "        return 'PETROL'\n",
    "    else:\n",
    "        return category\n",
    "def create_carburant(df):\n",
    "    df['carburant']= df['Ft'].apply(group_fuel_types)\n",
    "    df.drop(columns='Ft',inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_conforme(data_train)\n",
    "create_surface(data_train)\n",
    "create_carburant(data_train)\n",
    "\n",
    "create_conforme(data_test)\n",
    "create_surface(data_test)\n",
    "create_carburant(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = train_preprocessor.encode_that_var(\"carburant\")\n",
    "data_test = test_preprocessor.encode_that_var(\"carburant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_this_cuz_no_use=['VFN', 'Mh', 'Man', 'T', 'Mk', 'Cn','Mt']\n",
    "drop_this_cuz_personnal_choice = ['W (mm)', 'At1 (mm)', 'At2 (mm)','Fm','Erwltp (g/km)']\n",
    "\n",
    "data_train.drop(columns=drop_this_cuz_no_use, inplace=True)\n",
    "data_test.drop(columns=drop_this_cuz_no_use, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.drop(columns='ID',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from src.preprocessing import display_missing_values\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "sns.set_theme(style=\"ticks\", palette=\"pastel\")\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest (numerical)\n",
    "No drop dupplicates, only keep numerical cols\n",
    "\n",
    "MAE: 3.2843"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "columns_to_keep = ['m (kg)', 'Ewltp (g/km)', 'W (mm)', 'At1 (mm)', 'At2 (mm)', 'ec (cm3)', 'ep (KW)', 'z (Wh/km)', 'Fuel consumption ', 'Electric range (km)']\n",
    "\n",
    "train, test = train_test_split(data_train[data_train.columns.intersection(columns_to_keep)], test_size=0.33, random_state=42)\n",
    "\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X_train, y_train = train.drop(columns=[\"Ewltp (g/km)\"]), train[\"Ewltp (g/km)\"]\n",
    "X_test, y_test = test.drop(columns=[\"Ewltp (g/km)\"]), test[\"Ewltp (g/km)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 3.2811834066371595\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestRegressor(random_state=42)\n",
    "\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "y_pred = random_forest.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[\"Ewltp (g/km)\"] = random_forest.predict(data_test[data_test.columns.intersection(columns_to_keep)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[[\"ID\",\"Ewltp (g/km)\"]].to_csv(\"data/reg_rf_numerical_no_drop_dups.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF numerical (new set of predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "columns_to_keep = ['m (kg)', 'Ewltp (g/km)', 'W (mm)', 'At1 (mm)', 'At2 (mm)', 'ec (cm3)', 'ep (KW)', 'z (Wh/km)', 'Fuel consumption ']\n",
    "\n",
    "train, test = train_test_split(data_train[data_train.columns.intersection(columns_to_keep)], test_size=0.33, random_state=42)\n",
    "\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X_train, y_train = train.drop(columns=[\"Ewltp (g/km)\"]), train[\"Ewltp (g/km)\"]\n",
    "X_test, y_test = test.drop(columns=[\"Ewltp (g/km)\"]), test[\"Ewltp (g/km)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 3.314147394462553\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "y_pred = random_forest.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[\"Ewltp (g/km)\"] = random_forest.predict(data_test[data_test.columns.intersection(columns_to_keep)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[[\"ID\",\"Ewltp (g/km)\"]].to_csv(\"data/reg_rf_num_dups_no_e_range.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF outliers numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "columns_to_keep = ['m (kg)', 'Ewltp (g/km)', 'W (mm)', 'At1 (mm)', 'At2 (mm)', 'ec (cm3)', 'ep (KW)', 'z (Wh/km)', 'Fuel consumption ', 'Electric range (km)']\n",
    "\n",
    "train, test = train_test_split(data_train[data_train.columns.intersection(columns_to_keep)], test_size=0.33, random_state=42)\n",
    "\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X_train, y_train = train.drop(columns=[\"Ewltp (g/km)\"]), train[\"Ewltp (g/km)\"]\n",
    "X_test, y_test = test.drop(columns=[\"Ewltp (g/km)\"]), test[\"Ewltp (g/km)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 3.2811834066371603\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "y_pred = random_forest.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[\"Ewltp (g/km)\"] = random_forest.predict(data_test[data_test.columns.intersection(columns_to_keep)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[[\"ID\",\"Ewltp (g/km)\"]].to_csv(\"data/reg_rf_outlier.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF outliers numerical + country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "columns_to_keep = ['m (kg)', 'Ewltp (g/km)', 'W (mm)', 'At1 (mm)',\n",
    "                    'At2 (mm)', 'ec (cm3)', 'ep (KW)', 'z (Wh/km)',\n",
    "                      'Fuel consumption ', 'Electric range (km)','Country']\n",
    "\n",
    "train, test = train_test_split(data_train[data_train.columns.intersection(columns_to_keep)], test_size=0.33, random_state=42)\n",
    "\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X_train, y_train = train.drop(columns=[\"Ewltp (g/km)\"]), train[\"Ewltp (g/km)\"]\n",
    "X_test, y_test = test.drop(columns=[\"Ewltp (g/km)\"]), test[\"Ewltp (g/km)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 3.185450881133795\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "y_pred = random_forest.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[\"Ewltp (g/km)\"] = random_forest.predict(data_test[data_test.columns.intersection(columns_to_keep)])\n",
    "data_test[[\"ID\",\"Ewltp (g/km)\"]].to_csv(\"data/reg_rf_country.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### little test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "columns_to_keep = ['m (kg)', 'Ewltp (g/km)', 'W (mm)', 'At1 (mm)',\n",
    "                    'At2 (mm)', 'ec (cm3)', 'ep (KW)', 'z (Wh/km)',\n",
    "                      'Fuel consumption ', 'Electric range (km)','Country']\n",
    "\n",
    "train, test = train_test_split(data_train[data_train.columns.intersection(columns_to_keep)], test_size=0.33, random_state=42)\n",
    "\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X_train, y_train = train.drop(columns=[\"Ewltp (g/km)\"]), train[\"Ewltp (g/km)\"]\n",
    "X_test, y_test = test.drop(columns=[\"Ewltp (g/km)\"]), test[\"Ewltp (g/km)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\yoanj\\Documents\\M2S1\\Projet_Mining\\model_testing_yoan.ipynb Cellule 35\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yoanj/Documents/M2S1/Projet_Mining/model_testing_yoan.ipynb#X62sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m random_forest \u001b[39m=\u001b[39m RandomForestRegressor(criterion\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mabsolute_error\u001b[39m\u001b[39m\"\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m, n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/yoanj/Documents/M2S1/Projet_Mining/model_testing_yoan.ipynb#X62sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m random_forest\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yoanj/Documents/M2S1/Projet_Mining/model_testing_yoan.ipynb#X62sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m y_pred \u001b[39m=\u001b[39m random_forest\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yoanj/Documents/M2S1/Projet_Mining/model_testing_yoan.ipynb#X62sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m mae \u001b[39m=\u001b[39m mean_absolute_error(y_test, y_pred)\n",
      "File \u001b[1;32mc:\\Users\\yoanj\\Documents\\M2S1\\Projet_Mining\\.venv_mining\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\yoanj\\Documents\\M2S1\\Projet_Mining\\.venv_mining\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:456\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    445\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[0;32m    446\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(append\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[0;32m    447\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    448\u001b[0m ]\n\u001b[0;32m    450\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 456\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[0;32m    457\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[0;32m    458\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    459\u001b[0m     prefer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreads\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    460\u001b[0m )(\n\u001b[0;32m    461\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    462\u001b[0m         t,\n\u001b[0;32m    463\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbootstrap,\n\u001b[0;32m    464\u001b[0m         X,\n\u001b[0;32m    465\u001b[0m         y,\n\u001b[0;32m    466\u001b[0m         sample_weight,\n\u001b[0;32m    467\u001b[0m         i,\n\u001b[0;32m    468\u001b[0m         \u001b[39mlen\u001b[39;49m(trees),\n\u001b[0;32m    469\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    470\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[0;32m    471\u001b[0m         n_samples_bootstrap\u001b[39m=\u001b[39;49mn_samples_bootstrap,\n\u001b[0;32m    472\u001b[0m     )\n\u001b[0;32m    473\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, t \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(trees)\n\u001b[0;32m    474\u001b[0m )\n\u001b[0;32m    476\u001b[0m \u001b[39m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Users\\yoanj\\Documents\\M2S1\\Projet_Mining\\.venv_mining\\lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\yoanj\\Documents\\M2S1\\Projet_Mining\\.venv_mining\\lib\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[39m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[39m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[39m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[39m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n",
      "File \u001b[1;32mc:\\Users\\yoanj\\Documents\\M2S1\\Projet_Mining\\.venv_mining\\lib\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[39m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[39m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[39m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yoanj\\Documents\\M2S1\\Projet_Mining\\.venv_mining\\lib\\site-packages\\joblib\\parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1702\u001b[0m \u001b[39m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m \u001b[39m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1704\u001b[0m \u001b[39mif\u001b[39;00m ((\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m) \u001b[39mor\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m     (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget_status(\n\u001b[0;32m   1706\u001b[0m         timeout\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout) \u001b[39m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1707\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n\u001b[0;32m   1708\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m   1710\u001b[0m \u001b[39m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1711\u001b[0m \u001b[39m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1712\u001b[0m \u001b[39m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestRegressor(criterion=\"absolute_error\", random_state=42, n_jobs=-1)\n",
    "\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "y_pred = random_forest.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[\"Ewltp (g/km)\"] = random_forest.predict(data_test[data_test.columns.intersection(columns_to_keep)])\n",
    "data_test[[\"ID\",\"Ewltp (g/km)\"]].to_csv(\"data/reg_rf_outlier_country.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF outliers numerical + Country + Mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "columns_to_keep = ['m (kg)','Mp', 'Ewltp (g/km)', \n",
    "                   'W (mm)', 'At1 (mm)', 'At2 (mm)',\n",
    "                     'ec (cm3)', 'ep (KW)', 'z (Wh/km)', \n",
    "                     'Fuel consumption ', 'Electric range (km)','Country']\n",
    "\n",
    "train, test = train_test_split(data_train[data_train.columns.intersection(columns_to_keep)], test_size=0.33, random_state=42)\n",
    "\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X_train, y_train = train.drop(columns=[\"Ewltp (g/km)\"]), train[\"Ewltp (g/km)\"]\n",
    "X_test, y_test = test.drop(columns=[\"Ewltp (g/km)\"]), test[\"Ewltp (g/km)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 3.1826152819810107\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "y_pred = random_forest.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[\"Ewltp (g/km)\"] = random_forest.predict(data_test[data_test.columns.intersection(columns_to_keep)])\n",
    "data_test[[\"ID\",\"Ewltp (g/km)\"]].to_csv(\"data/reg_rf_outlier_country_mp.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF (numerical) + K fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep_no_target = ['m (kg)', 'W (mm)', 'At1 (mm)',\n",
    "                    'At2 (mm)', 'ec (cm3)', 'ep (KW)', 'z (Wh/km)',\n",
    "                      'Fuel consumption ', 'Electric range (km)','Country']\n",
    "\n",
    "X_train = data_train[data_train.columns.intersection(columns_to_keep_no_target)]\n",
    "y_train= data_train[\"Ewltp (g/km)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\yoanj\\Documents\\M2S1\\Projet_Mining\\model_testing_yoan.ipynb Cellule 35\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yoanj/Documents/M2S1/Projet_Mining/model_testing_yoan.ipynb#X42sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m y_train_fold, y_val_fold \u001b[39m=\u001b[39m y_train\u001b[39m.\u001b[39miloc[train_index], y_train\u001b[39m.\u001b[39miloc[test_index]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yoanj/Documents/M2S1/Projet_Mining/model_testing_yoan.ipynb#X42sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# Entraînez le modèle sur X_train_fold et y_train_fold\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/yoanj/Documents/M2S1/Projet_Mining/model_testing_yoan.ipynb#X42sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train_fold, y_train_fold)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yoanj/Documents/M2S1/Projet_Mining/model_testing_yoan.ipynb#X42sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Évaluez le modèle sur X_val_fold (vous pouvez utiliser la métrique de votre choix, par exemple MAE)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yoanj/Documents/M2S1/Projet_Mining/model_testing_yoan.ipynb#X42sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_val_fold)\n",
      "File \u001b[1;32mc:\\Users\\yoanj\\Documents\\M2S1\\Projet_Mining\\.venv_mining\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\yoanj\\Documents\\M2S1\\Projet_Mining\\.venv_mining\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:456\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    445\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[0;32m    446\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(append\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[0;32m    447\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    448\u001b[0m ]\n\u001b[0;32m    450\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 456\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[0;32m    457\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[0;32m    458\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    459\u001b[0m     prefer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreads\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    460\u001b[0m )(\n\u001b[0;32m    461\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    462\u001b[0m         t,\n\u001b[0;32m    463\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbootstrap,\n\u001b[0;32m    464\u001b[0m         X,\n\u001b[0;32m    465\u001b[0m         y,\n\u001b[0;32m    466\u001b[0m         sample_weight,\n\u001b[0;32m    467\u001b[0m         i,\n\u001b[0;32m    468\u001b[0m         \u001b[39mlen\u001b[39;49m(trees),\n\u001b[0;32m    469\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    470\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[0;32m    471\u001b[0m         n_samples_bootstrap\u001b[39m=\u001b[39;49mn_samples_bootstrap,\n\u001b[0;32m    472\u001b[0m     )\n\u001b[0;32m    473\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, t \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(trees)\n\u001b[0;32m    474\u001b[0m )\n\u001b[0;32m    476\u001b[0m \u001b[39m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Users\\yoanj\\Documents\\M2S1\\Projet_Mining\\.venv_mining\\lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\yoanj\\Documents\\M2S1\\Projet_Mining\\.venv_mining\\lib\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[39m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[39m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[39m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[39m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n",
      "File \u001b[1;32mc:\\Users\\yoanj\\Documents\\M2S1\\Projet_Mining\\.venv_mining\\lib\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[39m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[39m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[39m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yoanj\\Documents\\M2S1\\Projet_Mining\\.venv_mining\\lib\\site-packages\\joblib\\parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1702\u001b[0m \u001b[39m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m \u001b[39m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1704\u001b[0m \u001b[39mif\u001b[39;00m ((\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m) \u001b[39mor\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m     (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget_status(\n\u001b[0;32m   1706\u001b[0m         timeout\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout) \u001b[39m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1707\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n\u001b[0;32m   1708\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m   1710\u001b[0m \u001b[39m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1711\u001b[0m \u001b[39m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1712\u001b[0m \u001b[39m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "k_folds = 3\n",
    "\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Créez des listes pour stocker les modèles et les scores\n",
    "models = []\n",
    "scores = []\n",
    "\n",
    "# Effectuez la validation croisée\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "    # Entraînez le modèle sur X_train_fold et y_train_fold\n",
    "    model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # Évaluez le modèle sur X_val_fold (vous pouvez utiliser la métrique de votre choix, par exemple MAE)\n",
    "    y_pred = model.predict(X_val_fold)\n",
    "    mae = mean_absolute_error(y_val_fold, y_pred)\n",
    "    \n",
    "    # Stockez le modèle et le score\n",
    "    models.append(model)\n",
    "    scores.append(mae)\n",
    "\n",
    "# Affichez les scores MAE pour chaque pli\n",
    "for i, mae in enumerate(scores):\n",
    "    print(f\"Fold {i + 1} MAE: {mae}\")\n",
    "\n",
    "# Calculez la moyenne des scores MAE\n",
    "average_mae = sum(scores) / k_folds\n",
    "print(f\"Average MAE: {average_mae}\")\n",
    "\n",
    "best_model_index = scores.index(min(scores))\n",
    "best_model = models[best_model_index]\n",
    "\n",
    "print(f\"min MAE: {min(scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[\"Ewltp (g/km)\"] = best_model.predict(data_test.drop(columns='ID'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[[\"ID\",\"Ewltp (g/km)\"]].to_csv(\"data/reg_rf_numerical_kf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF validation set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille de l'ensemble d'entraînement : 5073004\n",
      "Taille de l'ensemble de validation : 1249322\n",
      "Taille de l'ensemble de test : 1249323\n"
     ]
    }
   ],
   "source": [
    "columns_to_keep = ['m (kg)','Mp', 'Ewltp (g/km)', \n",
    "                   'W (mm)', 'At1 (mm)', 'At2 (mm)',\n",
    "                     'ec (cm3)', 'ep (KW)', 'z (Wh/km)', \n",
    "                     'Fuel consumption ', 'Electric range (km)','Country']\n",
    "train_data, rest_data = train_test_split(data_train[data_train.columns.intersection(columns_to_keep)], test_size=0.33, random_state=42)\n",
    "\n",
    "validation_data, test_data = train_test_split(rest_data, test_size=0.5, random_state=42)\n",
    "\n",
    "# Vérifiez les tailles des ensembles\n",
    "print(\"Taille de l'ensemble d'entraînement :\", len(train_data))\n",
    "print(\"Taille de l'ensemble de validation :\", len(validation_data))\n",
    "print(\"Taille de l'ensemble de test :\", len(test_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleurs hyperparamètres : {'max_depth': 10, 'n_estimators': 150}\n",
      "MAE sur l'ensemble de test avec le meilleur modèle : 10.80078252858729\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Divisez les ensembles en caractéristiques (X) et cible (y)\n",
    "X_train, y_train = train_data.drop(columns=[\"Ewltp (g/km)\"]), train_data[\"Ewltp (g/km)\"]\n",
    "X_val, y_val = validation_data.drop(columns=[\"Ewltp (g/km)\"]), validation_data[\"Ewltp (g/km)\"]\n",
    "X_test, y_test = test_data.drop(columns=[\"Ewltp (g/km)\"]), test_data[\"Ewltp (g/km)\"]\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [5, 7, 10],\n",
    "}\n",
    "\n",
    "# Créez votre modèle\n",
    "base_model = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "# Utilisez GridSearchCV pour la recherche sur grille avec validation croisée\n",
    "grid_search = GridSearchCV(estimator=base_model, param_grid=param_grid, scoring='neg_mean_absolute_error', cv=3)\n",
    "grid_search.fit(X_val, y_val)\n",
    "\n",
    "# Affichez les meilleurs hyperparamètres trouvés\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Meilleurs hyperparamètres :\", best_params)\n",
    "\n",
    "# Utilisez le meilleur modèle trouvé sur l'ensemble de test\n",
    "best_model = grid_search.best_estimator_\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "print(\"MAE sur l'ensemble de test avec le meilleur modèle :\", mae_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF with test_fonctions script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_fonctions import Dataset\n",
    "from src.preprocessing import display_missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=Dataset(\"data/train.csv\")\n",
    "data_train=train.load_data()\n",
    "test=Dataset(\"data/test.csv\")\n",
    "data_test=test.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_fonctions import Preprocessor,TrainPreprocessor,TestPreprocessor\n",
    "train_preprocessor=TrainPreprocessor(data_train)\n",
    "test_preprocessor=TestPreprocessor(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_continues=[\"m (kg)\",\"Mt\",\"W (mm)\",\"At1 (mm)\", \"At2 (mm)\", \n",
    "                     \"ec (cm3)\", \"ep (KW)\", \"z (Wh/km)\",\"Fuel consumption \",\n",
    "                     \"Electric range (km)\"]\n",
    "\n",
    "for col in variables_continues:\n",
    "    train_preprocessor.outlier_detection(col)\n",
    "    test_preprocessor.outlier_detection(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preprocessor.fill_fuel_consumption()\n",
    "train_preprocessor.fill_electric_range()\n",
    "train_preprocessor.fill_engine_capacity()\n",
    "train_preprocessor.fill_electric_consumption()\n",
    "train_preprocessor.fill_category_type()\n",
    "train_preprocessor.fill_wheel_base()\n",
    "train_preprocessor.fill_At_1()\n",
    "train_preprocessor.fill_At_2()\n",
    "train_preprocessor.fill_mass()\n",
    "train_preprocessor.fill_test_mass()\n",
    "train_preprocessor.fill_fuel_mode()\n",
    "train_preprocessor.fill_engine_power()\n",
    "train_preprocessor.encode_country()\n",
    "train_preprocessor.encode_manufacture_pooling()\n",
    "train_preprocessor.encode_fuel_mode()\n",
    "train_preprocessor.encode_fuel_type()\n",
    "train_preprocessor.last_step()\n",
    "\n",
    "test_preprocessor.fill_fuel_consumption()\n",
    "test_preprocessor.fill_electric_range()\n",
    "test_preprocessor.fill_engine_capacity()\n",
    "test_preprocessor.fill_electric_consumption()\n",
    "test_preprocessor.fill_category_type()\n",
    "test_preprocessor.fill_wheel_base()\n",
    "test_preprocessor.fill_At_1()\n",
    "test_preprocessor.fill_At_2()\n",
    "test_preprocessor.fill_mass()\n",
    "test_preprocessor.fill_test_mass()\n",
    "test_preprocessor.fill_fuel_mode()\n",
    "test_preprocessor.fill_engine_power()\n",
    "test_preprocessor.encode_country()\n",
    "test_preprocessor.encode_manufacture_pooling()\n",
    "test_preprocessor.encode_fuel_mode()\n",
    "test_preprocessor.encode_fuel_type()\n",
    "test_preprocessor.last_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conforme(df):\n",
    "    df['conforme'] = df['Tan'].isna()\n",
    "    df['conforme'] = df['conforme'].apply(lambda x: 1 if x==False else 0)\n",
    "    df.drop(columns='Tan', inplace=True)\n",
    "    pass\n",
    "def compute_surface(obs):\n",
    "    max_largeur= max(obs['At1 (mm)'], obs['At2 (mm)'])\n",
    "    return obs['W (mm)']*obs['At1 (mm)'] if max_largeur == obs['At1 (mm)'] else obs['W (mm)'] * obs['At2 (mm)']\n",
    "\n",
    "def create_surface(df):\n",
    "    df['surface']= df.apply(compute_surface, axis=1)\n",
    "    pass\n",
    "\n",
    "create_conforme(data_train)\n",
    "create_surface(data_train)\n",
    "\n",
    "create_conforme(data_test)\n",
    "create_surface(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_this_cuz_no_use=['VFN', 'Mh', 'Man', 'T', 'Mk', 'Cn']\n",
    "# drop_this_cuz_personnal_choice = ['W (mm)', 'At1 (mm)', 'At2 (mm)','Fm','Erwltp (g/km)']\n",
    "\n",
    "data_train.drop(columns=drop_this_cuz_no_use, inplace=True)\n",
    "data_test.drop(columns=drop_this_cuz_no_use, inplace=True)\n",
    "\n",
    "data_train.drop(columns='ID',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from src.preprocessing import display_missing_values\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "sns.set_theme(style=\"ticks\", palette=\"pastel\")\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.drop(columns=['Ct','Cr','Erwltp (g/km)'], inplace=True)\n",
    "data_test.drop(columns=['Ct','Cr','Erwltp (g/km)'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Country', 'Mp', 'm (kg)', 'Mt', 'Ewltp (g/km)', 'W (mm)', 'At1 (mm)',\n",
       "       'At2 (mm)', 'Ft', 'Fm', 'ec (cm3)', 'ep (KW)', 'z (Wh/km)',\n",
       "       'Fuel consumption ', 'Electric range (km)', 'flag_Fuel consumption ',\n",
       "       'flag_Electric range (km)', 'flag_ec (cm3)', 'flag_z (Wh/km)',\n",
       "       'flag_W (mm)', 'flag_Mt', 'conforme', 'surface'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data_train, test_size=0.33, random_state=42)\n",
    "\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X_train, y_train = train.drop(columns=[\"Ewltp (g/km)\"]), train[\"Ewltp (g/km)\"]\n",
    "X_test, y_test = test.drop(columns=[\"Ewltp (g/km)\"]), test[\"Ewltp (g/km)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 2.976801154055315\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "y_pred = random_forest.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[\"Ewltp (g/km)\"] = random_forest.predict(data_test.drop(columns='ID'))\n",
    "data_test[[\"ID\",\"Ewltp (g/km)\"]].to_csv(\"data/rf_num_and_cat.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'criterion': 'squared_error',\n",
       " 'max_depth': None,\n",
       " 'max_features': 1.0,\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': -1,\n",
       " 'oob_score': False,\n",
       " 'random_state': 42,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_continues=[\"m (kg)\",\"Mt\",\"W (mm)\",\"At1 (mm)\", \"At2 (mm)\", \n",
    "                     \"ec (cm3)\", \"ep (KW)\", \"z (Wh/km)\",\"Fuel consumption \",\n",
    "                     \"Electric range (km)\",\"surface\"]\n",
    "\n",
    "others= [col for col in data_train.columns if col not in variables_continues]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=2,)\n",
    "\n",
    "# Application de la transformation aux variables continues\n",
    "variables_continues_transformees = poly.fit_transform(data_train[variables_continues])\n",
    "\n",
    "data_train_transforme = pd.DataFrame(variables_continues_transformees, columns=poly.get_feature_names_out(variables_continues))\n",
    "data_train= pd.concat([data_train[others],data_train_transforme], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "others= [col for col in data_test.columns if col not in variables_continues]\n",
    "\n",
    "variables_continues_transformees = poly.fit_transform(data_test[variables_continues])\n",
    "\n",
    "data_test_transforme = pd.DataFrame(variables_continues_transformees, columns=poly.get_feature_names_out(variables_continues))\n",
    "data_test= pd.concat([data_test[list(filter(lambda x: x !='Ewltp (g/km)',others))],data_train_transforme], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data_train, test_size=0.33, random_state=42)\n",
    "\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X_train, y_train = train.drop(columns=[\"Ewltp (g/km)\"]), train[\"Ewltp (g/km)\"]\n",
    "X_test, y_test = test.drop(columns=[\"Ewltp (g/km)\"]), test[\"Ewltp (g/km)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 2.9867784350291724\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "y_pred = random_forest.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nRandomForestRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\yoanj\\Documents\\M2S1\\Projet_Mining\\model_testing_yoan.ipynb Cellule 72\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/yoanj/Documents/M2S1/Projet_Mining/model_testing_yoan.ipynb#Y131sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m data_test[\u001b[39m\"\u001b[39m\u001b[39mEwltp (g/km)\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m random_forest\u001b[39m.\u001b[39;49mpredict(data_test\u001b[39m.\u001b[39;49mdrop(columns\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mID\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yoanj/Documents/M2S1/Projet_Mining/model_testing_yoan.ipynb#Y131sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m data_test[[\u001b[39m\"\u001b[39m\u001b[39mID\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mEwltp (g/km)\u001b[39m\u001b[39m\"\u001b[39m]]\u001b[39m.\u001b[39mto_csv(\u001b[39m\"\u001b[39m\u001b[39mdata/rf_poly2_num_and_cat.csv\u001b[39m\u001b[39m\"\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\yoanj\\Documents\\M2S1\\Projet_Mining\\.venv_mining\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:984\u001b[0m, in \u001b[0;36mForestRegressor.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    982\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m    983\u001b[0m \u001b[39m# Check data\u001b[39;00m\n\u001b[1;32m--> 984\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_X_predict(X)\n\u001b[0;32m    986\u001b[0m \u001b[39m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[0;32m    987\u001b[0m n_jobs, _, _ \u001b[39m=\u001b[39m _partition_estimators(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_estimators, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs)\n",
      "File \u001b[1;32mc:\\Users\\yoanj\\Documents\\M2S1\\Projet_Mining\\.venv_mining\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:599\u001b[0m, in \u001b[0;36mBaseForest._validate_X_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[39mValidate X whenever one tries to predict, apply, predict_proba.\"\"\"\u001b[39;00m\n\u001b[0;32m    598\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[1;32m--> 599\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, dtype\u001b[39m=\u001b[39;49mDTYPE, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    600\u001b[0m \u001b[39mif\u001b[39;00m issparse(X) \u001b[39mand\u001b[39;00m (X\u001b[39m.\u001b[39mindices\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m np\u001b[39m.\u001b[39mintc \u001b[39mor\u001b[39;00m X\u001b[39m.\u001b[39mindptr\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m np\u001b[39m.\u001b[39mintc):\n\u001b[0;32m    601\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\yoanj\\Documents\\M2S1\\Projet_Mining\\.venv_mining\\lib\\site-packages\\sklearn\\base.py:605\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    603\u001b[0m         out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    604\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 605\u001b[0m     out \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    606\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    607\u001b[0m     out \u001b[39m=\u001b[39m _check_y(y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\yoanj\\Documents\\M2S1\\Projet_Mining\\.venv_mining\\lib\\site-packages\\sklearn\\utils\\validation.py:957\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    951\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    952\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    953\u001b[0m             \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    954\u001b[0m         )\n\u001b[0;32m    956\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 957\u001b[0m         _assert_all_finite(\n\u001b[0;32m    958\u001b[0m             array,\n\u001b[0;32m    959\u001b[0m             input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[0;32m    960\u001b[0m             estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[0;32m    961\u001b[0m             allow_nan\u001b[39m=\u001b[39;49mforce_all_finite \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    962\u001b[0m         )\n\u001b[0;32m    964\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    965\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32mc:\\Users\\yoanj\\Documents\\M2S1\\Projet_Mining\\.venv_mining\\lib\\site-packages\\sklearn\\utils\\validation.py:122\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[39mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    120\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m _assert_all_finite_element_wise(\n\u001b[0;32m    123\u001b[0m     X,\n\u001b[0;32m    124\u001b[0m     xp\u001b[39m=\u001b[39;49mxp,\n\u001b[0;32m    125\u001b[0m     allow_nan\u001b[39m=\u001b[39;49mallow_nan,\n\u001b[0;32m    126\u001b[0m     msg_dtype\u001b[39m=\u001b[39;49mmsg_dtype,\n\u001b[0;32m    127\u001b[0m     estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[0;32m    128\u001b[0m     input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[0;32m    129\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\yoanj\\Documents\\M2S1\\Projet_Mining\\.venv_mining\\lib\\site-packages\\sklearn\\utils\\validation.py:171\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[39mif\u001b[39;00m estimator_name \u001b[39mand\u001b[39;00m input_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    155\u001b[0m     \u001b[39m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    156\u001b[0m     \u001b[39m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     msg_err \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[0;32m    158\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m does not accept missing values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    159\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    169\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m#estimators-that-handle-nan-values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    170\u001b[0m     )\n\u001b[1;32m--> 171\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nRandomForestRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "data_test[\"Ewltp (g/km)\"] = random_forest.predict(data_test.drop(columns='ID'))\n",
    "data_test[[\"ID\",\"Ewltp (g/km)\"]].to_csv(\"data/rf_poly2_num_and_cat.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another RF test\n",
    "\n",
    "Tourne en 21 min. Ccl: les variables flags sont pas forcément géniales à avoir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "columns_to_keep = ['Country', 'Mp', 'm (kg)', 'Mt', 'Ewltp (g/km)', 'W (mm)', 'At1 (mm)',\n",
    "       'At2 (mm)', 'Ft', 'Fm', 'ec (cm3)', 'ep (KW)', 'z (Wh/km)',\n",
    "       'Fuel consumption ', 'Electric range (km)', 'conforme', 'surface']\n",
    "\n",
    "train, test = train_test_split(data_train[data_train.columns.intersection(columns_to_keep)], test_size=0.33, random_state=42)\n",
    "\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X_train, y_train = train.drop(columns=[\"Ewltp (g/km)\"]), train[\"Ewltp (g/km)\"]\n",
    "X_test, y_test = test.drop(columns=[\"Ewltp (g/km)\"]), test[\"Ewltp (g/km)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 2.9767661924047735\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "y_pred = random_forest.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[\"Ewltp (g/km)\"] = random_forest.predict(data_test[data_test.columns.intersection(columns_to_keep)])\n",
    "data_test[[\"ID\",\"Ewltp (g/km)\"]].to_csv(\"data/rf_no_flag_num_cat.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF\n",
    "\n",
    "No drop duplicates, outliers treatment, using numericals. \n",
    "\n",
    "Ordinal Encoding  pour: \"Ct\", \"Cr\",\"Fm\"\n",
    "\n",
    "Label Encoding pour : \"Country\",\"Mp\"\n",
    "\n",
    "Utilisation colonnes surface, conforme. Pas d'utilisation des colonnes flag (outliers = 1 else 0)\n",
    "\n",
    "### Import + Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonctions import Dataset\n",
    "from src.preprocessing import display_missing_values\n",
    "\n",
    "train=Dataset(\"data/train.csv\")\n",
    "data_train=train.load_data()\n",
    "test=Dataset(\"data/test.csv\")\n",
    "data_test=test.load_data()\n",
    "\n",
    "from fonctions import TrainPreprocessor,TestPreprocessor\n",
    "train_preprocessor=TrainPreprocessor(data_train)\n",
    "test_preprocessor=TestPreprocessor(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_continues=[\"m (kg)\",\"Mt\",\"W (mm)\",\"At1 (mm)\", \"At2 (mm)\", \n",
    "                     \"ec (cm3)\", \"ep (KW)\", \"z (Wh/km)\",\"Fuel consumption \",\n",
    "                     \"Electric range (km)\"]\n",
    "\n",
    "for col in variables_continues:\n",
    "    train_preprocessor.outlier_detection(col)\n",
    "    test_preprocessor.outlier_detection(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preprocessor.fill_fuel_consumption()\n",
    "train_preprocessor.fill_electric_range()\n",
    "train_preprocessor.fill_engine_capacity()\n",
    "train_preprocessor.fill_electric_consumption()\n",
    "train_preprocessor.fill_category_type()\n",
    "train_preprocessor.fill_wheel_base()\n",
    "train_preprocessor.fill_At_1()\n",
    "train_preprocessor.fill_At_2()\n",
    "train_preprocessor.fill_mass()\n",
    "train_preprocessor.fill_test_mass()\n",
    "train_preprocessor.fill_fuel_mode()\n",
    "train_preprocessor.fill_engine_power()\n",
    "train_preprocessor.encode_country()\n",
    "train_preprocessor.encode_manufacture_pooling()\n",
    "train_preprocessor.encode_category_registered() #new\n",
    "train_preprocessor.encode_category_type() #new\n",
    "train_preprocessor.encode_fuel_mode()\n",
    "train_preprocessor.encode_fuel_type()\n",
    "train_preprocessor.last_step()\n",
    "\n",
    "test_preprocessor.fill_fuel_consumption()\n",
    "test_preprocessor.fill_electric_range()\n",
    "test_preprocessor.fill_engine_capacity()\n",
    "test_preprocessor.fill_electric_consumption()\n",
    "test_preprocessor.fill_category_type()\n",
    "test_preprocessor.fill_wheel_base()\n",
    "test_preprocessor.fill_At_1()\n",
    "test_preprocessor.fill_At_2()\n",
    "test_preprocessor.fill_mass()\n",
    "test_preprocessor.fill_test_mass()\n",
    "test_preprocessor.fill_fuel_mode()\n",
    "test_preprocessor.fill_engine_power()\n",
    "test_preprocessor.encode_country()\n",
    "test_preprocessor.encode_manufacture_pooling()\n",
    "test_preprocessor.encode_category_registered() #new\n",
    "test_preprocessor.encode_category_type() #new\n",
    "test_preprocessor.encode_fuel_mode()\n",
    "test_preprocessor.encode_fuel_type()\n",
    "test_preprocessor.last_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conforme(df):\n",
    "    df['conforme'] = df['Tan'].isna()\n",
    "    df['conforme'] = df['conforme'].apply(lambda x: 1 if x==False else 0)\n",
    "    df.drop(columns='Tan', inplace=True)\n",
    "    pass\n",
    "def compute_surface(obs):\n",
    "    max_largeur= max(obs['At1 (mm)'], obs['At2 (mm)'])\n",
    "    return obs['W (mm)']*obs['At1 (mm)'] if max_largeur == obs['At1 (mm)'] else obs['W (mm)'] * obs['At2 (mm)']\n",
    "\n",
    "def create_surface(df):\n",
    "    df['surface']= df.apply(compute_surface, axis=1)\n",
    "    pass\n",
    "\n",
    "# Feature Engineering\n",
    "create_conforme(data_train)\n",
    "create_surface(data_train)\n",
    "\n",
    "create_conforme(data_test)\n",
    "create_surface(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_this_cuz_no_use=['VFN', 'Mh', 'Man', 'T', 'Mk', 'Cn','Erwltp (g/km)']\n",
    "\n",
    "data_train.drop(columns=drop_this_cuz_no_use, inplace=True)\n",
    "data_test.drop(columns=drop_this_cuz_no_use, inplace=True)\n",
    "\n",
    "data_train.drop(columns='ID',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Country', 'Mp', 'Ct', 'Cr', 'm (kg)', 'Mt', 'Ewltp (g/km)', 'W (mm)',\n",
       "       'At1 (mm)', 'At2 (mm)', 'Ft', 'Fm', 'ec (cm3)', 'ep (KW)', 'z (Wh/km)',\n",
       "       'Fuel consumption ', 'Electric range (km)', 'flag_m (kg)', 'flag_Mt',\n",
       "       'flag_W (mm)', 'flag_At1 (mm)', 'flag_At2 (mm)', 'flag_ec (cm3)',\n",
       "       'flag_ep (KW)', 'flag_z (Wh/km)', 'flag_Fuel consumption ',\n",
       "       'flag_Electric range (km)', 'conforme', 'surface'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "columns_to_keep = ['Country', 'Mp', 'Ct', 'Cr', 'm (kg)', 'Mt', 'Ewltp (g/km)', 'W (mm)',\n",
    "       'At1 (mm)', 'At2 (mm)', 'Ft', 'Fm', 'ec (cm3)', 'ep (KW)', 'z (Wh/km)',\n",
    "       'Fuel consumption ', 'Electric range (km)', 'conforme', 'surface']\n",
    "\n",
    "train, test = train_test_split(data_train[data_train.columns.intersection(columns_to_keep)], test_size=0.33, random_state=42)\n",
    "\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X_train, y_train = train.drop(columns=[\"Ewltp (g/km)\"]), train[\"Ewltp (g/km)\"]\n",
    "X_test, y_test = test.drop(columns=[\"Ewltp (g/km)\"]), test[\"Ewltp (g/km)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 2.974584586928926\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "y_pred = random_forest.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[\"Ewltp (g/km)\"] = random_forest.predict(data_test[data_test.columns.intersection(columns_to_keep)])\n",
    "data_test[[\"ID\",\"Ewltp (g/km)\"]].to_csv(\"data/rf_more_encoded.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data_train, test_size=0.33, random_state=42)\n",
    "\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X_train, y_train = train.drop(columns=[\"Ewltp (g/km)\"]), train[\"Ewltp (g/km)\"]\n",
    "X_test, y_test = test.drop(columns=[\"Ewltp (g/km)\"]), test[\"Ewltp (g/km)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data_train,test_size=0.33,random_state=42)\n",
    "\n",
    "train.reset_index(drop=True, inplace=True ) #car ça fout la merde dans l'index\n",
    "test.reset_index(drop = True, inplace = True)\n",
    "\n",
    "X_train, y_train =train.drop(columns=[\"Ewltp (g/km)\"]), train[\"Ewltp (g/km)\"]\n",
    "X_test, y_test =test.drop(columns=[\"Ewltp (g/km)\"]), test[\"Ewltp (g/km)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleurs hyperparamètres : {'n_jobs': -1, 'n_estimators': 300, 'min_child_weight': 2, 'max_depth': 5, 'learning_rate': 0.2}\n",
      "Mean Absolute Error (MAE): 6.035682412861069\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Créez un modèle XGBoost\n",
    "xgb_model = XGBRegressor(objective=\"reg:squarederror\", random_state=42)\n",
    "\n",
    "# Liste des hyperparamètres à régler et leurs plages de valeurs\n",
    "param_dist = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'min_child_weight': [1, 2, 3],\n",
    "    'n_jobs':[-1,-1,-1]\n",
    "}\n",
    "\n",
    "# Recherche aléatoire des hyperparamètres\n",
    "random_search = RandomizedSearchCV(xgb_model, param_distributions=param_dist, n_iter=10, cv=5, n_jobs=-1, random_state=42)\n",
    "\n",
    "# Exécutez la recherche aléatoire sur les données d'entraînement\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtenez les meilleurs hyperparamètres\n",
    "best_params = random_search.best_params_\n",
    "print(\"Meilleurs hyperparamètres :\", best_params)\n",
    "\n",
    "# Utilisez les meilleurs hyperparamètres pour entraîner le modèle final\n",
    "best_xgb_model = XGBRegressor(objective=\"reg:squarederror\", random_state=42, **best_params)\n",
    "best_xgb_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred =best_xgb_model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 7.848606144154054\n"
     ]
    }
   ],
   "source": [
    "hist_gradient_boosting = HistGradientBoostingRegressor(interaction_cst=\"pairwise\",warm_start=True,\n",
    "                                                        learning_rate=0.3, random_state=42,\n",
    "                                                        categorical_features=['Country', 'Mp','Ft'])\n",
    "\n",
    "# Entraîner le modèle sur l'ensemble d'entraînement\n",
    "hist_gradient_boosting.fit(X_train, y_train)\n",
    "\n",
    "# Faire des prédictions sur l'ensemble de test\n",
    "y_pred = hist_gradient_boosting.predict(X_test)\n",
    "\n",
    "# Calculer la MAE\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(data_train.drop(columns='Ewltp (g/km)'), data_train['Ewltp (g/km)']\n",
    "                                                    , test_size=0.33, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yoanj\\Documents\\M2S1\\Projet_Mining\\.venv_mining\\lib\\site-packages\\sklearn\\experimental\\enable_hist_gradient_boosting.py:15: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=3,\n",
       "                   estimator=HistGradientBoostingRegressor(random_state=42),\n",
       "                   param_distributions={&#x27;l2_regularization&#x27;: [0.0, 0.1, 0.2,\n",
       "                                                              0.4, 0.6, 0.8],\n",
       "                                        &#x27;learning_rate&#x27;: [0.01, 0.1, 0.2, 0.3,\n",
       "                                                          0.8, 0.9],\n",
       "                                        &#x27;max_depth&#x27;: [3, 5, 7, 10, 12, 20],\n",
       "                                        &#x27;max_iter&#x27;: [100, 200, 300, 400, 500,\n",
       "                                                     600],\n",
       "                                        &#x27;min_samples_leaf&#x27;: [1, 2, 4, 6, 7, 9]},\n",
       "                   random_state=42, scoring=&#x27;neg_mean_absolute_error&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=3,\n",
       "                   estimator=HistGradientBoostingRegressor(random_state=42),\n",
       "                   param_distributions={&#x27;l2_regularization&#x27;: [0.0, 0.1, 0.2,\n",
       "                                                              0.4, 0.6, 0.8],\n",
       "                                        &#x27;learning_rate&#x27;: [0.01, 0.1, 0.2, 0.3,\n",
       "                                                          0.8, 0.9],\n",
       "                                        &#x27;max_depth&#x27;: [3, 5, 7, 10, 12, 20],\n",
       "                                        &#x27;max_iter&#x27;: [100, 200, 300, 400, 500,\n",
       "                                                     600],\n",
       "                                        &#x27;min_samples_leaf&#x27;: [1, 2, 4, 6, 7, 9]},\n",
       "                   random_state=42, scoring=&#x27;neg_mean_absolute_error&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: HistGradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>HistGradientBoostingRegressor(random_state=42)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">HistGradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>HistGradientBoostingRegressor(random_state=42)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=HistGradientBoostingRegressor(random_state=42),\n",
       "                   param_distributions={'l2_regularization': [0.0, 0.1, 0.2,\n",
       "                                                              0.4, 0.6, 0.8],\n",
       "                                        'learning_rate': [0.01, 0.1, 0.2, 0.3,\n",
       "                                                          0.8, 0.9],\n",
       "                                        'max_depth': [3, 5, 7, 10, 12, 20],\n",
       "                                        'max_iter': [100, 200, 300, 400, 500,\n",
       "                                                     600],\n",
       "                                        'min_samples_leaf': [1, 2, 4, 6, 7, 9]},\n",
       "                   random_state=42, scoring='neg_mean_absolute_error')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "# Définir la grille des hyperparamètres\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2,0.3,0.8,0.9],\n",
    "    'max_depth': [3, 5, 7,10,12,20],\n",
    "    'max_iter': [100, 200, 300,400,500,600],\n",
    "    'min_samples_leaf': [1, 2, 4,6,7,9],\n",
    "    'l2_regularization': [0.0, 0.1, 0.2,0.4,0.6,0.8]\n",
    "}\n",
    "\n",
    "# Initialiser le modèle\n",
    "hgb_regressor = HistGradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# Utiliser la recherche aléatoire\n",
    "random_search = RandomizedSearchCV(hgb_regressor, param_distributions=param_grid, n_iter=10, cv=3, scoring='neg_mean_absolute_error', random_state=42)\n",
    "\n",
    "# Exécuter la recherche\n",
    "random_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 4.0602364248951\n"
     ]
    }
   ],
   "source": [
    "y_pred = random_search.predict(X_test)\n",
    "\n",
    "# Calculer la MAE\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'min_samples_leaf': 4,\n",
       " 'max_iter': 600,\n",
       " 'max_depth': 20,\n",
       " 'learning_rate': 0.3,\n",
       " 'l2_regularization': 0.0}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Réseau de neurones pour le fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "151433/151433 [==============================] - 256s 2ms/step - loss: 635.3922 - val_loss: 315.9615\n",
      "Epoch 2/50\n",
      "151433/151433 [==============================] - 270s 2ms/step - loss: 290.4791 - val_loss: 276.5474\n",
      "Epoch 3/50\n",
      "151433/151433 [==============================] - 264s 2ms/step - loss: 259.0506 - val_loss: 238.8933\n",
      "Epoch 4/50\n",
      "151433/151433 [==============================] - 260s 2ms/step - loss: 242.1173 - val_loss: 232.1328\n",
      "Epoch 5/50\n",
      "151433/151433 [==============================] - 296s 2ms/step - loss: 230.9690 - val_loss: 228.8635\n",
      "Epoch 6/50\n",
      "151433/151433 [==============================] - 274s 2ms/step - loss: 223.9817 - val_loss: 216.8509\n",
      "Epoch 7/50\n",
      "151433/151433 [==============================] - 257s 2ms/step - loss: 218.4716 - val_loss: 213.0065\n",
      "Epoch 8/50\n",
      "151433/151433 [==============================] - 242s 2ms/step - loss: 214.7158 - val_loss: 208.7912\n",
      "Epoch 9/50\n",
      "151433/151433 [==============================] - 246s 2ms/step - loss: 211.6960 - val_loss: 205.3407\n",
      "Epoch 10/50\n",
      "151433/151433 [==============================] - 243s 2ms/step - loss: 207.8899 - val_loss: 205.1863\n",
      "Epoch 11/50\n",
      "151433/151433 [==============================] - 244s 2ms/step - loss: 204.9559 - val_loss: 197.0775\n",
      "Epoch 12/50\n",
      "151433/151433 [==============================] - 247s 2ms/step - loss: 201.6217 - val_loss: 204.2804\n",
      "Epoch 13/50\n",
      "151433/151433 [==============================] - 250s 2ms/step - loss: 199.2526 - val_loss: 193.7775\n",
      "Epoch 14/50\n",
      "151433/151433 [==============================] - 250s 2ms/step - loss: 197.3618 - val_loss: 202.7020\n",
      "Epoch 15/50\n",
      "151433/151433 [==============================] - 251s 2ms/step - loss: 194.8690 - val_loss: 209.1766\n",
      "Epoch 16/50\n",
      "151433/151433 [==============================] - 252s 2ms/step - loss: 193.0952 - val_loss: 195.3308\n",
      "Epoch 17/50\n",
      "151433/151433 [==============================] - 248s 2ms/step - loss: 190.7817 - val_loss: 197.8229\n",
      "Epoch 18/50\n",
      "151433/151433 [==============================] - 254s 2ms/step - loss: 188.4060 - val_loss: 204.3167\n",
      "Epoch 19/50\n",
      "151433/151433 [==============================] - 253s 2ms/step - loss: 186.3131 - val_loss: 192.3080\n",
      "Epoch 20/50\n",
      "151433/151433 [==============================] - 250s 2ms/step - loss: 184.4358 - val_loss: 176.8297\n",
      "Epoch 21/50\n",
      "151433/151433 [==============================] - 251s 2ms/step - loss: 182.6873 - val_loss: 180.6386\n",
      "Epoch 22/50\n",
      "151433/151433 [==============================] - 254s 2ms/step - loss: 181.0263 - val_loss: 197.3812\n",
      "Epoch 23/50\n",
      "151433/151433 [==============================] - 253s 2ms/step - loss: 179.0834 - val_loss: 180.4373\n",
      "Epoch 24/50\n",
      "151433/151433 [==============================] - 250s 2ms/step - loss: 177.2500 - val_loss: 186.9733\n",
      "Epoch 25/50\n",
      "151433/151433 [==============================] - 253s 2ms/step - loss: 175.9581 - val_loss: 174.6779\n",
      "Epoch 26/50\n",
      "151433/151433 [==============================] - 253s 2ms/step - loss: 174.5632 - val_loss: 181.7817\n",
      "Epoch 27/50\n",
      "151433/151433 [==============================] - 254s 2ms/step - loss: 173.4442 - val_loss: 181.6966\n",
      "Epoch 28/50\n",
      "151433/151433 [==============================] - 253s 2ms/step - loss: 172.0563 - val_loss: 168.1711\n",
      "Epoch 29/50\n",
      "151433/151433 [==============================] - 253s 2ms/step - loss: 170.9709 - val_loss: 178.3278\n",
      "Epoch 30/50\n",
      "151433/151433 [==============================] - 249s 2ms/step - loss: 170.5916 - val_loss: 171.9642\n",
      "Epoch 31/50\n",
      "151433/151433 [==============================] - 253s 2ms/step - loss: 169.1010 - val_loss: 174.9920\n",
      "Epoch 32/50\n",
      "151433/151433 [==============================] - 253s 2ms/step - loss: 167.8883 - val_loss: 177.4191\n",
      "Epoch 33/50\n",
      "151433/151433 [==============================] - 253s 2ms/step - loss: 166.9921 - val_loss: 166.2205\n",
      "Epoch 34/50\n",
      "151433/151433 [==============================] - 251s 2ms/step - loss: 165.7988 - val_loss: 162.8911\n",
      "Epoch 35/50\n",
      "151433/151433 [==============================] - 256s 2ms/step - loss: 164.9725 - val_loss: 165.1106\n",
      "Epoch 36/50\n",
      "151433/151433 [==============================] - 254s 2ms/step - loss: 164.2161 - val_loss: 168.4459\n",
      "Epoch 37/50\n",
      "151433/151433 [==============================] - 251s 2ms/step - loss: 163.3973 - val_loss: 164.9182\n",
      "Epoch 38/50\n",
      "151433/151433 [==============================] - 250s 2ms/step - loss: 162.8877 - val_loss: 160.5349\n",
      "Epoch 39/50\n",
      "151433/151433 [==============================] - 248s 2ms/step - loss: 162.2285 - val_loss: 162.7116\n",
      "Epoch 40/50\n",
      "151433/151433 [==============================] - 251s 2ms/step - loss: 161.5204 - val_loss: 168.7410\n",
      "Epoch 41/50\n",
      "151433/151433 [==============================] - 249s 2ms/step - loss: 161.1395 - val_loss: 162.1255\n",
      "Epoch 42/50\n",
      "151433/151433 [==============================] - 246s 2ms/step - loss: 160.8530 - val_loss: 160.2200\n",
      "Epoch 43/50\n",
      "151433/151433 [==============================] - 247s 2ms/step - loss: 159.9180 - val_loss: 168.3236\n",
      "Epoch 44/50\n",
      "151433/151433 [==============================] - 249s 2ms/step - loss: 159.7346 - val_loss: 157.2596\n",
      "Epoch 45/50\n",
      "151433/151433 [==============================] - 253s 2ms/step - loss: 159.2495 - val_loss: 166.7866\n",
      "Epoch 46/50\n",
      "151433/151433 [==============================] - 255s 2ms/step - loss: 158.6555 - val_loss: 159.1855\n",
      "Epoch 47/50\n",
      "151433/151433 [==============================] - 245s 2ms/step - loss: 158.1540 - val_loss: 165.4224\n",
      "Epoch 48/50\n",
      "151433/151433 [==============================] - 249s 2ms/step - loss: 157.7901 - val_loss: 166.5821\n",
      "Epoch 49/50\n",
      "151433/151433 [==============================] - 250s 2ms/step - loss: 157.6403 - val_loss: 165.6296\n",
      "Epoch 50/50\n",
      "151433/151433 [==============================] - 252s 2ms/step - loss: 156.6501 - val_loss: 162.6908\n",
      "47323/47323 [==============================] - 53s 1ms/step\n",
      "Mean Absolute Error (MAE): 7.006961559326027\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Charger vos données\n",
    "# Assurez-vous que 'Country', 'Mp', 'Ft' sont déjà label encodés\n",
    "\n",
    "# Séparation des features et de la target\n",
    "X = data_train.drop(columns=[\"Ewltp (g/km)\"])\n",
    "y = data_train[\"Ewltp (g/km)\"]\n",
    "\n",
    "# Séparation des données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalisation des données\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Définition du modèle\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1)  # Couche de sortie pour la régression\n",
    "])\n",
    "\n",
    "# Compilation du modèle\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Entraînement du modèle\n",
    "model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Évaluation sur l'ensemble de test\n",
    "y_pred = model.predict(X_test_scaled).flatten()\n",
    "\n",
    "# Calcul de la MAE\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_mining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
