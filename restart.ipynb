{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data + Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor,BaggingRegressor,AdaBoostRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, OrdinalEncoder, RobustScaler, PolynomialFeatures, TargetEncoder\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from category_encoders import LeaveOneOutEncoder\n",
    "from category_encoders.count import CountEncoder\n",
    "from category_encoders.cat_boost import CatBoostEncoder\n",
    "import joblib\n",
    "\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from xgboost.callback import EarlyStopping, LearningRateScheduler\n",
    "\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "sns.set_theme(style=\"ticks\", palette=\"pastel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train= pd.read_csv(\"data/train.csv\",sep=\",\",low_memory=False)\n",
    "data_test = pd.read_csv(\"data/test.csv\",sep=\",\",low_memory=False)\n",
    "\n",
    "data_train.name=\"data_train\"\n",
    "data_test.name=\"data_test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupération d'observations\n",
    "\n",
    "Lorsque la voiture est électrique: on peut se permettre de set `ec(cm3)`, `Fuel consumption `, `z (Wh/km)` à 0\n",
    "\n",
    "lorsque la voiture n'est pas hybride / électrique : on peut mettre `Electric range (km)` à 0\n",
    "\n",
    "A voir si on choisir de prendre le traitement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_fuel_types(category: str):\n",
    "    if category in ['PETROL/ELECTRIC', 'DIESEL/ELECTRIC']:\n",
    "        return \"HYBRID\"\n",
    "    elif category in ['NG-BIOMETHANE', 'HYDROGEN', 'NG','E85']:\n",
    "        return \"BIO-FUEL\"\n",
    "    elif category in ['PETROL','LPG'] :\n",
    "        return 'PETROL'\n",
    "    else:\n",
    "        return category   \n",
    "\n",
    "def recup_electric(df):\n",
    "    #  ec (cm3)\n",
    "    df.loc[(df[\"Ft\"].apply(group_fuel_types)==\"ELECTRIC\") & (df[\"ec (cm3)\"].isna()),\"ec (cm3)\"] = 0\n",
    "    #  Fm\n",
    "    df.loc[(df[\"Ft\"].apply(group_fuel_types) ==\"ELECTRIC\") & (df[\"Fm\"].isna()),\"Fm\"] = \"E\"\n",
    "    df.loc[(df[\"Ft\"].apply(group_fuel_types) ==\"HYBRID\") & (df[\"Fm\"].isna()),\"Fm\"] = \"P\"\n",
    "\n",
    "    # Electric range (km)\n",
    "    df.loc[~(df[\"Ft\"].apply(group_fuel_types).isin([\"ELECTRIC\", \"HYBRID\"])) & (df[\"Electric range (km)\"].isna()),\"Electric range (km)\"] = 0\n",
    "\n",
    "    #  Fuel consumption \n",
    "    df.loc[(df[\"Ft\"].apply(group_fuel_types) ==\"ELECTRIC\") & (df[\"Fuel consumption \"].isna()),\"Fuel consumption \"] = 0\n",
    "\n",
    "    #  z (Wh/km)\n",
    "    df.loc[~(df[\"Ft\"].apply(group_fuel_types).isin([\"ELECTRIC\", \"HYBRID\"])) & (df[\"z (Wh/km)\"].isna()),\"z (Wh/km)\"] = 0\n",
    "    pass\n",
    "\n",
    "recup_electric(data_train)\n",
    "recup_electric(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run if you want to use **mean_group_imputer** then run **outliers** and then the **delete columns** finally the mean_group_imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_fuel_types(category: str):\n",
    "    if category in ['PETROL/ELECTRIC', 'DIESEL/ELECTRIC']:\n",
    "        return \"HYBRID\"\n",
    "    elif category in ['NG-BIOMETHANE', 'HYDROGEN', 'NG','E85']:\n",
    "        return \"BIO-FUEL\"\n",
    "    elif category in ['PETROL','LPG'] :\n",
    "        return 'PETROL'\n",
    "    else:\n",
    "        return category   \n",
    "    \n",
    "def recup_fuel_mode(df):\n",
    "    df.loc[(df[\"Ft\"].apply(group_fuel_types) ==\"ELECTRIC\") & (df[\"Fm\"].isna()),\"Fm\"] = \"E\"\n",
    "    df.loc[(df[\"Ft\"].apply(group_fuel_types) ==\"HYBRID\") & (df[\"Fm\"].isna()),\"Fm\"] = \"P\"\n",
    "    pass\n",
    "\n",
    "recup_fuel_mode(data_train)\n",
    "recup_fuel_mode(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete columns\n",
    "\n",
    "Supprimer les colonnes avec 1 seul valeur unique (aucune info) ou 0 valeur unique (que des NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colonne supprimée: MMS\n",
      "colonne supprimée: r\n",
      "colonne supprimée: Ernedc (g/km)\n",
      "colonne supprimée: De\n",
      "colonne supprimée: Vf\n",
      "colonne supprimée: Status\n"
     ]
    }
   ],
   "source": [
    "valeurs_uniques = {}\n",
    "nombre_val_unique={}\n",
    "for col in data_train.columns:\n",
    "    valeurs_uniques[col]=data_train[col].unique().tolist()\n",
    "    nombre_val_unique[col]=data_train[col].nunique()\n",
    "\n",
    "for element in nombre_val_unique:\n",
    "    if nombre_val_unique[element]<=1:\n",
    "        print(f\"colonne supprimée: {element}\")\n",
    "        data_train.drop(columns=element, inplace=True)\n",
    "        data_test.drop(columns=element, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supprimer les colonnes avec **+ de 50%** de NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colonne supprimée: Enedc (g/km)\n"
     ]
    }
   ],
   "source": [
    "for col in data_train.columns:\n",
    "    if (data_train[col].isna().sum()/data_train.shape[0] > 0.5):\n",
    "        print(f\"colonne supprimée: {col}\")\n",
    "        data_train.drop(columns=col, inplace=True)\n",
    "        data_test.drop(columns=col, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supprimer les `Date` et `ID` (seulement pour train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colonne supprimée pour data_train: Date of registration, ID\n",
      "colonne supprimée pour data_test: Date of registration\n"
     ]
    }
   ],
   "source": [
    "data_train.drop(columns=['Date of registration','ID'], inplace=True)\n",
    "data_test.drop(columns='Date of registration', inplace=True)\n",
    "\n",
    "print(f\"colonne supprimée pour data_train: Date of registration, ID\")\n",
    "print(f\"colonne supprimée pour data_test: Date of registration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_categoricals = data_test.select_dtypes(include=\"object\").columns.tolist()\n",
    "col_numericals = [col for col in data_test.columns if col not in col_categoricals]\n",
    "col_numericals.remove(\"ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer les doublons en moyenne de Y\n",
    "\n",
    "En considérant qu'on a drop: `Date of registration`,`ID` on va prendre la moyenne des Y pour lesquelles les caractéristiques sont des doublons.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1163819, 26)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colus=list(data_train.columns)\n",
    "colus.remove('Ewltp (g/km)')\n",
    "duplicates = data_train.drop(columns=\"Ewltp (g/km)\").duplicated(keep=False)\n",
    "\n",
    "mean_target = data_train[duplicates].groupby(colus)['Ewltp (g/km)'].mean()\n",
    "\n",
    "# Supprimer les lignes en double et réindexer le DataFrame\n",
    "data_train = pd.concat([data_train[~duplicates], mean_target.reset_index()], ignore_index=True)\n",
    "data_train.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers \n",
    "\n",
    "Utilisation de l'écart interquartile pour identifier les valeurs aberrantes.\n",
    "\n",
    "Imputation des outliers:\n",
    "\n",
    "Fixer les valeurs aberrantes à un certain pourcentage (par exemple, 5e et 95e percentiles).\n",
    "\n",
    "**on pourrait aussi tenter d'imputer par la médiane si cela n'aboutit pas** \n",
    "\n",
    "### windorization of outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles={}\n",
    "\n",
    "def winsorize_outliers(data, column_name, lower_percentile=5, upper_percentile=95,train=True):\n",
    "    \"\"\"\n",
    "    Detects and imputes outliers using winsorizing for a specific column in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - data: Pandas DataFrame, input data\n",
    "    - column_name: str, name of the column to be winsorized\n",
    "    - lower_percentile: int, lower percentile for winsorizing (default: 5)\n",
    "    - upper_percentile: int, upper percentile for winsorizing (default: 95)\n",
    "\n",
    "    Returns:\n",
    "    - winsorized_data: Pandas DataFrame, data with outliers winsorized for the specified column\n",
    "    \"\"\"\n",
    "\n",
    "    column_data = data[column_name]\n",
    "    if train:\n",
    "        quantiles[\"q1\"] = np.percentile(column_data, lower_percentile)\n",
    "        quantiles[\"q3\"] = np.percentile(column_data, upper_percentile)\n",
    "        iqr = quantiles[\"q3\"] - quantiles[\"q1\"]\n",
    "        quantiles[\"lower_bound\"] = quantiles[\"q1\"] - 1.5 * iqr\n",
    "        quantiles[\"upper_bound\"] = quantiles[\"q3\"] + 1.5 * iqr\n",
    "\n",
    "    data[column_name] = np.clip(column_data, quantiles[\"lower_bound\"], quantiles[\"upper_bound\"])\n",
    "\n",
    "    return data\n",
    "\n",
    "for col in col_numericals:\n",
    "    data_train=winsorize_outliers(data_train,col)\n",
    "    data_test =winsorize_outliers(data_test,col,train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Median imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles={}\n",
    "\n",
    "def replace_outliers_with_median(data, column_name,train=True):\n",
    "    \"\"\"\n",
    "    Detects and replaces outliers with the median for a specific column in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - data: Pandas DataFrame, input data\n",
    "    - column_name: str, name of the column to be processed\n",
    "    - train: bool\n",
    "\n",
    "    Returns:\n",
    "    - data_with_median: Pandas DataFrame, data with outliers replaced by the median for the specified column\n",
    "    \"\"\"\n",
    "\n",
    "    column_data = data[column_name]\n",
    "\n",
    "    if train:\n",
    "        q1 = np.percentile(column_data,25)\n",
    "        q3 = np.percentile(column_data, 75)\n",
    "        iqr = q3 - q1\n",
    "        quantiles[\"lower_bound\"] = q1 - 1.5 * iqr\n",
    "        quantiles[\"upper_bound\"] = q3 + 1.5 * iqr\n",
    "        quantiles[column_name]=column_data.median()\n",
    "\n",
    "\n",
    "    outliers_mask = (column_data < quantiles[\"lower_bound\"]) | (column_data > quantiles[\"upper_bound\"])\n",
    "\n",
    "    data.loc[outliers_mask, column_name] = quantiles[column_name]\n",
    "    return data\n",
    "\n",
    "\n",
    "for col in col_numericals:\n",
    "    data_train = replace_outliers_with_median(data_train,col)\n",
    "    data_test =replace_outliers_with_median(data_test,col,train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute NaN by median/mode\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputers={}\n",
    "_coefficient_variation= lambda series : series.std()/series.mean()\n",
    "\n",
    "def fill_missing_values(colname : str,data:pd.DataFrame) -> None:\n",
    "    \n",
    "    if data[colname].dtype in [\"float64\"]:\n",
    "        if _coefficient_variation(data[colname]) > 0.15 :\n",
    "            imputers[colname]=SimpleImputer(missing_values=np.nan,strategy=\"median\")\n",
    "        else:\n",
    "            imputers[colname]=SimpleImputer(missing_values=np.nan,strategy=\"mean\")\n",
    "    else:\n",
    "        imputers[colname]=SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "    imputers[colname].fit(data[colname].to_numpy().reshape(-1,1))\n",
    "    pass\n",
    "\n",
    "for col in data_test.columns[1:]:\n",
    "    fill_missing_values(col,data_train)\n",
    "    data_train[col]=pd.Series(imputers[col].transform(data_train[col].to_numpy().reshape(-1,1)).flatten())\n",
    "    data_test[col]=pd.Series(imputers[col].transform(data_test[col].to_numpy().reshape(-1,1)).flatten())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "group imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonne Country de data_train est imputée\n",
      "Colonne Country de data_test est imputée\n",
      "Colonne VFN de data_train est imputée\n",
      "Colonne VFN de data_test est imputée\n",
      "Colonne Mp de data_train est imputée\n",
      "Colonne Mp de data_test est imputée\n",
      "Colonne Mh de data_train est imputée\n",
      "Colonne Mh de data_test est imputée\n",
      "Colonne Man de data_train est imputée\n",
      "Colonne Man de data_test est imputée\n",
      "Colonne Tan de data_train est imputée\n",
      "Colonne Tan de data_test est imputée\n",
      "Colonne T de data_train est imputée\n",
      "Colonne T de data_test est imputée\n",
      "Colonne Va de data_train est imputée\n",
      "Colonne Va de data_test est imputée\n",
      "Colonne Ve de data_train est imputée\n",
      "Colonne Ve de data_test est imputée\n",
      "Colonne Mk de data_train est imputée\n",
      "Colonne Mk de data_test est imputée\n",
      "Colonne Cn de data_train est imputée\n",
      "Colonne Cn de data_test est imputée\n",
      "Colonne Ct de data_train est imputée\n",
      "Colonne Ct de data_test est imputée\n",
      "Colonne Cr de data_train est imputée\n",
      "Colonne Cr de data_test est imputée\n",
      "Colonne m (kg) de data_train est imputée\n",
      "Colonne m (kg) de data_test est imputée\n",
      "Colonne Mt de data_train est imputée\n",
      "Colonne Mt de data_test est imputée\n",
      "Colonne W (mm) de data_train est imputée\n",
      "Colonne W (mm) de data_test est imputée\n",
      "Colonne At1 (mm) de data_train est imputée\n",
      "Colonne At1 (mm) de data_test est imputée\n",
      "Colonne At2 (mm) de data_train est imputée\n",
      "Colonne At2 (mm) de data_test est imputée\n",
      "Colonne Ft de data_train est imputée\n",
      "Colonne Ft de data_test est imputée\n",
      "Colonne Fm de data_train est imputée\n",
      "Colonne Fm de data_test est imputée\n",
      "Colonne ec (cm3) de data_train est imputée\n",
      "Colonne ec (cm3) de data_test est imputée\n",
      "Colonne ep (KW) de data_train est imputée\n",
      "Colonne ep (KW) de data_test est imputée\n",
      "Colonne IT de data_train est imputée\n",
      "Colonne IT de data_test est imputée\n",
      "Colonne Erwltp (g/km) de data_train est imputée\n",
      "Colonne Erwltp (g/km) de data_test est imputée\n",
      "Colonne Date of registration de data_train est imputée\n",
      "Colonne Date of registration de data_test est imputée\n",
      "Colonne Fuel consumption  de data_train est imputée\n",
      "Colonne Fuel consumption  de data_test est imputée\n"
     ]
    }
   ],
   "source": [
    "imputers={}\n",
    "\n",
    "def mean_group_imputer(col,df,train=True):\n",
    "    if (df[col].isna().sum()/df[col].shape[0] >0.5) or (df[col].isna().sum()/df[col].shape[0] ==1): #data_train[col].isna().sum()/data_train.shape[0] > 0.5\n",
    "        pass\n",
    "    else:\n",
    "        if df[col].dtype in [\"float64\"]: # numérique\n",
    "            if train:\n",
    "                imputers[col]={}\n",
    "                for energy in list(df[\"Ft\"].unique()):\n",
    "                    imputers[col][energy]=(df.loc[(~df[col].isna())& (df[\"Ft\"]==energy),col].median())\n",
    "                    df.loc[(df[col].isna())& (df[\"Ft\"]==energy),col]= imputers[col][energy]\n",
    "            else:\n",
    "                for energy in list(df[\"Ft\"].unique()):\n",
    "                    df.loc[(df[col].isna())& (df[\"Ft\"]==energy),col]= imputers[col][energy]\n",
    "        else:\n",
    "            if train:\n",
    "                imputers[col]={}\n",
    "                for energy in list(df[\"Ft\"].unique()):\n",
    "                    try:\n",
    "                        imputers[col][energy]=(df.loc[(~df[col].isna())& (df[\"Ft\"]==energy),col].mode()).iloc[0]\n",
    "                        df.loc[(df[col].isna())& (df[\"Ft\"]==energy),col]= imputers[col][energy]\n",
    "                    except:\n",
    "                        imputers[col][energy]=\"UNKNOWN\"\n",
    "                        df.loc[(df[col].isna())& (df[\"Ft\"]==energy),col]= imputers[col][energy]\n",
    "            else:\n",
    "                for energy in list(df[\"Ft\"].unique()):\n",
    "                    df.loc[(df[col].isna())& (df[\"Ft\"]==energy),col]= imputers[col][energy]\n",
    "        print(f\"Colonne {col} de {df.name} est imputée\")\n",
    "        pass\n",
    "\n",
    "for col in data_test.columns[1:]:\n",
    "    mean_group_imputer(col,data_train)\n",
    "    mean_group_imputer(col,data_test,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode categorical columns\n",
    "\n",
    "Many choices:\n",
    "- Customized encoding\n",
    "- Label/Ordinal encoding\n",
    "- Target encoding\n",
    "- Impact encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customized encoding. \n",
    "\n",
    "- Count Encoder (nunique >=15)\n",
    "- OHE Encoder (nunique < 15) rajoute 35 colonnes \n",
    "```\n",
    "Mp 10  valeurs uniques\n",
    "Ct 5  valeurs uniques\n",
    "Cr 3  valeurs uniques\n",
    "Ft 11  valeurs uniques\n",
    "Fm 6  valeurs uniques\n",
    "```\n",
    "- Catboost Encoder (nunique>=15) **USES THE TARGET TO FIT**\n",
    "```\n",
    "Country 29  valeurs uniques\n",
    "VFN 8456  valeurs uniques\n",
    "Mh 95  valeurs uniques\n",
    "Man 104  valeurs uniques\n",
    "Tan 6318  valeurs uniques\n",
    "T 1506  valeurs uniques\n",
    "Va 5413  valeurs uniques\n",
    "Ve 25570  valeurs uniques\n",
    "Mk 694  valeurs uniques\n",
    "Cn 8323  valeurs uniques\n",
    "IT 487  valeurs uniques\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding count : Country\n",
      "encoding count : VFN\n",
      "encoding OHE : Mp\n",
      "encoding count : Mh\n",
      "encoding count : Man\n",
      "encoding count : Tan\n",
      "encoding count : T\n",
      "encoding count : Va\n",
      "encoding count : Ve\n",
      "encoding count : Mk\n",
      "encoding count : Cn\n",
      "encoding OHE : Ct\n",
      "encoding OHE : Cr\n",
      "encoding OHE : Ft\n",
      "encoding OHE : Fm\n",
      "encoding count : IT\n"
     ]
    }
   ],
   "source": [
    "encoders = {}\n",
    "\n",
    "def cat_boost_encoder(col,df,train=True):\n",
    "    if train:\n",
    "        encoders[col]= CatBoostEncoder(random_state=42)\n",
    "        df[col]=encoders[col].fit_transform(df[[col]],df[['Ewltp (g/km)']])\n",
    "    else:\n",
    "        df[col]=encoders[col].transform(df[[col]])\n",
    "    pass\n",
    "\n",
    "def ohe_encoder(col,df,train=True):\n",
    "    if train:\n",
    "        encoders[col] = OneHotEncoder(sparse_output=False, drop='first',handle_unknown='ignore') #sparse = false sinn jsais pas gérer\n",
    "        ohe_features = encoders[col].fit_transform(df[[col]])\n",
    "    else: \n",
    "        ohe_features = encoders[col].transform(df[[col]])\n",
    "\n",
    "    ohe_features = pd.DataFrame(ohe_features, columns=encoders[col].get_feature_names_out([col]))\n",
    "\n",
    "    df.drop(columns=col, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df = pd.concat([df, ohe_features], axis=1)\n",
    "    return df\n",
    "\n",
    "def count_encoder(col, df, train=True):\n",
    "    if train:\n",
    "        encoders[col]=CountEncoder(handle_unknown='value')\n",
    "        df[col]=encoders[col].fit_transform(df[[col]])\n",
    "    else:\n",
    "        df[col]=encoders[col].transform(df[[col]])\n",
    "    pass\n",
    "\n",
    "def ordinal_encoder(colname:str,data:pd.DataFrame,train=True):\n",
    "    if train:\n",
    "        encoders[colname]=OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "        data[colname]=encoders[colname].fit_transform(data[[colname]])\n",
    "    else:\n",
    "        data[colname]=encoders[colname].transform(data[[colname]])\n",
    "    pass\n",
    "\n",
    "for col in col_categoricals:\n",
    "    if nombre_val_unique[col]>=15: #eventually replace by catboost encoder but careful cuz of Target ! (do TTS first)\n",
    "        count_encoder(col,data_train)\n",
    "        count_encoder(col,data_test,False)\n",
    "        print(f\"encoding count : {col}\")\n",
    "    else:\n",
    "        data_train=ohe_encoder(col,data_train) #reassign cuz you don't know how to do it...\n",
    "        data_test=ohe_encoder(col,data_test, False)\n",
    "        print(f\"encoding OHE : {col}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full Count encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding count : Country\n",
      "encoding count : VFN\n",
      "encoding count : Mp\n",
      "encoding count : Mh\n",
      "encoding count : Man\n",
      "encoding count : Tan\n",
      "encoding count : T\n",
      "encoding count : Va\n",
      "encoding count : Ve\n",
      "encoding count : Mk\n",
      "encoding count : Cn\n",
      "encoding count : Ct\n",
      "encoding count : Cr\n",
      "encoding count : Ft\n",
      "encoding count : Fm\n",
      "encoding count : IT\n"
     ]
    }
   ],
   "source": [
    "encoders={}\n",
    "def count_encoder(col, df, train=True):\n",
    "    if train:\n",
    "        encoders[col]=CountEncoder(handle_unknown='value')\n",
    "        df[col]=encoders[col].fit_transform(df[[col]])\n",
    "    else:\n",
    "        df[col]=encoders[col].transform(df[[col]])\n",
    "    pass\n",
    "\n",
    "for col in col_categoricals:\n",
    "    count_encoder(col,data_train)\n",
    "    count_encoder(col,data_test,train=False)\n",
    "    print(f\"encoding count : {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count + ordinal encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding count : Country\n",
      "encoding count : VFN\n",
      "encoding ordinal : Mp\n",
      "encoding count : Mh\n",
      "encoding count : Man\n",
      "encoding count : Tan\n",
      "encoding count : T\n",
      "encoding count : Va\n",
      "encoding count : Ve\n",
      "encoding count : Mk\n",
      "encoding count : Cn\n",
      "encoding ordinal : Ct\n",
      "encoding ordinal : Cr\n",
      "encoding ordinal : Ft\n",
      "encoding ordinal : Fm\n",
      "encoding count : IT\n"
     ]
    }
   ],
   "source": [
    "encoders={}\n",
    "\n",
    "def count_encoder(col, df, train=True):\n",
    "    if train:\n",
    "        encoders[col]=CountEncoder(handle_unknown='value')\n",
    "        df[col]=encoders[col].fit_transform(df[[col]])\n",
    "    else:\n",
    "        df[col]=encoders[col].transform(df[[col]])\n",
    "    pass\n",
    "\n",
    "def ordinal_encoder(colname:str,data:pd.DataFrame,train=True):\n",
    "    if train:\n",
    "        encoders[colname]=OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "        data[colname]=encoders[colname].fit_transform(data[[colname]])\n",
    "    else:\n",
    "        data[colname]=encoders[colname].transform(data[[colname]])\n",
    "    pass\n",
    "\n",
    "for col in col_categoricals:\n",
    "    if nombre_val_unique[col]>=15: #eventually replace by catboost encoder but careful cuz of Target ! (do TTS first)\n",
    "        count_encoder(col,data_train)\n",
    "        count_encoder(col,data_test,False)\n",
    "        print(f\"encoding count : {col}\")\n",
    "    else:\n",
    "        ordinal_encoder(col,data_train) #reassign cuz you don't know how to do it...\n",
    "        ordinal_encoder(col,data_test, False)\n",
    "        print(f\"encoding ordinal : {col}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label/Ordinal encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders={}\n",
    "def ordinal_encoder(colname:str,data:pd.DataFrame,train=True):\n",
    "    if train:\n",
    "        encoders[colname]=OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "        data[colname]=encoders[colname].fit_transform(data[[colname]])\n",
    "    else:\n",
    "        data[colname]=encoders[colname].transform(data[[colname]])\n",
    "\n",
    "\n",
    "for col in col_categoricals:\n",
    "    ordinal_encoder(col,data_train)\n",
    "    ordinal_encoder(col,data_test,train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders={}\n",
    "def target_encoder(colname:str,data:pd.DataFrame,train=True):\n",
    "    if train:\n",
    "        encoders[colname]=TargetEncoder(target_type='continuous', smooth='auto',random_state=42)\n",
    "        data[colname]=encoders[colname].fit_transform(data[[colname]],data['Ewltp (g/km)'])\n",
    "        pass\n",
    "    else:\n",
    "        data[colname]=encoders[colname].transform(data[[colname]])\n",
    "        pass\n",
    "\n",
    "for col in col_categoricals:\n",
    "    target_encoder(col,data_train)\n",
    "    target_encoder(col,data_test,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impact Encoding\n",
    "\n",
    "Proposé par Sam B. J'ai utilisé Chat GPT pour l'implémenter honnêtement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train, test = train_test_split(data_train, test_size=0.33, random_state=42)\n",
    "\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X_train, y_train = train.drop(columns=[\"Ewltp (g/km)\"]), train[\"Ewltp (g/km)\"]\n",
    "X_test, y_test = test.drop(columns=[\"Ewltp (g/km)\"]), test[\"Ewltp (g/km)\"]\n",
    "\n",
    "encoders={}\n",
    "def impact_encoder(colname:str,data:pd.DataFrame,target=None,train=True):\n",
    "    if train:\n",
    "        encoders[colname]=LeaveOneOutEncoder(handle_unknown=\"value\")\n",
    "        data[colname]=encoders[colname].fit_transform(data[[colname]],target)\n",
    "        pass\n",
    "    else:\n",
    "        data[colname]=encoders[colname].transform(data[[colname]])\n",
    "        pass\n",
    "\n",
    "for col in col_categoricals:\n",
    "    impact_encoder(col,X_train,target=y_train)\n",
    "    impact_encoder(col,X_test,train=False)\n",
    "    impact_encoder(col,data_test,train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Doit-on utiliser ces variables ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conforme(df):\n",
    "    df['conforme'] = df['Tan'].isna()\n",
    "    df['conforme'] = df['conforme'].apply(lambda x: 1 if x==False else 0)\n",
    "    pass\n",
    "\n",
    "create_conforme(data_train)\n",
    "create_conforme(data_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_surface(obs):\n",
    "    max_largeur= max(obs['At1 (mm)'], obs['At2 (mm)'])\n",
    "    return obs['W (mm)']*obs['At1 (mm)'] if max_largeur == obs['At1 (mm)'] else obs['W (mm)'] * obs['At2 (mm)']\n",
    "\n",
    "def create_surface(df):\n",
    "    df['surface']= df.apply(compute_surface, axis=1)\n",
    "    pass\n",
    "\n",
    "create_surface(data_train)\n",
    "create_surface(data_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data_train, test_size=0.33, random_state=42)\n",
    "\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X_train, y_train = train.drop(columns=[\"Ewltp (g/km)\"]), train[\"Ewltp (g/km)\"]\n",
    "X_test, y_test = test.drop(columns=[\"Ewltp (g/km)\"]), test[\"Ewltp (g/km)\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest 1\n",
    "\n",
    "Use all columns except those dropped (MMS, r, Ernedc (g/km), De,Vf,StatusEnedc (g/km),z (Wh/km),Electric range (km),Date of registration). \n",
    "\n",
    "ordinal encoding. Strandard fill NaN. Random forest. No feature Engineering.\n",
    "\n",
    "Computation time: 41min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 2.8986731992878796\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "y_pred = random_forest.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[\"Ewltp (g/km)\"] = random_forest.predict(data_test.drop(columns='ID'))\n",
    "data_test[[\"ID\",\"Ewltp (g/km)\"]].to_csv(\"data/new_simple_rf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['random_forest_simple_model.joblib']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(random_forest, 'models/random_forest_simple_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest 2\n",
    "\n",
    "Use all columns except those dropped (MMS, r, Ernedc (g/km), De,Vf,StatusEnedc (g/km),z (Wh/km),Electric range (km),Date of registration). \n",
    "\n",
    "**target** encoding. Strandard fill NaN. Random forest. No feature Engineering.\n",
    "\n",
    "Computation time: 23min (Home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 2.986984486605863\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "y_pred = random_forest.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[\"Ewltp (g/km)\"] = random_forest.predict(data_test.drop(columns='ID'))\n",
    "data_test[[\"ID\",\"Ewltp (g/km)\"]].to_csv(\"results/new_target_rf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest 3\n",
    "\n",
    "Use all columns except those dropped (MMS, r, Ernedc (g/km), De,Vf,StatusEnedc (g/km),z (Wh/km),Electric range (km),Date of registration). \n",
    "\n",
    "**impact** encoding. Strandard fill NaN. Random forest. No feature Engineering.\n",
    "\n",
    "Computation time: 22 (Home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 26.539493069959914\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "y_pred = random_forest.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[\"Ewltp (g/km)\"] = random_forest.predict(data_test.drop(columns='ID'))\n",
    "data_test[[\"ID\",\"Ewltp (g/km)\"]].to_csv(\"results/new_impact_rf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest 4\n",
    "\n",
    "Use all columns except those dropped (MMS, r, Ernedc (g/km), De,Vf,StatusEnedc (g/km),z (Wh/km),Electric range (km),Date of registration). \n",
    "\n",
    "ordinal encoding. Strandard fill NaN. Random forest. No feature Engineering. **use mean of Y**\n",
    "\n",
    "Computation time: 1.33min (Home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 3.6239323258632914\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "y_pred = random_forest.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[\"Ewltp (g/km)\"] = random_forest.predict(data_test.drop(columns='ID'))\n",
    "data_test[[\"ID\",\"Ewltp (g/km)\"]].to_csv(\"results/new_mean_rf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest 5\n",
    "\n",
    "Use all columns except those dropped (MMS, r, Ernedc (g/km), De,Vf,StatusEnedc (g/km),z (Wh/km),Electric range (km),Date of registration). \n",
    "\n",
    "ordinal encoding. Strandard fill NaN. Random forest. No feature Engineering. **feature engineering variables (conformes, surface)**\n",
    "\n",
    "Computation time: 23min (Home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 2.8998117328283994\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "y_pred = random_forest.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[\"Ewltp (g/km)\"] = random_forest.predict(data_test.drop(columns='ID'))\n",
    "data_test[[\"ID\",\"Ewltp (g/km)\"]].to_csv(\"results/new_fe_rf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest 6\n",
    "\n",
    "Récupération des NaN avec les colonnes électriques etc. \n",
    "\n",
    "ordinal encoder, outliers winsorize, median/mode impute. no feature engineering.\n",
    "\n",
    "computing time : 23(Home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 2.8953469493264583\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "y_pred = random_forest.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[\"Ewltp (g/km)\"] = random_forest.predict(data_test.drop(columns='ID'))\n",
    "data_test[[\"ID\",\"Ewltp (g/km)\"]].to_csv(\"results/new_all_col_rf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest 7\n",
    "\n",
    "Récupération des NaN avec les colonnes électriques etc. \n",
    "\n",
    "ordinal encoder, outliers **median**, median/mode impute. no feature engineering.\n",
    "\n",
    "computing time : 24min(Home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 2.8953469493264583\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "y_pred = random_forest.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[\"Ewltp (g/km)\"] = random_forest.predict(data_test.drop(columns='ID'))\n",
    "data_test[[\"ID\",\"Ewltp (g/km)\"]].to_csv(\"results/new_all_col_outliers_med_rf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest 8\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m random_forest \u001b[38;5;241m=\u001b[39m RandomForestRegressor(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,criterion\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabsolute_error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mrandom_forest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m random_forest\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m      7\u001b[0m mae \u001b[38;5;241m=\u001b[39m mean_absolute_error(y_test, y_pred)\n",
      "File \u001b[1;32md:\\Yoan Python\\.venv\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Yoan Python\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:456\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    445\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    448\u001b[0m ]\n\u001b[0;32m    450\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 456\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    471\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32md:\\Yoan Python\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Yoan Python\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Yoan Python\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\Yoan Python\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1707\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestRegressor(random_state=42, n_jobs=-1,criterion=\"absolute_error\")\n",
    "\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "y_pred = random_forest.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[\"Ewltp (g/km)\"] = random_forest.predict(data_test.drop(columns='ID'))\n",
    "data_test[[\"ID\",\"Ewltp (g/km)\"]].to_csv(\"results/new_abs_err_rf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging 1\n",
    "Use all columns except those dropped (MMS, r, Ernedc (g/km), De,Vf,StatusEnedc (g/km),z (Wh/km),Electric range (km),Date of registration). \n",
    "\n",
    "ordinal encoding. Strandard fill NaN. Random forest. No feature Engineering. estimator = DecistionTreeRegressor\n",
    "\n",
    "Computation time: 2,14min (Home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Yoan Python\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:1261: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 2.929723205821804\n"
     ]
    }
   ],
   "source": [
    "bag=BaggingRegressor(estimator=DecisionTreeRegressor(),n_jobs=-1,bootstrap=True,oob_score=True, random_state=42)\n",
    "\n",
    "bag.fit(X_train, y_train)\n",
    "\n",
    "y_pred = bag.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[\"Ewltp (g/km)\"] = bag.predict(data_test.drop(columns='ID'))\n",
    "data_test[[\"ID\",\"Ewltp (g/km)\"]].to_csv(\"results/new_tree_bagging.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_estimator': 'deprecated',\n",
       " 'bootstrap': True,\n",
       " 'bootstrap_features': False,\n",
       " 'estimator__ccp_alpha': 0.0,\n",
       " 'estimator__criterion': 'squared_error',\n",
       " 'estimator__max_depth': None,\n",
       " 'estimator__max_features': None,\n",
       " 'estimator__max_leaf_nodes': None,\n",
       " 'estimator__min_impurity_decrease': 0.0,\n",
       " 'estimator__min_samples_leaf': 1,\n",
       " 'estimator__min_samples_split': 2,\n",
       " 'estimator__min_weight_fraction_leaf': 0.0,\n",
       " 'estimator__random_state': None,\n",
       " 'estimator__splitter': 'best',\n",
       " 'estimator': DecisionTreeRegressor(),\n",
       " 'max_features': 1.0,\n",
       " 'max_samples': 1.0,\n",
       " 'n_estimators': 10,\n",
       " 'n_jobs': -1,\n",
       " 'oob_score': True,\n",
       " 'random_state': 42,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging 2\n",
    "Use all columns except those dropped (MMS, r, Ernedc (g/km), De,Vf,StatusEnedc (g/km),z (Wh/km),Electric range (km),Date of registration). \n",
    "\n",
    "ordinal encoding. Strandard fill NaN. Random forest. No feature Engineering. **n_estimator = 20**\n",
    "\n",
    "Computation time: 5min (Home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done   2 out of  20 | elapsed:  4.4min remaining: 39.8min\n",
      "[Parallel(n_jobs=20)]: Done   9 out of  20 | elapsed:  4.5min remaining:  5.5min\n",
      "[Parallel(n_jobs=20)]: Done  16 out of  20 | elapsed:  4.5min remaining:  1.1min\n",
      "[Parallel(n_jobs=20)]: Done  20 out of  20 | elapsed:  4.5min finished\n",
      "d:\\Yoan Python\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:1261: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\n",
      "[Parallel(n_jobs=20)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done   2 out of  20 | elapsed:    5.4s remaining:   49.1s\n",
      "[Parallel(n_jobs=20)]: Done   9 out of  20 | elapsed:    5.9s remaining:    7.2s\n",
      "[Parallel(n_jobs=20)]: Done  16 out of  20 | elapsed:    6.3s remaining:    1.5s\n",
      "[Parallel(n_jobs=20)]: Done  20 out of  20 | elapsed:    6.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 2.9137636972220546\n"
     ]
    }
   ],
   "source": [
    "bag=BaggingRegressor(estimator=DecisionTreeRegressor(),n_jobs=-1,bootstrap=True,oob_score=True, random_state=42,verbose=3,n_estimators=20)\n",
    "\n",
    "bag.fit(X_train, y_train)\n",
    "\n",
    "y_pred = bag.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done   2 out of  20 | elapsed:    1.8s remaining:   17.0s\n",
      "[Parallel(n_jobs=20)]: Done   9 out of  20 | elapsed:    2.6s remaining:    3.3s\n",
      "[Parallel(n_jobs=20)]: Done  16 out of  20 | elapsed:    3.1s remaining:    0.7s\n",
      "[Parallel(n_jobs=20)]: Done  20 out of  20 | elapsed:    3.3s finished\n"
     ]
    }
   ],
   "source": [
    "data_test[\"Ewltp (g/km)\"] = bag.predict(data_test.drop(columns='ID'))\n",
    "data_test[[\"ID\",\"Ewltp (g/km)\"]].to_csv(\"results/new_20_estim_bagging.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging 3\n",
    "Use all columns except those dropped (MMS, r, Ernedc (g/km), De,Vf,StatusEnedc (g/km),z (Wh/km),Electric range (km),Date of registration). \n",
    "\n",
    "ordinal encoding. Strandard fill NaN. Random forest. No feature Engineering. **n_estimator = 30**\n",
    "\n",
    "Computation time: 7min (Home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=24)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=24)]: Done   4 out of  24 | elapsed:  5.3min remaining: 26.7min\n",
      "[Parallel(n_jobs=24)]: Done  13 out of  24 | elapsed:  5.5min remaining:  4.7min\n",
      "[Parallel(n_jobs=24)]: Done  22 out of  24 | elapsed:  6.7min remaining:   36.2s\n",
      "[Parallel(n_jobs=24)]: Done  24 out of  24 | elapsed:  6.7min finished\n",
      "d:\\Yoan Python\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:1261: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\n",
      "[Parallel(n_jobs=24)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=24)]: Done   4 out of  24 | elapsed:    7.2s remaining:   36.2s\n",
      "[Parallel(n_jobs=24)]: Done  13 out of  24 | elapsed:    8.2s remaining:    6.9s\n",
      "[Parallel(n_jobs=24)]: Done  22 out of  24 | elapsed:    9.8s remaining:    0.8s\n",
      "[Parallel(n_jobs=24)]: Done  24 out of  24 | elapsed:    9.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 2.9082315288155627\n"
     ]
    }
   ],
   "source": [
    "bag=BaggingRegressor(estimator=DecisionTreeRegressor(),n_jobs=-1,bootstrap=True,oob_score=True, random_state=42,verbose=3,n_estimators= 30)\n",
    "\n",
    "bag.fit(X_train, y_train)\n",
    "\n",
    "y_pred = bag.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=24)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=24)]: Done   4 out of  24 | elapsed:    3.5s remaining:   18.1s\n",
      "[Parallel(n_jobs=24)]: Done  13 out of  24 | elapsed:    4.3s remaining:    3.6s\n",
      "[Parallel(n_jobs=24)]: Done  22 out of  24 | elapsed:    4.8s remaining:    0.3s\n",
      "[Parallel(n_jobs=24)]: Done  24 out of  24 | elapsed:    4.9s finished\n"
     ]
    }
   ],
   "source": [
    "data_test[\"Ewltp (g/km)\"] = bag.predict(data_test.drop(columns='ID'))\n",
    "data_test[[\"ID\",\"Ewltp (g/km)\"]].to_csv(\"results/new_30_estim_bagging.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging 4\n",
    "Use all columns except those dropped (MMS, r, Ernedc (g/km), De,Vf,StatusEnedc (g/km),z (Wh/km),Electric range (km),Date of registration). \n",
    "\n",
    "ordinal encoding. Strandard fill NaN. Random forest. No feature Engineering. **n_estimator = 50**\n",
    "\n",
    "Computation time: 13min (Home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=24)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=24)]: Done   4 out of  24 | elapsed: 10.8min remaining: 53.9min\n",
      "[Parallel(n_jobs=24)]: Done  13 out of  24 | elapsed: 11.0min remaining:  9.3min\n",
      "[Parallel(n_jobs=24)]: Done  22 out of  24 | elapsed: 11.2min remaining:  1.0min\n",
      "[Parallel(n_jobs=24)]: Done  24 out of  24 | elapsed: 11.8min finished\n",
      "[Parallel(n_jobs=24)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=24)]: Done   4 out of  24 | elapsed:   12.2s remaining:  1.0min\n",
      "[Parallel(n_jobs=24)]: Done  13 out of  24 | elapsed:   14.0s remaining:   11.9s\n",
      "[Parallel(n_jobs=24)]: Done  22 out of  24 | elapsed:   15.6s remaining:    1.3s\n",
      "[Parallel(n_jobs=24)]: Done  24 out of  24 | elapsed:   15.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 2.9028932743761215\n"
     ]
    }
   ],
   "source": [
    "bag=BaggingRegressor(estimator=DecisionTreeRegressor(),n_jobs=-1,bootstrap=True,oob_score=True, random_state=42,verbose=3,n_estimators= 50)\n",
    "\n",
    "bag.fit(X_train, y_train)\n",
    "\n",
    "y_pred = bag.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=24)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=24)]: Done   4 out of  24 | elapsed:    5.7s remaining:   28.8s\n",
      "[Parallel(n_jobs=24)]: Done  13 out of  24 | elapsed:    7.5s remaining:    6.4s\n",
      "[Parallel(n_jobs=24)]: Done  22 out of  24 | elapsed:    8.8s remaining:    0.7s\n",
      "[Parallel(n_jobs=24)]: Done  24 out of  24 | elapsed:    9.0s finished\n"
     ]
    }
   ],
   "source": [
    "data_test[\"Ewltp (g/km)\"] = bag.predict(data_test.drop(columns='ID'))\n",
    "data_test[[\"ID\",\"Ewltp (g/km)\"]].to_csv(\"results/new_50_estim_bagging.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging 5\n",
    "Use all columns except those dropped (MMS, r, Ernedc (g/km), De,Vf,StatusEnedc (g/km),z (Wh/km),Electric range (km),Date of registration). \n",
    "\n",
    "ordinal encoding. Strandard fill NaN. Random forest. No feature Engineering. **n_estimator = 10, bootstrap_features= True**\n",
    "\n",
    "Computation time: 2,15min (Home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done   3 out of  10 | elapsed:  1.7min remaining:  4.0min\n",
      "[Parallel(n_jobs=10)]: Done   7 out of  10 | elapsed:  1.8min remaining:   46.2s\n",
      "[Parallel(n_jobs=10)]: Done  10 out of  10 | elapsed:  2.0min finished\n",
      "d:\\Yoan Python\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:1261: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\n",
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done   3 out of  10 | elapsed:    2.9s remaining:    6.9s\n",
      "[Parallel(n_jobs=10)]: Done   7 out of  10 | elapsed:    3.2s remaining:    1.3s\n",
      "[Parallel(n_jobs=10)]: Done  10 out of  10 | elapsed:    3.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 2.94623303681266\n"
     ]
    }
   ],
   "source": [
    "bag=BaggingRegressor(estimator=DecisionTreeRegressor(),n_jobs=-1,bootstrap=True,oob_score=True, random_state=42,verbose=3,n_estimators= 10,bootstrap_features=True)\n",
    "\n",
    "bag.fit(X_train, y_train)\n",
    "\n",
    "y_pred = bag.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging 6\n",
    "Use all columns except those dropped (MMS, r, Ernedc (g/km), De,Vf,StatusEnedc (g/km),z (Wh/km),Electric range (km),Date of registration). \n",
    "\n",
    "ordinal encoding. Strandard fill NaN. Random forest. No feature Engineering. **n_estimator = 20, oob_score=False**\n",
    "\n",
    "Computation time: 4.35min (Home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done   2 out of  20 | elapsed:  4.4min remaining: 39.3min\n",
      "[Parallel(n_jobs=20)]: Done   9 out of  20 | elapsed:  4.4min remaining:  5.4min\n",
      "[Parallel(n_jobs=20)]: Done  16 out of  20 | elapsed:  4.4min remaining:  1.1min\n",
      "[Parallel(n_jobs=20)]: Done  20 out of  20 | elapsed:  4.5min finished\n",
      "[Parallel(n_jobs=20)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done   2 out of  20 | elapsed:    5.5s remaining:   50.5s\n",
      "[Parallel(n_jobs=20)]: Done   9 out of  20 | elapsed:    5.9s remaining:    7.2s\n",
      "[Parallel(n_jobs=20)]: Done  16 out of  20 | elapsed:    6.4s remaining:    1.5s\n",
      "[Parallel(n_jobs=20)]: Done  20 out of  20 | elapsed:    6.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 2.9137636972220546\n"
     ]
    }
   ],
   "source": [
    "bag=BaggingRegressor(estimator=DecisionTreeRegressor(),n_jobs=-1,bootstrap=True,oob_score=False, random_state=42,verbose=3,n_estimators= 20)\n",
    "\n",
    "bag.fit(X_train, y_train)\n",
    "\n",
    "y_pred = bag.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging 7\n",
    "\n",
    "Use all columns except those dropped (MMS, r, Ernedc (g/km), De,Vf,Status,Enedc (g/km),Date of registration). \n",
    "\n",
    "ordinal encoding. Strandard fill NaN. Random forest. No feature Engineering. **n_estimator = 100, oob_score=True**\n",
    "\n",
    "Computation time:  27min (Home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=24)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=24)]: Done   1 tasks      | elapsed: 22.7min\n",
      "[Parallel(n_jobs=24)]: Done   2 out of  24 | elapsed: 22.7min remaining: 249.9min\n",
      "[Parallel(n_jobs=24)]: Done   3 out of  24 | elapsed: 22.8min remaining: 159.4min\n",
      "[Parallel(n_jobs=24)]: Done   4 out of  24 | elapsed: 22.8min remaining: 114.0min\n",
      "[Parallel(n_jobs=24)]: Done   5 out of  24 | elapsed: 22.8min remaining: 86.8min\n",
      "[Parallel(n_jobs=24)]: Done   6 out of  24 | elapsed: 22.9min remaining: 68.7min\n",
      "[Parallel(n_jobs=24)]: Done   7 out of  24 | elapsed: 23.0min remaining: 55.9min\n",
      "[Parallel(n_jobs=24)]: Done   8 out of  24 | elapsed: 23.1min remaining: 46.3min\n",
      "[Parallel(n_jobs=24)]: Done   9 out of  24 | elapsed: 23.2min remaining: 38.6min\n",
      "[Parallel(n_jobs=24)]: Done  10 out of  24 | elapsed: 23.4min remaining: 32.8min\n",
      "[Parallel(n_jobs=24)]: Done  11 out of  24 | elapsed: 23.5min remaining: 27.7min\n",
      "[Parallel(n_jobs=24)]: Done  12 out of  24 | elapsed: 23.5min remaining: 23.5min\n",
      "[Parallel(n_jobs=24)]: Done  13 out of  24 | elapsed: 23.5min remaining: 19.9min\n",
      "[Parallel(n_jobs=24)]: Done  14 out of  24 | elapsed: 23.6min remaining: 16.8min\n",
      "[Parallel(n_jobs=24)]: Done  15 out of  24 | elapsed: 23.7min remaining: 14.2min\n",
      "[Parallel(n_jobs=24)]: Done  16 out of  24 | elapsed: 23.7min remaining: 11.8min\n",
      "[Parallel(n_jobs=24)]: Done  17 out of  24 | elapsed: 23.7min remaining:  9.8min\n",
      "[Parallel(n_jobs=24)]: Done  18 out of  24 | elapsed: 23.8min remaining:  7.9min\n",
      "[Parallel(n_jobs=24)]: Done  19 out of  24 | elapsed: 24.0min remaining:  6.3min\n",
      "[Parallel(n_jobs=24)]: Done  20 out of  24 | elapsed: 24.0min remaining:  4.8min\n",
      "[Parallel(n_jobs=24)]: Done  21 out of  24 | elapsed: 24.5min remaining:  3.5min\n",
      "[Parallel(n_jobs=24)]: Done  22 out of  24 | elapsed: 24.5min remaining:  2.2min\n",
      "[Parallel(n_jobs=24)]: Done  24 out of  24 | elapsed: 24.6min finished\n",
      "[Parallel(n_jobs=24)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=24)]: Done   1 tasks      | elapsed:   36.7s\n",
      "[Parallel(n_jobs=24)]: Done   2 out of  24 | elapsed:   36.8s remaining:  6.8min\n",
      "[Parallel(n_jobs=24)]: Done   3 out of  24 | elapsed:   37.5s remaining:  4.4min\n",
      "[Parallel(n_jobs=24)]: Done   4 out of  24 | elapsed:   37.7s remaining:  3.1min\n",
      "[Parallel(n_jobs=24)]: Done   5 out of  24 | elapsed:   38.7s remaining:  2.5min\n",
      "[Parallel(n_jobs=24)]: Done   6 out of  24 | elapsed:   38.8s remaining:  1.9min\n",
      "[Parallel(n_jobs=24)]: Done   7 out of  24 | elapsed:   38.9s remaining:  1.6min\n",
      "[Parallel(n_jobs=24)]: Done   8 out of  24 | elapsed:   39.0s remaining:  1.3min\n",
      "[Parallel(n_jobs=24)]: Done   9 out of  24 | elapsed:   39.1s remaining:  1.1min\n",
      "[Parallel(n_jobs=24)]: Done  10 out of  24 | elapsed:   39.5s remaining:   55.3s\n",
      "[Parallel(n_jobs=24)]: Done  11 out of  24 | elapsed:   40.1s remaining:   47.4s\n",
      "[Parallel(n_jobs=24)]: Done  12 out of  24 | elapsed:   41.3s remaining:   41.3s\n",
      "[Parallel(n_jobs=24)]: Done  13 out of  24 | elapsed:   41.4s remaining:   35.0s\n",
      "[Parallel(n_jobs=24)]: Done  14 out of  24 | elapsed:   41.4s remaining:   29.6s\n",
      "[Parallel(n_jobs=24)]: Done  15 out of  24 | elapsed:   41.9s remaining:   25.1s\n",
      "[Parallel(n_jobs=24)]: Done  16 out of  24 | elapsed:   42.1s remaining:   21.0s\n",
      "[Parallel(n_jobs=24)]: Done  17 out of  24 | elapsed:   43.0s remaining:   17.6s\n",
      "[Parallel(n_jobs=24)]: Done  18 out of  24 | elapsed:   43.9s remaining:   14.6s\n",
      "[Parallel(n_jobs=24)]: Done  19 out of  24 | elapsed:   45.1s remaining:   11.8s\n",
      "[Parallel(n_jobs=24)]: Done  20 out of  24 | elapsed:   46.4s remaining:    9.2s\n",
      "[Parallel(n_jobs=24)]: Done  21 out of  24 | elapsed:   47.4s remaining:    6.7s\n",
      "[Parallel(n_jobs=24)]: Done  22 out of  24 | elapsed:   48.7s remaining:    4.3s\n",
      "[Parallel(n_jobs=24)]: Done  24 out of  24 | elapsed:   50.1s finished\n",
      "Mean Absolute Error (MAE): 2.89536978640537\n"
     ]
    }
   ],
   "source": [
    "bag=BaggingRegressor(estimator=DecisionTreeRegressor(),n_jobs=-1,bootstrap=True,oob_score=True, random_state=42,verbose=100,n_estimators= 100)\n",
    "\n",
    "bag.fit(X_train, y_train)\n",
    "\n",
    "y_pred = bag.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=24)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=24)]: Done   1 tasks      | elapsed:    6.5s\n",
      "[Parallel(n_jobs=24)]: Done   2 out of  24 | elapsed:    7.2s remaining:  1.3min\n",
      "[Parallel(n_jobs=24)]: Done   3 out of  24 | elapsed:    8.6s remaining:  1.0min\n",
      "[Parallel(n_jobs=24)]: Done   4 out of  24 | elapsed:    9.4s remaining:   47.2s\n",
      "[Parallel(n_jobs=24)]: Done   5 out of  24 | elapsed:    9.8s remaining:   37.5s\n",
      "[Parallel(n_jobs=24)]: Done   6 out of  24 | elapsed:   10.6s remaining:   32.1s\n",
      "[Parallel(n_jobs=24)]: Done   7 out of  24 | elapsed:   11.5s remaining:   28.1s\n",
      "[Parallel(n_jobs=24)]: Done   8 out of  24 | elapsed:   12.3s remaining:   24.8s\n",
      "[Parallel(n_jobs=24)]: Done   9 out of  24 | elapsed:   13.3s remaining:   22.2s\n",
      "[Parallel(n_jobs=24)]: Done  10 out of  24 | elapsed:   13.7s remaining:   19.2s\n",
      "[Parallel(n_jobs=24)]: Done  11 out of  24 | elapsed:   14.7s remaining:   17.4s\n",
      "[Parallel(n_jobs=24)]: Done  12 out of  24 | elapsed:   15.1s remaining:   15.1s\n",
      "[Parallel(n_jobs=24)]: Done  13 out of  24 | elapsed:   16.0s remaining:   13.5s\n",
      "[Parallel(n_jobs=24)]: Done  14 out of  24 | elapsed:   16.9s remaining:   12.0s\n",
      "[Parallel(n_jobs=24)]: Done  15 out of  24 | elapsed:   17.8s remaining:   10.7s\n",
      "[Parallel(n_jobs=24)]: Done  16 out of  24 | elapsed:   18.7s remaining:    9.3s\n",
      "[Parallel(n_jobs=24)]: Done  17 out of  24 | elapsed:   19.7s remaining:    8.0s\n",
      "[Parallel(n_jobs=24)]: Done  18 out of  24 | elapsed:   20.7s remaining:    6.8s\n",
      "[Parallel(n_jobs=24)]: Done  19 out of  24 | elapsed:   21.4s remaining:    5.6s\n",
      "[Parallel(n_jobs=24)]: Done  20 out of  24 | elapsed:   21.8s remaining:    4.3s\n",
      "[Parallel(n_jobs=24)]: Done  21 out of  24 | elapsed:   22.3s remaining:    3.1s\n",
      "[Parallel(n_jobs=24)]: Done  22 out of  24 | elapsed:   22.6s remaining:    2.0s\n",
      "[Parallel(n_jobs=24)]: Done  24 out of  24 | elapsed:   23.0s finished\n"
     ]
    }
   ],
   "source": [
    "data_test[\"Ewltp (g/km)\"] = bag.predict(data_test.drop(columns='ID'))\n",
    "data_test[[\"ID\",\"Ewltp (g/km)\"]].to_csv(\"results/new_100_estim_bagging.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging 8\n",
    "Tests\n",
    "- 'max_samples': 0.8 meilleurs résultats que default (1)\n",
    "\n",
    "- 'max_features': 0.7 (0.8 et 0.5 baisse)\n",
    "17 min(home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=24)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=24)]: Done   1 tasks      | elapsed: 12.6min\n",
      "[Parallel(n_jobs=24)]: Done   2 out of  24 | elapsed: 12.6min remaining: 138.6min\n",
      "[Parallel(n_jobs=24)]: Done   3 out of  24 | elapsed: 13.2min remaining: 92.1min\n",
      "[Parallel(n_jobs=24)]: Done   4 out of  24 | elapsed: 13.4min remaining: 66.8min\n",
      "[Parallel(n_jobs=24)]: Done   5 out of  24 | elapsed: 13.4min remaining: 50.8min\n",
      "[Parallel(n_jobs=24)]: Done   6 out of  24 | elapsed: 13.5min remaining: 40.5min\n",
      "[Parallel(n_jobs=24)]: Done   7 out of  24 | elapsed: 13.6min remaining: 33.0min\n",
      "[Parallel(n_jobs=24)]: Done   8 out of  24 | elapsed: 13.6min remaining: 27.2min\n",
      "[Parallel(n_jobs=24)]: Done   9 out of  24 | elapsed: 13.8min remaining: 23.1min\n",
      "[Parallel(n_jobs=24)]: Done  10 out of  24 | elapsed: 13.9min remaining: 19.5min\n",
      "[Parallel(n_jobs=24)]: Done  11 out of  24 | elapsed: 13.9min remaining: 16.5min\n",
      "[Parallel(n_jobs=24)]: Done  12 out of  24 | elapsed: 14.1min remaining: 14.1min\n",
      "[Parallel(n_jobs=24)]: Done  13 out of  24 | elapsed: 14.1min remaining: 11.9min\n",
      "[Parallel(n_jobs=24)]: Done  14 out of  24 | elapsed: 14.1min remaining: 10.1min\n",
      "[Parallel(n_jobs=24)]: Done  15 out of  24 | elapsed: 14.1min remaining:  8.5min\n",
      "[Parallel(n_jobs=24)]: Done  16 out of  24 | elapsed: 14.1min remaining:  7.1min\n",
      "[Parallel(n_jobs=24)]: Done  17 out of  24 | elapsed: 14.2min remaining:  5.8min\n",
      "[Parallel(n_jobs=24)]: Done  18 out of  24 | elapsed: 14.2min remaining:  4.7min\n",
      "[Parallel(n_jobs=24)]: Done  19 out of  24 | elapsed: 14.2min remaining:  3.7min\n",
      "[Parallel(n_jobs=24)]: Done  20 out of  24 | elapsed: 14.3min remaining:  2.9min\n",
      "[Parallel(n_jobs=24)]: Done  21 out of  24 | elapsed: 14.3min remaining:  2.0min\n",
      "[Parallel(n_jobs=24)]: Done  22 out of  24 | elapsed: 14.4min remaining:  1.3min\n",
      "[Parallel(n_jobs=24)]: Done  24 out of  24 | elapsed: 14.5min finished\n",
      "[Parallel(n_jobs=24)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=24)]: Done   1 tasks      | elapsed:   15.8s\n",
      "[Parallel(n_jobs=24)]: Done   2 out of  24 | elapsed:   16.8s remaining:  3.1min\n",
      "[Parallel(n_jobs=24)]: Done   3 out of  24 | elapsed:   18.2s remaining:  2.1min\n",
      "[Parallel(n_jobs=24)]: Done   4 out of  24 | elapsed:   19.3s remaining:  1.6min\n",
      "[Parallel(n_jobs=24)]: Done   5 out of  24 | elapsed:   20.0s remaining:  1.3min\n",
      "[Parallel(n_jobs=24)]: Done   6 out of  24 | elapsed:   20.3s remaining:  1.0min\n",
      "[Parallel(n_jobs=24)]: Done   7 out of  24 | elapsed:   20.4s remaining:   49.8s\n",
      "[Parallel(n_jobs=24)]: Done   8 out of  24 | elapsed:   20.9s remaining:   42.0s\n",
      "[Parallel(n_jobs=24)]: Done   9 out of  24 | elapsed:   21.1s remaining:   35.3s\n",
      "[Parallel(n_jobs=24)]: Done  10 out of  24 | elapsed:   21.5s remaining:   30.1s\n",
      "[Parallel(n_jobs=24)]: Done  11 out of  24 | elapsed:   22.1s remaining:   26.2s\n",
      "[Parallel(n_jobs=24)]: Done  12 out of  24 | elapsed:   22.4s remaining:   22.4s\n",
      "[Parallel(n_jobs=24)]: Done  13 out of  24 | elapsed:   22.5s remaining:   19.1s\n",
      "[Parallel(n_jobs=24)]: Done  14 out of  24 | elapsed:   22.6s remaining:   16.1s\n",
      "[Parallel(n_jobs=24)]: Done  15 out of  24 | elapsed:   22.7s remaining:   13.6s\n",
      "[Parallel(n_jobs=24)]: Done  16 out of  24 | elapsed:   22.8s remaining:   11.3s\n",
      "[Parallel(n_jobs=24)]: Done  17 out of  24 | elapsed:   23.0s remaining:    9.4s\n",
      "[Parallel(n_jobs=24)]: Done  18 out of  24 | elapsed:   23.0s remaining:    7.6s\n",
      "[Parallel(n_jobs=24)]: Done  19 out of  24 | elapsed:   23.2s remaining:    6.0s\n",
      "[Parallel(n_jobs=24)]: Done  20 out of  24 | elapsed:   23.3s remaining:    4.6s\n",
      "[Parallel(n_jobs=24)]: Done  21 out of  24 | elapsed:   23.4s remaining:    3.3s\n",
      "[Parallel(n_jobs=24)]: Done  22 out of  24 | elapsed:   23.8s remaining:    2.1s\n",
      "[Parallel(n_jobs=24)]: Done  24 out of  24 | elapsed:   24.2s finished\n",
      "Mean Absolute Error (MAE): 2.8924736954634347\n"
     ]
    }
   ],
   "source": [
    "bag=BaggingRegressor(estimator=DecisionTreeRegressor(),n_jobs=-1,bootstrap=True,oob_score=True,\n",
    "                      random_state=42,verbose=100,n_estimators= 100,\n",
    "                      max_features=0.7, max_samples=0.8)\n",
    "\n",
    "bag.fit(X_train, y_train)\n",
    "\n",
    "y_pred = bag.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=24)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=24)]: Done   1 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=24)]: Done   2 out of  24 | elapsed:    5.6s remaining:  1.1min\n",
      "[Parallel(n_jobs=24)]: Done   3 out of  24 | elapsed:    6.4s remaining:   45.7s\n",
      "[Parallel(n_jobs=24)]: Done   4 out of  24 | elapsed:    6.9s remaining:   34.8s\n",
      "[Parallel(n_jobs=24)]: Done   5 out of  24 | elapsed:    7.5s remaining:   28.9s\n",
      "[Parallel(n_jobs=24)]: Done   6 out of  24 | elapsed:    7.7s remaining:   23.2s\n",
      "[Parallel(n_jobs=24)]: Done   7 out of  24 | elapsed:    8.0s remaining:   19.5s\n",
      "[Parallel(n_jobs=24)]: Done   8 out of  24 | elapsed:    8.4s remaining:   16.9s\n",
      "[Parallel(n_jobs=24)]: Done   9 out of  24 | elapsed:    8.5s remaining:   14.2s\n",
      "[Parallel(n_jobs=24)]: Done  10 out of  24 | elapsed:    8.7s remaining:   12.3s\n",
      "[Parallel(n_jobs=24)]: Done  11 out of  24 | elapsed:    8.8s remaining:   10.4s\n",
      "[Parallel(n_jobs=24)]: Done  12 out of  24 | elapsed:    9.9s remaining:    9.9s\n",
      "[Parallel(n_jobs=24)]: Done  13 out of  24 | elapsed:   10.0s remaining:    8.5s\n",
      "[Parallel(n_jobs=24)]: Done  14 out of  24 | elapsed:   10.2s remaining:    7.3s\n",
      "[Parallel(n_jobs=24)]: Done  15 out of  24 | elapsed:   10.5s remaining:    6.3s\n",
      "[Parallel(n_jobs=24)]: Done  16 out of  24 | elapsed:   10.7s remaining:    5.3s\n",
      "[Parallel(n_jobs=24)]: Done  17 out of  24 | elapsed:   11.1s remaining:    4.5s\n",
      "[Parallel(n_jobs=24)]: Done  18 out of  24 | elapsed:   11.2s remaining:    3.7s\n",
      "[Parallel(n_jobs=24)]: Done  19 out of  24 | elapsed:   11.2s remaining:    2.9s\n",
      "[Parallel(n_jobs=24)]: Done  20 out of  24 | elapsed:   11.3s remaining:    2.2s\n",
      "[Parallel(n_jobs=24)]: Done  21 out of  24 | elapsed:   11.3s remaining:    1.5s\n",
      "[Parallel(n_jobs=24)]: Done  22 out of  24 | elapsed:   11.8s remaining:    1.0s\n",
      "[Parallel(n_jobs=24)]: Done  24 out of  24 | elapsed:   11.8s finished\n"
     ]
    }
   ],
   "source": [
    "data_test[\"Ewltp (g/km)\"] = bag.predict(data_test.drop(columns='ID'))\n",
    "data_test[[\"ID\",\"Ewltp (g/km)\"]].to_csv(\"results/new_100_07_08_bagging.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging 9\n",
    "**n_estimators = 50**\n",
    "- 'max_samples': 0.8 meilleurs résultats que default (1)\n",
    "\n",
    "- 'max_features': 0.7 (0.8 et 0.5 baisse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=24)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=24)]: Done   1 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=24)]: Done   2 out of  24 | elapsed:  6.5min remaining: 71.6min\n",
      "[Parallel(n_jobs=24)]: Done   3 out of  24 | elapsed:  6.5min remaining: 45.8min\n",
      "[Parallel(n_jobs=24)]: Done   4 out of  24 | elapsed:  6.6min remaining: 32.8min\n",
      "[Parallel(n_jobs=24)]: Done   5 out of  24 | elapsed:  6.6min remaining: 25.1min\n",
      "[Parallel(n_jobs=24)]: Done   6 out of  24 | elapsed:  6.6min remaining: 19.8min\n",
      "[Parallel(n_jobs=24)]: Done   7 out of  24 | elapsed:  6.6min remaining: 16.1min\n",
      "[Parallel(n_jobs=24)]: Done   8 out of  24 | elapsed:  6.7min remaining: 13.3min\n",
      "[Parallel(n_jobs=24)]: Done   9 out of  24 | elapsed:  6.7min remaining: 11.1min\n",
      "[Parallel(n_jobs=24)]: Done  10 out of  24 | elapsed:  6.7min remaining:  9.4min\n",
      "[Parallel(n_jobs=24)]: Done  11 out of  24 | elapsed:  6.7min remaining:  8.0min\n",
      "[Parallel(n_jobs=24)]: Done  12 out of  24 | elapsed:  6.8min remaining:  6.8min\n",
      "[Parallel(n_jobs=24)]: Done  13 out of  24 | elapsed:  6.8min remaining:  5.7min\n",
      "[Parallel(n_jobs=24)]: Done  14 out of  24 | elapsed:  6.8min remaining:  4.9min\n",
      "[Parallel(n_jobs=24)]: Done  15 out of  24 | elapsed:  6.8min remaining:  4.1min\n",
      "[Parallel(n_jobs=24)]: Done  16 out of  24 | elapsed:  6.9min remaining:  3.4min\n",
      "[Parallel(n_jobs=24)]: Done  17 out of  24 | elapsed:  6.9min remaining:  2.8min\n",
      "[Parallel(n_jobs=24)]: Done  18 out of  24 | elapsed:  6.9min remaining:  2.3min\n",
      "[Parallel(n_jobs=24)]: Done  19 out of  24 | elapsed:  7.0min remaining:  1.8min\n",
      "[Parallel(n_jobs=24)]: Done  20 out of  24 | elapsed:  7.0min remaining:  1.4min\n",
      "[Parallel(n_jobs=24)]: Done  21 out of  24 | elapsed:  7.1min remaining:  1.0min\n",
      "[Parallel(n_jobs=24)]: Done  22 out of  24 | elapsed:  7.2min remaining:   39.0s\n",
      "[Parallel(n_jobs=24)]: Done  24 out of  24 | elapsed:  7.4min finished\n",
      "[Parallel(n_jobs=24)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=24)]: Done   1 tasks      | elapsed:    8.4s\n",
      "[Parallel(n_jobs=24)]: Done   2 out of  24 | elapsed:    8.4s remaining:  1.6min\n",
      "[Parallel(n_jobs=24)]: Done   3 out of  24 | elapsed:    8.6s remaining:  1.0min\n",
      "[Parallel(n_jobs=24)]: Done   4 out of  24 | elapsed:    8.7s remaining:   44.1s\n",
      "[Parallel(n_jobs=24)]: Done   5 out of  24 | elapsed:    8.9s remaining:   34.2s\n",
      "[Parallel(n_jobs=24)]: Done   6 out of  24 | elapsed:    9.7s remaining:   29.4s\n",
      "[Parallel(n_jobs=24)]: Done   7 out of  24 | elapsed:    9.8s remaining:   23.9s\n",
      "[Parallel(n_jobs=24)]: Done   8 out of  24 | elapsed:   10.0s remaining:   20.1s\n",
      "[Parallel(n_jobs=24)]: Done   9 out of  24 | elapsed:   10.4s remaining:   17.4s\n",
      "[Parallel(n_jobs=24)]: Done  10 out of  24 | elapsed:   10.5s remaining:   14.7s\n",
      "[Parallel(n_jobs=24)]: Done  11 out of  24 | elapsed:   10.5s remaining:   12.5s\n",
      "[Parallel(n_jobs=24)]: Done  12 out of  24 | elapsed:   10.6s remaining:   10.6s\n",
      "[Parallel(n_jobs=24)]: Done  13 out of  24 | elapsed:   10.8s remaining:    9.1s\n",
      "[Parallel(n_jobs=24)]: Done  14 out of  24 | elapsed:   10.8s remaining:    7.7s\n",
      "[Parallel(n_jobs=24)]: Done  15 out of  24 | elapsed:   10.8s remaining:    6.5s\n",
      "[Parallel(n_jobs=24)]: Done  16 out of  24 | elapsed:   10.9s remaining:    5.4s\n",
      "[Parallel(n_jobs=24)]: Done  17 out of  24 | elapsed:   10.9s remaining:    4.4s\n",
      "[Parallel(n_jobs=24)]: Done  18 out of  24 | elapsed:   11.1s remaining:    3.6s\n",
      "[Parallel(n_jobs=24)]: Done  19 out of  24 | elapsed:   11.2s remaining:    2.9s\n",
      "[Parallel(n_jobs=24)]: Done  20 out of  24 | elapsed:   11.2s remaining:    2.2s\n",
      "[Parallel(n_jobs=24)]: Done  21 out of  24 | elapsed:   11.2s remaining:    1.5s\n",
      "[Parallel(n_jobs=24)]: Done  22 out of  24 | elapsed:   11.3s remaining:    0.9s\n",
      "[Parallel(n_jobs=24)]: Done  24 out of  24 | elapsed:   12.0s finished\n",
      "Mean Absolute Error (MAE): 2.902106426038576\n"
     ]
    }
   ],
   "source": [
    "bag=BaggingRegressor(estimator=DecisionTreeRegressor(),n_jobs=-1,bootstrap=True,oob_score=True,\n",
    "                      random_state=42,verbose=100,n_estimators= 50,\n",
    "                      max_features=0.7, max_samples=0.8)\n",
    "\n",
    "bag.fit(X_train, y_train)\n",
    "\n",
    "y_pred = bag.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging 10\n",
    "\n",
    "- n_estimators = 100\n",
    "- 'max_samples': 0.7 meilleurs résultats que default (1)\n",
    "\n",
    "- 'max_features': 0.8 (0.8 et 0.5 baisse)\n",
    "\n",
    "computing time : 18min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done   1 tasks      | elapsed: 14.5min\n",
      "[Parallel(n_jobs=20)]: Done   2 out of  20 | elapsed: 14.8min remaining: 133.4min\n",
      "[Parallel(n_jobs=20)]: Done   3 out of  20 | elapsed: 14.9min remaining: 84.2min\n",
      "[Parallel(n_jobs=20)]: Done   4 out of  20 | elapsed: 14.9min remaining: 59.6min\n",
      "[Parallel(n_jobs=20)]: Done   5 out of  20 | elapsed: 14.9min remaining: 44.8min\n",
      "[Parallel(n_jobs=20)]: Done   6 out of  20 | elapsed: 15.0min remaining: 34.9min\n",
      "[Parallel(n_jobs=20)]: Done   7 out of  20 | elapsed: 15.0min remaining: 27.9min\n",
      "[Parallel(n_jobs=20)]: Done   8 out of  20 | elapsed: 15.0min remaining: 22.6min\n",
      "[Parallel(n_jobs=20)]: Done   9 out of  20 | elapsed: 15.1min remaining: 18.4min\n",
      "[Parallel(n_jobs=20)]: Done  10 out of  20 | elapsed: 15.1min remaining: 15.1min\n",
      "[Parallel(n_jobs=20)]: Done  11 out of  20 | elapsed: 15.1min remaining: 12.4min\n",
      "[Parallel(n_jobs=20)]: Done  12 out of  20 | elapsed: 15.1min remaining: 10.1min\n",
      "[Parallel(n_jobs=20)]: Done  13 out of  20 | elapsed: 15.1min remaining:  8.1min\n",
      "[Parallel(n_jobs=20)]: Done  14 out of  20 | elapsed: 15.2min remaining:  6.5min\n",
      "[Parallel(n_jobs=20)]: Done  15 out of  20 | elapsed: 15.2min remaining:  5.1min\n",
      "[Parallel(n_jobs=20)]: Done  16 out of  20 | elapsed: 15.2min remaining:  3.8min\n",
      "[Parallel(n_jobs=20)]: Done  17 out of  20 | elapsed: 15.2min remaining:  2.7min\n",
      "[Parallel(n_jobs=20)]: Done  18 out of  20 | elapsed: 15.2min remaining:  1.7min\n",
      "[Parallel(n_jobs=20)]: Done  20 out of  20 | elapsed: 15.3min finished\n",
      "[Parallel(n_jobs=20)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done   1 tasks      | elapsed:   20.0s\n",
      "[Parallel(n_jobs=20)]: Done   2 out of  20 | elapsed:   21.6s remaining:  3.3min\n",
      "[Parallel(n_jobs=20)]: Done   3 out of  20 | elapsed:   22.0s remaining:  2.1min\n",
      "[Parallel(n_jobs=20)]: Done   4 out of  20 | elapsed:   22.0s remaining:  1.5min\n",
      "[Parallel(n_jobs=20)]: Done   5 out of  20 | elapsed:   24.1s remaining:  1.2min\n",
      "[Parallel(n_jobs=20)]: Done   6 out of  20 | elapsed:   24.3s remaining:   56.8s\n",
      "[Parallel(n_jobs=20)]: Done   7 out of  20 | elapsed:   24.5s remaining:   45.5s\n",
      "[Parallel(n_jobs=20)]: Done   8 out of  20 | elapsed:   24.6s remaining:   37.0s\n",
      "[Parallel(n_jobs=20)]: Done   9 out of  20 | elapsed:   25.0s remaining:   30.6s\n",
      "[Parallel(n_jobs=20)]: Done  10 out of  20 | elapsed:   25.6s remaining:   25.6s\n",
      "[Parallel(n_jobs=20)]: Done  11 out of  20 | elapsed:   26.1s remaining:   21.4s\n",
      "[Parallel(n_jobs=20)]: Done  12 out of  20 | elapsed:   26.4s remaining:   17.6s\n",
      "[Parallel(n_jobs=20)]: Done  13 out of  20 | elapsed:   26.6s remaining:   14.3s\n",
      "[Parallel(n_jobs=20)]: Done  14 out of  20 | elapsed:   26.7s remaining:   11.4s\n",
      "[Parallel(n_jobs=20)]: Done  15 out of  20 | elapsed:   26.8s remaining:    8.9s\n",
      "[Parallel(n_jobs=20)]: Done  16 out of  20 | elapsed:   26.9s remaining:    6.6s\n",
      "[Parallel(n_jobs=20)]: Done  17 out of  20 | elapsed:   26.9s remaining:    4.7s\n",
      "[Parallel(n_jobs=20)]: Done  18 out of  20 | elapsed:   27.1s remaining:    2.9s\n",
      "[Parallel(n_jobs=20)]: Done  20 out of  20 | elapsed:   27.6s finished\n",
      "Mean Absolute Error (MAE): 2.8709543508285624\n"
     ]
    }
   ],
   "source": [
    "bag=BaggingRegressor(estimator=DecisionTreeRegressor(),n_jobs=20,bootstrap=True,oob_score=True,\n",
    "                      random_state=42,verbose=100, max_features=0.8, max_samples=0.7,n_estimators=100)\n",
    "\n",
    "bag.fit(X_train, y_train)\n",
    "\n",
    "y_pred = bag.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done   1 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=20)]: Done   2 out of  20 | elapsed:    8.2s remaining:  1.2min\n",
      "[Parallel(n_jobs=20)]: Done   3 out of  20 | elapsed:    8.4s remaining:   47.9s\n",
      "[Parallel(n_jobs=20)]: Done   4 out of  20 | elapsed:    9.3s remaining:   37.6s\n",
      "[Parallel(n_jobs=20)]: Done   5 out of  20 | elapsed:   10.1s remaining:   30.5s\n",
      "[Parallel(n_jobs=20)]: Done   6 out of  20 | elapsed:   10.1s remaining:   23.8s\n",
      "[Parallel(n_jobs=20)]: Done   7 out of  20 | elapsed:   10.5s remaining:   19.6s\n",
      "[Parallel(n_jobs=20)]: Done   8 out of  20 | elapsed:   10.8s remaining:   16.3s\n",
      "[Parallel(n_jobs=20)]: Done   9 out of  20 | elapsed:   11.5s remaining:   14.0s\n",
      "[Parallel(n_jobs=20)]: Done  10 out of  20 | elapsed:   11.7s remaining:   11.7s\n",
      "[Parallel(n_jobs=20)]: Done  11 out of  20 | elapsed:   12.3s remaining:   10.1s\n",
      "[Parallel(n_jobs=20)]: Done  12 out of  20 | elapsed:   12.8s remaining:    8.5s\n",
      "[Parallel(n_jobs=20)]: Done  13 out of  20 | elapsed:   13.0s remaining:    6.9s\n",
      "[Parallel(n_jobs=20)]: Done  14 out of  20 | elapsed:   13.4s remaining:    5.7s\n",
      "[Parallel(n_jobs=20)]: Done  15 out of  20 | elapsed:   13.5s remaining:    4.4s\n",
      "[Parallel(n_jobs=20)]: Done  16 out of  20 | elapsed:   13.5s remaining:    3.3s\n",
      "[Parallel(n_jobs=20)]: Done  17 out of  20 | elapsed:   13.6s remaining:    2.3s\n",
      "[Parallel(n_jobs=20)]: Done  18 out of  20 | elapsed:   14.0s remaining:    1.5s\n",
      "[Parallel(n_jobs=20)]: Done  20 out of  20 | elapsed:   14.3s finished\n"
     ]
    }
   ],
   "source": [
    "data_test[\"Ewltp (g/km)\"] = bag.predict(data_test.drop(columns='ID'))\n",
    "data_test[[\"ID\",\"Ewltp (g/km)\"]].to_csv(\"results/new_100_08_07_bagging.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging 11\n",
    "\n",
    "- n_estimators = 100\n",
    "- max_features=**0.9**\n",
    "- max_samples=**0.9**\n",
    "\n",
    "computing time: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=15)]: Using backend LokyBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done   1 tasks      | elapsed: 16.5min\n",
      "[Parallel(n_jobs=15)]: Done   2 out of  15 | elapsed: 16.6min remaining: 107.8min\n",
      "[Parallel(n_jobs=15)]: Done   3 out of  15 | elapsed: 16.6min remaining: 66.5min\n",
      "[Parallel(n_jobs=15)]: Done   4 out of  15 | elapsed: 16.8min remaining: 46.1min\n",
      "[Parallel(n_jobs=15)]: Done   5 out of  15 | elapsed: 16.9min remaining: 33.7min\n",
      "[Parallel(n_jobs=15)]: Done   6 out of  15 | elapsed: 18.2min remaining: 27.3min\n",
      "[Parallel(n_jobs=15)]: Done   7 out of  15 | elapsed: 18.2min remaining: 20.8min\n",
      "[Parallel(n_jobs=15)]: Done   8 out of  15 | elapsed: 18.2min remaining: 16.0min\n",
      "[Parallel(n_jobs=15)]: Done   9 out of  15 | elapsed: 18.3min remaining: 12.2min\n",
      "[Parallel(n_jobs=15)]: Done  10 out of  15 | elapsed: 18.3min remaining:  9.1min\n",
      "[Parallel(n_jobs=15)]: Done  11 out of  15 | elapsed: 18.3min remaining:  6.7min\n",
      "[Parallel(n_jobs=15)]: Done  12 out of  15 | elapsed: 18.4min remaining:  4.6min\n",
      "[Parallel(n_jobs=15)]: Done  13 out of  15 | elapsed: 18.4min remaining:  2.8min\n",
      "[Parallel(n_jobs=15)]: Done  15 out of  15 | elapsed: 18.4min finished\n",
      "[Parallel(n_jobs=15)]: Using backend LokyBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done   1 tasks      | elapsed:   25.8s\n",
      "[Parallel(n_jobs=15)]: Done   2 out of  15 | elapsed:   27.6s remaining:  3.0min\n",
      "[Parallel(n_jobs=15)]: Done   3 out of  15 | elapsed:   29.4s remaining:  2.0min\n",
      "[Parallel(n_jobs=15)]: Done   4 out of  15 | elapsed:   29.7s remaining:  1.4min\n",
      "[Parallel(n_jobs=15)]: Done   5 out of  15 | elapsed:   29.8s remaining:   59.6s\n",
      "[Parallel(n_jobs=15)]: Done   6 out of  15 | elapsed:   29.8s remaining:   44.8s\n",
      "[Parallel(n_jobs=15)]: Done   7 out of  15 | elapsed:   29.9s remaining:   34.2s\n",
      "[Parallel(n_jobs=15)]: Done   8 out of  15 | elapsed:   30.2s remaining:   26.4s\n",
      "[Parallel(n_jobs=15)]: Done   9 out of  15 | elapsed:   30.6s remaining:   20.4s\n",
      "[Parallel(n_jobs=15)]: Done  10 out of  15 | elapsed:   31.1s remaining:   15.5s\n",
      "[Parallel(n_jobs=15)]: Done  11 out of  15 | elapsed:   31.1s remaining:   11.2s\n",
      "[Parallel(n_jobs=15)]: Done  12 out of  15 | elapsed:   31.4s remaining:    7.8s\n",
      "[Parallel(n_jobs=15)]: Done  13 out of  15 | elapsed:   31.7s remaining:    4.8s\n",
      "[Parallel(n_jobs=15)]: Done  15 out of  15 | elapsed:   32.6s finished\n",
      "Mean Absolute Error (MAE): 2.8615560500608073\n"
     ]
    }
   ],
   "source": [
    "bag=BaggingRegressor(estimator=DecisionTreeRegressor(),n_jobs=15,bootstrap=True,oob_score=True,\n",
    "                      random_state=42,verbose=100, max_features=0.9, max_samples=0.9, n_estimators=100)\n",
    "\n",
    "bag.fit(X_train, y_train)\n",
    "\n",
    "y_pred = bag.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=15)]: Using backend LokyBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done   1 tasks      | elapsed:    9.7s\n",
      "[Parallel(n_jobs=15)]: Done   2 out of  15 | elapsed:   11.5s remaining:  1.3min\n",
      "[Parallel(n_jobs=15)]: Done   3 out of  15 | elapsed:   11.9s remaining:   48.0s\n",
      "[Parallel(n_jobs=15)]: Done   4 out of  15 | elapsed:   13.1s remaining:   36.3s\n",
      "[Parallel(n_jobs=15)]: Done   5 out of  15 | elapsed:   13.3s remaining:   26.8s\n",
      "[Parallel(n_jobs=15)]: Done   6 out of  15 | elapsed:   13.9s remaining:   20.9s\n",
      "[Parallel(n_jobs=15)]: Done   7 out of  15 | elapsed:   14.0s remaining:   16.0s\n",
      "[Parallel(n_jobs=15)]: Done   8 out of  15 | elapsed:   15.2s remaining:   13.3s\n",
      "[Parallel(n_jobs=15)]: Done   9 out of  15 | elapsed:   15.6s remaining:   10.4s\n",
      "[Parallel(n_jobs=15)]: Done  10 out of  15 | elapsed:   15.8s remaining:    7.8s\n",
      "[Parallel(n_jobs=15)]: Done  11 out of  15 | elapsed:   15.9s remaining:    5.7s\n",
      "[Parallel(n_jobs=15)]: Done  12 out of  15 | elapsed:   16.0s remaining:    3.9s\n",
      "[Parallel(n_jobs=15)]: Done  13 out of  15 | elapsed:   16.1s remaining:    2.4s\n",
      "[Parallel(n_jobs=15)]: Done  15 out of  15 | elapsed:   16.9s finished\n"
     ]
    }
   ],
   "source": [
    "data_test[\"Ewltp (g/km)\"] = bag.predict(data_test.drop(columns='ID'))\n",
    "data_test[[\"ID\",\"Ewltp (g/km)\"]].to_csv(\"results/new_100_09_09_bagging.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging Blind\n",
    "\n",
    "9min (pc) \n",
    "\n",
    "MAE Kaggle: 2.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:  7.1min\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:  7.2min remaining:  7.2min\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:  8.7min finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yoanj\\Documents\\M2S1\\Projet_Mining\\.venv_mining\\lib\\site-packages\\sklearn\\ensemble\\_bagging.py:1261: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BaggingRegressor(estimator=DecisionTreeRegressor(), max_features=0.9,\n",
       "                 max_samples=0.9, n_jobs=4, oob_score=True, random_state=42,\n",
       "                 verbose=100)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BaggingRegressor</label><div class=\"sk-toggleable__content\"><pre>BaggingRegressor(estimator=DecisionTreeRegressor(), max_features=0.9,\n",
       "                 max_samples=0.9, n_jobs=4, oob_score=True, random_state=42,\n",
       "                 verbose=100)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "BaggingRegressor(estimator=DecisionTreeRegressor(), max_features=0.9,\n",
       "                 max_samples=0.9, n_jobs=4, oob_score=True, random_state=42,\n",
       "                 verbose=100)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag=BaggingRegressor(estimator=DecisionTreeRegressor(),n_jobs=4,bootstrap=True,oob_score=True,\n",
    "                      random_state=42,verbose=100, max_features=0.9, max_samples=0.9)\n",
    "\n",
    "bag.fit(data_train.drop(columns='Ewltp (g/km)'), data_train['Ewltp (g/km)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:   10.0s\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:   11.0s remaining:   11.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:   12.8s finished\n"
     ]
    }
   ],
   "source": [
    "data_test[\"Ewltp (g/km)\"] = bag.predict(data_test.drop(columns='ID'))\n",
    "data_test[[\"ID\",\"Ewltp (g/km)\"]].to_csv(\"data/new_100_09_09_blind_bagging.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging Blind full\n",
    "\n",
    "With 100 estimators. **MAE Kaggle 2.8320**\n",
    "\n",
    "26min(home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "bag=BaggingRegressor(estimator=DecisionTreeRegressor(),n_jobs=8,bootstrap=True,oob_score=True,\n",
    "                      random_state=42,verbose=100, max_features=0.9, max_samples=0.9, n_estimators=100)\n",
    "\n",
    "bag.fit(data_train.drop(columns='Ewltp (g/km)'), data_train['Ewltp (g/km)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done   1 tasks      | elapsed:   11.3s\n",
      "[Parallel(n_jobs=12)]: Done   2 out of  12 | elapsed:   15.8s remaining:  1.3min\n",
      "[Parallel(n_jobs=12)]: Done   3 out of  12 | elapsed:   16.9s remaining:   50.8s\n",
      "[Parallel(n_jobs=12)]: Done   4 out of  12 | elapsed:   17.2s remaining:   34.4s\n",
      "[Parallel(n_jobs=12)]: Done   5 out of  12 | elapsed:   17.3s remaining:   24.2s\n",
      "[Parallel(n_jobs=12)]: Done   6 out of  12 | elapsed:   17.6s remaining:   17.6s\n",
      "[Parallel(n_jobs=12)]: Done   7 out of  12 | elapsed:   19.0s remaining:   13.5s\n",
      "[Parallel(n_jobs=12)]: Done   8 out of  12 | elapsed:   19.1s remaining:    9.5s\n",
      "[Parallel(n_jobs=12)]: Done   9 out of  12 | elapsed:   20.0s remaining:    6.6s\n",
      "[Parallel(n_jobs=12)]: Done  10 out of  12 | elapsed:   21.0s remaining:    4.1s\n",
      "[Parallel(n_jobs=12)]: Done  12 out of  12 | elapsed:   22.5s finished\n"
     ]
    }
   ],
   "source": [
    "data_test[\"Ewltp (g/km)\"] = bag.predict(data_test.drop(columns='ID'))\n",
    "data_test[[\"ID\",\"Ewltp (g/km)\"]].to_csv(\"results/new_100_09_09_blind_cstm_encoder_bagging_full.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging Test\n",
    "\n",
    "normal preprocessing VS Mean_group_imputer preprocessing. \n",
    "\n",
    "MAE 2.88 vs 38.52 \n",
    "\n",
    "that's shit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done   1 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=10)]: Done   2 out of  10 | elapsed:  1.7min remaining:  6.7min\n",
      "[Parallel(n_jobs=10)]: Done   3 out of  10 | elapsed:  1.7min remaining:  3.9min\n",
      "[Parallel(n_jobs=10)]: Done   4 out of  10 | elapsed:  1.7min remaining:  2.5min\n",
      "[Parallel(n_jobs=10)]: Done   5 out of  10 | elapsed:  1.7min remaining:  1.7min\n",
      "[Parallel(n_jobs=10)]: Done   6 out of  10 | elapsed:  1.7min remaining:  1.1min\n",
      "[Parallel(n_jobs=10)]: Done   7 out of  10 | elapsed:  1.7min remaining:   43.8s\n",
      "[Parallel(n_jobs=10)]: Done   8 out of  10 | elapsed:  1.7min remaining:   25.6s\n",
      "[Parallel(n_jobs=10)]: Done  10 out of  10 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Yoan Python\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:1261: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done   1 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=10)]: Done   2 out of  10 | elapsed:    2.6s remaining:   10.6s\n",
      "[Parallel(n_jobs=10)]: Done   3 out of  10 | elapsed:    2.9s remaining:    6.9s\n",
      "[Parallel(n_jobs=10)]: Done   4 out of  10 | elapsed:    3.0s remaining:    4.6s\n",
      "[Parallel(n_jobs=10)]: Done   5 out of  10 | elapsed:    3.2s remaining:    3.2s\n",
      "[Parallel(n_jobs=10)]: Done   6 out of  10 | elapsed:    3.2s remaining:    2.1s\n",
      "[Parallel(n_jobs=10)]: Done   7 out of  10 | elapsed:    3.3s remaining:    1.3s\n",
      "[Parallel(n_jobs=10)]: Done   8 out of  10 | elapsed:    3.3s remaining:    0.8s\n",
      "[Parallel(n_jobs=10)]: Done  10 out of  10 | elapsed:    3.5s finished\n",
      "Mean Absolute Error (MAE): 2.8898037952154683\n"
     ]
    }
   ],
   "source": [
    "bag=BaggingRegressor(estimator=DecisionTreeRegressor(),n_jobs=15,bootstrap=True,oob_score=True,\n",
    "                      random_state=42,verbose=100, max_features=0.9, max_samples=0.9)\n",
    "\n",
    "bag.fit(X_train, y_train)\n",
    "\n",
    "y_pred = bag.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done   1 tasks      | elapsed:   19.1s\n",
      "[Parallel(n_jobs=10)]: Done   2 out of  10 | elapsed:   27.9s remaining:  1.9min\n",
      "[Parallel(n_jobs=10)]: Done   3 out of  10 | elapsed:   29.0s remaining:  1.1min\n",
      "[Parallel(n_jobs=10)]: Done   4 out of  10 | elapsed:   29.2s remaining:   43.9s\n",
      "[Parallel(n_jobs=10)]: Done   5 out of  10 | elapsed:   33.1s remaining:   33.1s\n",
      "[Parallel(n_jobs=10)]: Done   6 out of  10 | elapsed:   37.7s remaining:   25.1s\n",
      "[Parallel(n_jobs=10)]: Done   7 out of  10 | elapsed:   39.7s remaining:   16.9s\n",
      "[Parallel(n_jobs=10)]: Done   8 out of  10 | elapsed:   53.0s remaining:   13.2s\n",
      "[Parallel(n_jobs=10)]: Done  10 out of  10 | elapsed:  1.0min finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Yoan Python\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:1261: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done   1 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=10)]: Done   2 out of  10 | elapsed:    1.1s remaining:    4.5s\n",
      "[Parallel(n_jobs=10)]: Done   3 out of  10 | elapsed:    1.1s remaining:    2.7s\n",
      "[Parallel(n_jobs=10)]: Done   4 out of  10 | elapsed:    1.1s remaining:    1.7s\n",
      "[Parallel(n_jobs=10)]: Done   5 out of  10 | elapsed:    1.2s remaining:    1.2s\n",
      "[Parallel(n_jobs=10)]: Done   6 out of  10 | elapsed:    1.2s remaining:    0.8s\n",
      "[Parallel(n_jobs=10)]: Done   7 out of  10 | elapsed:    1.2s remaining:    0.5s\n",
      "[Parallel(n_jobs=10)]: Done   8 out of  10 | elapsed:    1.2s remaining:    0.2s\n",
      "[Parallel(n_jobs=10)]: Done  10 out of  10 | elapsed:    1.2s finished\n",
      "Mean Absolute Error (MAE): 38.52301057321326\n"
     ]
    }
   ],
   "source": [
    "bag=BaggingRegressor(estimator=DecisionTreeRegressor(),n_jobs=15,bootstrap=True,oob_score=True,\n",
    "                      random_state=42,verbose=100, max_features=0.9, max_samples=0.9)\n",
    "\n",
    "bag.fit(X_train, y_train)\n",
    "\n",
    "y_pred = bag.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging 12\n",
    "\n",
    "Test with newly encoded columns: count encoder + OHE\n",
    "\n",
    "- n_estimators = 100\n",
    "- max_features=0.9\n",
    "- max_samples=0.9\n",
    "- no n_jobs specified\n",
    "\n",
    "computing time: 69min(home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building estimator 1 of 100 for this parallel run (total 100)...\n",
      "Building estimator 2 of 100 for this parallel run (total 100)...\n",
      "Building estimator 3 of 100 for this parallel run (total 100)...\n",
      "Building estimator 4 of 100 for this parallel run (total 100)...\n",
      "Building estimator 5 of 100 for this parallel run (total 100)...\n",
      "Building estimator 6 of 100 for this parallel run (total 100)...\n",
      "Building estimator 7 of 100 for this parallel run (total 100)...\n",
      "Building estimator 8 of 100 for this parallel run (total 100)...\n",
      "Building estimator 9 of 100 for this parallel run (total 100)...\n",
      "Building estimator 10 of 100 for this parallel run (total 100)...\n",
      "Building estimator 11 of 100 for this parallel run (total 100)...\n",
      "Building estimator 12 of 100 for this parallel run (total 100)...\n",
      "Building estimator 13 of 100 for this parallel run (total 100)...\n",
      "Building estimator 14 of 100 for this parallel run (total 100)...\n",
      "Building estimator 15 of 100 for this parallel run (total 100)...\n",
      "Building estimator 16 of 100 for this parallel run (total 100)...\n",
      "Building estimator 17 of 100 for this parallel run (total 100)...\n",
      "Building estimator 18 of 100 for this parallel run (total 100)...\n",
      "Building estimator 19 of 100 for this parallel run (total 100)...\n",
      "Building estimator 20 of 100 for this parallel run (total 100)...\n",
      "Building estimator 21 of 100 for this parallel run (total 100)...\n",
      "Building estimator 22 of 100 for this parallel run (total 100)...\n",
      "Building estimator 23 of 100 for this parallel run (total 100)...\n",
      "Building estimator 24 of 100 for this parallel run (total 100)...\n",
      "Building estimator 25 of 100 for this parallel run (total 100)...\n",
      "Building estimator 26 of 100 for this parallel run (total 100)...\n",
      "Building estimator 27 of 100 for this parallel run (total 100)...\n",
      "Building estimator 28 of 100 for this parallel run (total 100)...\n",
      "Building estimator 29 of 100 for this parallel run (total 100)...\n",
      "Building estimator 30 of 100 for this parallel run (total 100)...\n",
      "Building estimator 31 of 100 for this parallel run (total 100)...\n",
      "Building estimator 32 of 100 for this parallel run (total 100)...\n",
      "Building estimator 33 of 100 for this parallel run (total 100)...\n",
      "Building estimator 34 of 100 for this parallel run (total 100)...\n",
      "Building estimator 35 of 100 for this parallel run (total 100)...\n",
      "Building estimator 36 of 100 for this parallel run (total 100)...\n",
      "Building estimator 37 of 100 for this parallel run (total 100)...\n",
      "Building estimator 38 of 100 for this parallel run (total 100)...\n",
      "Building estimator 39 of 100 for this parallel run (total 100)...\n",
      "Building estimator 40 of 100 for this parallel run (total 100)...\n",
      "Building estimator 41 of 100 for this parallel run (total 100)...\n",
      "Building estimator 42 of 100 for this parallel run (total 100)...\n",
      "Building estimator 43 of 100 for this parallel run (total 100)...\n",
      "Building estimator 44 of 100 for this parallel run (total 100)...\n",
      "Building estimator 45 of 100 for this parallel run (total 100)...\n",
      "Building estimator 46 of 100 for this parallel run (total 100)...\n",
      "Building estimator 47 of 100 for this parallel run (total 100)...\n",
      "Building estimator 48 of 100 for this parallel run (total 100)...\n",
      "Building estimator 49 of 100 for this parallel run (total 100)...\n",
      "Building estimator 50 of 100 for this parallel run (total 100)...\n",
      "Building estimator 51 of 100 for this parallel run (total 100)...\n",
      "Building estimator 52 of 100 for this parallel run (total 100)...\n",
      "Building estimator 53 of 100 for this parallel run (total 100)...\n",
      "Building estimator 54 of 100 for this parallel run (total 100)...\n",
      "Building estimator 55 of 100 for this parallel run (total 100)...\n",
      "Building estimator 56 of 100 for this parallel run (total 100)...\n",
      "Building estimator 57 of 100 for this parallel run (total 100)...\n",
      "Building estimator 58 of 100 for this parallel run (total 100)...\n",
      "Building estimator 59 of 100 for this parallel run (total 100)...\n",
      "Building estimator 60 of 100 for this parallel run (total 100)...\n",
      "Building estimator 61 of 100 for this parallel run (total 100)...\n",
      "Building estimator 62 of 100 for this parallel run (total 100)...\n",
      "Building estimator 63 of 100 for this parallel run (total 100)...\n",
      "Building estimator 64 of 100 for this parallel run (total 100)...\n",
      "Building estimator 65 of 100 for this parallel run (total 100)...\n",
      "Building estimator 66 of 100 for this parallel run (total 100)...\n",
      "Building estimator 67 of 100 for this parallel run (total 100)...\n",
      "Building estimator 68 of 100 for this parallel run (total 100)...\n",
      "Building estimator 69 of 100 for this parallel run (total 100)...\n",
      "Building estimator 70 of 100 for this parallel run (total 100)...\n",
      "Building estimator 71 of 100 for this parallel run (total 100)...\n",
      "Building estimator 72 of 100 for this parallel run (total 100)...\n",
      "Building estimator 73 of 100 for this parallel run (total 100)...\n",
      "Building estimator 74 of 100 for this parallel run (total 100)...\n",
      "Building estimator 75 of 100 for this parallel run (total 100)...\n",
      "Building estimator 76 of 100 for this parallel run (total 100)...\n",
      "Building estimator 77 of 100 for this parallel run (total 100)...\n",
      "Building estimator 78 of 100 for this parallel run (total 100)...\n",
      "Building estimator 79 of 100 for this parallel run (total 100)...\n",
      "Building estimator 80 of 100 for this parallel run (total 100)...\n",
      "Building estimator 81 of 100 for this parallel run (total 100)...\n",
      "Building estimator 82 of 100 for this parallel run (total 100)...\n",
      "Building estimator 83 of 100 for this parallel run (total 100)...\n",
      "Building estimator 84 of 100 for this parallel run (total 100)...\n",
      "Building estimator 85 of 100 for this parallel run (total 100)...\n",
      "Building estimator 86 of 100 for this parallel run (total 100)...\n",
      "Building estimator 87 of 100 for this parallel run (total 100)...\n",
      "Building estimator 88 of 100 for this parallel run (total 100)...\n",
      "Building estimator 89 of 100 for this parallel run (total 100)...\n",
      "Building estimator 90 of 100 for this parallel run (total 100)...\n",
      "Building estimator 91 of 100 for this parallel run (total 100)...\n",
      "Building estimator 92 of 100 for this parallel run (total 100)...\n",
      "Building estimator 93 of 100 for this parallel run (total 100)...\n",
      "Building estimator 94 of 100 for this parallel run (total 100)...\n",
      "Building estimator 95 of 100 for this parallel run (total 100)...\n",
      "Building estimator 96 of 100 for this parallel run (total 100)...\n",
      "Building estimator 97 of 100 for this parallel run (total 100)...\n",
      "Building estimator 98 of 100 for this parallel run (total 100)...\n",
      "Building estimator 99 of 100 for this parallel run (total 100)...\n",
      "Building estimator 100 of 100 for this parallel run (total 100)...\n",
      "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed: 63.6min\n",
      "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed: 63.6min\n",
      "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:  2.6min\n",
      "Mean Absolute Error (MAE): 2.873600306363281\n"
     ]
    }
   ],
   "source": [
    "bag=BaggingRegressor(estimator=DecisionTreeRegressor(),bootstrap=True,oob_score=True,\n",
    "                      random_state=42,verbose=100, max_features=0.9, max_samples=0.9, n_estimators=100)\n",
    "\n",
    "bag.fit(X_train, y_train)\n",
    "\n",
    "y_pred = bag.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:  1.0min\n"
     ]
    }
   ],
   "source": [
    "data_test[\"Ewltp (g/km)\"] = bag.predict(data_test.drop(columns='ID'))\n",
    "data_test[[\"ID\",\"Ewltp (g/km)\"]].to_csv(\"results/new_100_09_09_cstm_encoding_bagging.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging 13\n",
    "\n",
    "**Full count encoder**, same param as bagging 11\n",
    "\n",
    "n_jobs= 10\n",
    "\n",
    "computing time: 19min(home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done   1 tasks      | elapsed: 15.7min\n",
      "[Parallel(n_jobs=10)]: Done   2 out of  10 | elapsed: 15.7min remaining: 62.9min\n",
      "[Parallel(n_jobs=10)]: Done   3 out of  10 | elapsed: 15.7min remaining: 36.7min\n",
      "[Parallel(n_jobs=10)]: Done   4 out of  10 | elapsed: 15.8min remaining: 23.7min\n",
      "[Parallel(n_jobs=10)]: Done   5 out of  10 | elapsed: 15.9min remaining: 15.9min\n",
      "[Parallel(n_jobs=10)]: Done   6 out of  10 | elapsed: 15.9min remaining: 10.6min\n",
      "[Parallel(n_jobs=10)]: Done   7 out of  10 | elapsed: 16.0min remaining:  6.9min\n",
      "[Parallel(n_jobs=10)]: Done   8 out of  10 | elapsed: 16.0min remaining:  4.0min\n",
      "[Parallel(n_jobs=10)]: Done  10 out of  10 | elapsed: 16.1min finished\n",
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done   1 tasks      | elapsed:   28.2s\n",
      "[Parallel(n_jobs=10)]: Done   2 out of  10 | elapsed:   30.4s remaining:  2.0min\n",
      "[Parallel(n_jobs=10)]: Done   3 out of  10 | elapsed:   31.2s remaining:  1.2min\n",
      "[Parallel(n_jobs=10)]: Done   4 out of  10 | elapsed:   31.3s remaining:   47.0s\n",
      "[Parallel(n_jobs=10)]: Done   5 out of  10 | elapsed:   31.6s remaining:   31.6s\n",
      "[Parallel(n_jobs=10)]: Done   6 out of  10 | elapsed:   33.6s remaining:   22.4s\n",
      "[Parallel(n_jobs=10)]: Done   7 out of  10 | elapsed:   33.9s remaining:   14.5s\n",
      "[Parallel(n_jobs=10)]: Done   8 out of  10 | elapsed:   34.0s remaining:    8.4s\n",
      "[Parallel(n_jobs=10)]: Done  10 out of  10 | elapsed:   34.6s finished\n",
      "Mean Absolute Error (MAE): 2.869795319859196\n"
     ]
    }
   ],
   "source": [
    "bag=BaggingRegressor(estimator=DecisionTreeRegressor(),n_jobs=10,bootstrap=True,oob_score=True,\n",
    "                      random_state=42,verbose=100, max_features=0.9, max_samples=0.9, n_estimators=100)\n",
    "\n",
    "bag.fit(X_train, y_train)\n",
    "\n",
    "y_pred = bag.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done   1 tasks      | elapsed:   14.1s\n",
      "[Parallel(n_jobs=10)]: Done   2 out of  10 | elapsed:   15.3s remaining:  1.0min\n",
      "[Parallel(n_jobs=10)]: Done   3 out of  10 | elapsed:   15.7s remaining:   36.9s\n",
      "[Parallel(n_jobs=10)]: Done   4 out of  10 | elapsed:   16.0s remaining:   24.1s\n",
      "[Parallel(n_jobs=10)]: Done   5 out of  10 | elapsed:   16.5s remaining:   16.5s\n",
      "[Parallel(n_jobs=10)]: Done   6 out of  10 | elapsed:   17.6s remaining:   11.7s\n",
      "[Parallel(n_jobs=10)]: Done   7 out of  10 | elapsed:   17.9s remaining:    7.6s\n",
      "[Parallel(n_jobs=10)]: Done   8 out of  10 | elapsed:   18.5s remaining:    4.6s\n",
      "[Parallel(n_jobs=10)]: Done  10 out of  10 | elapsed:   19.5s finished\n"
     ]
    }
   ],
   "source": [
    "data_test[\"Ewltp (g/km)\"] = bag.predict(data_test.drop(columns='ID'))\n",
    "data_test[[\"ID\",\"Ewltp (g/km)\"]].to_csv(\"results/new_100_09_09_count_encod_bagging.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging 14\n",
    "\n",
    "**count + ordinal encoder**, same param as bagging 11\n",
    "\n",
    "n_jobs= 10\n",
    "\n",
    "computing time: 19min(home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done   1 tasks      | elapsed: 15.4min\n",
      "[Parallel(n_jobs=10)]: Done   2 out of  10 | elapsed: 15.5min remaining: 61.9min\n",
      "[Parallel(n_jobs=10)]: Done   3 out of  10 | elapsed: 15.5min remaining: 36.3min\n",
      "[Parallel(n_jobs=10)]: Done   4 out of  10 | elapsed: 15.7min remaining: 23.5min\n",
      "[Parallel(n_jobs=10)]: Done   5 out of  10 | elapsed: 15.7min remaining: 15.7min\n",
      "[Parallel(n_jobs=10)]: Done   6 out of  10 | elapsed: 15.8min remaining: 10.5min\n",
      "[Parallel(n_jobs=10)]: Done   7 out of  10 | elapsed: 15.8min remaining:  6.8min\n",
      "[Parallel(n_jobs=10)]: Done   8 out of  10 | elapsed: 15.8min remaining:  4.0min\n",
      "[Parallel(n_jobs=10)]: Done  10 out of  10 | elapsed: 15.8min finished\n",
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done   1 tasks      | elapsed:   28.4s\n",
      "[Parallel(n_jobs=10)]: Done   2 out of  10 | elapsed:   30.4s remaining:  2.0min\n",
      "[Parallel(n_jobs=10)]: Done   3 out of  10 | elapsed:   30.5s remaining:  1.2min\n",
      "[Parallel(n_jobs=10)]: Done   4 out of  10 | elapsed:   31.0s remaining:   46.6s\n",
      "[Parallel(n_jobs=10)]: Done   5 out of  10 | elapsed:   31.5s remaining:   31.5s\n",
      "[Parallel(n_jobs=10)]: Done   6 out of  10 | elapsed:   33.3s remaining:   22.2s\n",
      "[Parallel(n_jobs=10)]: Done   7 out of  10 | elapsed:   33.4s remaining:   14.3s\n",
      "[Parallel(n_jobs=10)]: Done   8 out of  10 | elapsed:   33.7s remaining:    8.3s\n",
      "[Parallel(n_jobs=10)]: Done  10 out of  10 | elapsed:   34.7s finished\n",
      "Mean Absolute Error (MAE): 2.869528908365036\n"
     ]
    }
   ],
   "source": [
    "bag=BaggingRegressor(estimator=DecisionTreeRegressor(),n_jobs=10,bootstrap=True,oob_score=True,\n",
    "                      random_state=42,verbose=100, max_features=0.9, max_samples=0.9, n_estimators=100)\n",
    "\n",
    "bag.fit(X_train, y_train)\n",
    "\n",
    "y_pred = bag.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done   1 tasks      | elapsed:   14.0s\n",
      "[Parallel(n_jobs=10)]: Done   2 out of  10 | elapsed:   15.1s remaining:  1.0min\n",
      "[Parallel(n_jobs=10)]: Done   3 out of  10 | elapsed:   15.7s remaining:   36.7s\n",
      "[Parallel(n_jobs=10)]: Done   4 out of  10 | elapsed:   15.9s remaining:   23.9s\n",
      "[Parallel(n_jobs=10)]: Done   5 out of  10 | elapsed:   16.8s remaining:   16.8s\n",
      "[Parallel(n_jobs=10)]: Done   6 out of  10 | elapsed:   17.7s remaining:   11.8s\n",
      "[Parallel(n_jobs=10)]: Done   7 out of  10 | elapsed:   17.7s remaining:    7.5s\n",
      "[Parallel(n_jobs=10)]: Done   8 out of  10 | elapsed:   18.3s remaining:    4.5s\n",
      "[Parallel(n_jobs=10)]: Done  10 out of  10 | elapsed:   19.4s finished\n"
     ]
    }
   ],
   "source": [
    "data_test[\"Ewltp (g/km)\"] = bag.predict(data_test.drop(columns='ID'))\n",
    "data_test[[\"ID\",\"Ewltp (g/km)\"]].to_csv(\"results/new_100_09_09_count_ordinal_encod_bagging.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 56.538170887529546\n"
     ]
    }
   ],
   "source": [
    "boosting = AdaBoostRegressor(random_state=42,loss='exponential')\n",
    "\n",
    "boosting.fit(X_train, y_train)\n",
    "\n",
    "y_pred = boosting.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[\"Ewltp (g/km)\"] = bag.predict(data_test.drop(columns='ID'))\n",
    "data_test[[\"ID\",\"Ewltp (g/km)\"]].to_csv(\"results/new_simple_boosting.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    }
   ],
   "source": [
    "decision_tree_model = DecisionTreeRegressor()\n",
    "\n",
    "param_grid = {\n",
    "    'criterion': ['absolute_error', 'poisson', 'squared_error'],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]}\n",
    "\n",
    "grid_search = GridSearchCV(decision_tree_model, param_grid, cv=5, scoring='neg_mean_absolute_error',verbose=3)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Meilleurs hyperparamètres:\", grid_search.best_params_)\n",
    "\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error sur les données de test:\", mae)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baggin Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_params = {\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    \n",
    "}\n",
    "\n",
    "# Définir les hyperparamètres spécifiques au Bagging\n",
    "bagging_params = {\n",
    "    'n_estimators': [20,50,100],\n",
    "    'max_samples': [1.0, 0.8, 0.9],  # La fraction d'échantillons à utiliser pour chaque sac\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Créer un modèle DecisionTreeRegressor pour être utilisé comme modèle de base\n",
    "base_model = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# Créer un modèle BaggingRegressor\n",
    "bagging_model = BaggingRegressor(base_model, random_state=42)\n",
    "\n",
    "# Combiner les deux ensembles de paramètres\n",
    "param_grid = {**base_model_params, **bagging_params}\n",
    "\n",
    "# Utiliser GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=bagging_model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_absolute_error',  # Utilisez 'neg_mean_absolute_error' pour la régression\n",
    "    cv=3,\n",
    "    verbose=3)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Meilleurs paramètres : \", grid_search.best_params_)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE) sur les données de test : \", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xgboost 1\n",
    "\n",
    "- depth:30\n",
    "- 'gamma':10,\n",
    "- 'learning_rate': 0.05,\n",
    "- 'eval_metric': 'rmse',\n",
    "- 'early_stopping_rounds':50,\n",
    "- 'n_jobs':-1\n",
    "\n",
    "5min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 2.882604650941162\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "params = {\n",
    "    'objective': 'reg:squarederror', \n",
    "    'max_depth': 30,\n",
    "    'gamma':10,\n",
    "    'learning_rate': 0.05,\n",
    "    'eval_metric': 'rmse',\n",
    "    'n_jobs':-1\n",
    "}\n",
    "\n",
    "num_rounds = 4000\n",
    "model = xgb.train(params, dtrain, num_rounds)\n",
    "\n",
    "y_pred = model.predict(dtest)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing= xgb.DMatrix(data_test.drop(columns='ID'))\n",
    "data_test[\"Ewltp (g/km)\"] = model.predict(testing)\n",
    "data_test[[\"ID\",\"Ewltp (g/km)\"]].to_csv(\"results/new_xgb.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xgboost 2\n",
    "\n",
    "- 'n_estimators':4000,\n",
    "- 'max_depth': 35,\n",
    "- 'learning_rate': 0.005,\n",
    "- 'colsample_bytree':0.80,\n",
    "- 'gamma':10,\n",
    "- 'reg_alpha':0.8,\n",
    "- 'reg_lambda':0.1,\n",
    "- 'objective': 'reg:squarederror',\n",
    "- 'tree_method':'hist',\n",
    "- 'n_jobs':-1\n",
    "\n",
    "computing time: 25min (home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 2.8647867328354377\n"
     ]
    }
   ],
   "source": [
    "params = {'n_estimators':4000,\n",
    "          'max_depth': 35,\n",
    "          'learning_rate': 0.005,\n",
    "          'colsample_bytree':0.80,\n",
    "          'gamma':10,\n",
    "          'reg_alpha':0.8,\n",
    "          'reg_lambda':0.1,\n",
    "          'objective': 'reg:squarederror',\n",
    "          'tree_method': 'hist',\n",
    "          'n_jobs':-1\n",
    "          \n",
    "}\n",
    "\n",
    "model = xgb.XGBRegressor(**params)\n",
    "\n",
    "model.fit(X_train, y_train,verbose=True)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[\"Ewltp (g/km)\"] = model.predict(data_test.drop(columns=\"ID\"))\n",
    "data_test[[\"ID\",\"Ewltp (g/km)\"]].to_csv(\"results/new_yao_xgb.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xgboost blind full\n",
    "\n",
    "- 'n_estimators':4000,\n",
    "- 'max_depth': 35,\n",
    "- 'learning_rate': 0.005,\n",
    "- 'colsample_bytree':0.80,\n",
    "- 'gamma':10,\n",
    "- 'reg_alpha':0.8,\n",
    "- 'reg_lambda':0.1,\n",
    "- 'objective': 'reg:squarederror',\n",
    "- 'tree_method':'hist',\n",
    "- 'n_jobs':-1\n",
    "\n",
    "with full dataset\n",
    "\n",
    "computing time : 32min(home)\n",
    "\n",
    "MAE Kaggle: 2.8459. Revoir le preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=10, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.005, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=35, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=4000, n_jobs=-1,\n",
       "             num_parallel_tree=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=10, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.005, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=35, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=4000, n_jobs=-1,\n",
       "             num_parallel_tree=None, random_state=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=10, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.005, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=35, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=4000, n_jobs=-1,\n",
       "             num_parallel_tree=None, random_state=None, ...)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'n_estimators':4000,\n",
    "          'max_depth': 35,\n",
    "          'learning_rate': 0.005,\n",
    "          'colsample_bytree':0.80,\n",
    "          'gamma':10,\n",
    "          'reg_alpha':0.8,\n",
    "          'reg_lambda':0.1,\n",
    "          'objective': 'reg:squarederror',\n",
    "          'tree_method': 'hist',\n",
    "          'n_jobs':-1\n",
    "}\n",
    "\n",
    "model = xgb.XGBRegressor(**params)\n",
    "\n",
    "model.fit(data_train.drop(columns='Ewltp (g/km)'), data_train['Ewltp (g/km)'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[\"Ewltp (g/km)\"] = model.predict(data_test.drop(columns='ID'))\n",
    "data_test[[\"ID\",\"Ewltp (g/km)\"]].to_csv(\"results/new_blind_yao_xgb_full.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xgboost 3\n",
    "With new encoding\n",
    "- 'n_estimators':4000,\n",
    "- 'max_depth': 35,\n",
    "- 'learning_rate': 0.005,\n",
    "- 'colsample_bytree':0.80,\n",
    "- 'gamma':10,\n",
    "- 'reg_alpha':0.8,\n",
    "- 'reg_lambda':0.1,\n",
    "- 'objective': 'reg:squarederror',\n",
    "- 'tree_method':'hist',\n",
    "- 'n_jobs':-1\n",
    "\n",
    "computing time: 31min (home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 2.8182787570889554\n"
     ]
    }
   ],
   "source": [
    "params = {'n_estimators':4000,\n",
    "          'max_depth': 35,\n",
    "          'learning_rate': 0.005,\n",
    "          'colsample_bytree':0.80,\n",
    "          'gamma':10,\n",
    "          'reg_alpha':0.8,\n",
    "          'reg_lambda':0.1,\n",
    "          'objective': 'reg:squarederror',\n",
    "          'tree_method': 'hist',\n",
    "          'n_jobs':-1\n",
    "}\n",
    "\n",
    "model = xgb.XGBRegressor(**params)\n",
    "\n",
    "model.fit(X_train, y_train,verbose=True)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[\"Ewltp (g/km)\"] = model.predict(data_test.drop(columns=\"ID\"))\n",
    "data_test[[\"ID\",\"Ewltp (g/km)\"]].to_csv(\"results/new_yao_xgb_cstm_encoded.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xgboost 4\n",
    "\n",
    "With new encoding (encoding counter + OHE)\n",
    "- 'n_estimators':4000,\n",
    "- 'max_depth': 35,\n",
    "- 'learning_rate': 0.005,\n",
    "- 'colsample_bytree':1,\n",
    "- 'subsample': 1,\n",
    "- 'gamma':30,\n",
    "- 'reg_alpha':0.71111,\n",
    "- 'reg_lambda':0.1,\n",
    "- 'objective': 'reg:squarederror',\n",
    "- 'tree_method': 'hist',\n",
    "- 'n_jobs':-1\n",
    "\n",
    "Computing time:  24min(home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 2.846653662793189\n"
     ]
    }
   ],
   "source": [
    "params = {'n_estimators':4000,\n",
    "          'max_depth': 35,\n",
    "          'learning_rate': 0.005,\n",
    "          'colsample_bytree':1,\n",
    "          'subsample': 1,\n",
    "          'gamma':30,\n",
    "          'reg_alpha':0.71111,\n",
    "          'reg_lambda':0.1,\n",
    "          'objective': 'reg:squarederror',\n",
    "          'tree_method': 'hist',\n",
    "          'n_jobs':-1\n",
    "}\n",
    "\n",
    "model = xgb.XGBRegressor(**params)\n",
    "\n",
    "model.fit(X_train, y_train,verbose=True)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[\"Ewltp (g/km)\"] = model.predict(data_test.drop(columns=\"ID\"))\n",
    "data_test[[\"ID\",\"Ewltp (g/km)\"]].to_csv(\"results/new_fine_tuned_xgb_cstm_encoded.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xgboost 5 \n",
    "With new encoding (encoding counter + OHE)\n",
    "- 'n_estimators':4000,\n",
    "- 'max_depth': 35,\n",
    "- 'learning_rate':**1**,\n",
    "- 'colsample_bytree':1,\n",
    "- 'subsample': 1,\n",
    "- 'gamma':30,\n",
    "- 'reg_alpha':0.71111,\n",
    "- 'reg_lambda':0.1,\n",
    "- 'objective': 'reg:squarederror',\n",
    "- 'tree_method': 'hist',\n",
    "- 'n_jobs':-1\n",
    "\n",
    "Computing time:  3min(home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 2.9375179938868228\n"
     ]
    }
   ],
   "source": [
    "params = {'n_estimators':4000,\n",
    "          'max_depth': 35,\n",
    "          'learning_rate': 1,\n",
    "          'colsample_bytree':1,\n",
    "          'subsample': 1,\n",
    "          'gamma':30,\n",
    "          'reg_alpha':0.71111,\n",
    "          'reg_lambda':0.1,\n",
    "          'objective': 'reg:squarederror',\n",
    "          'tree_method': 'hist',\n",
    "          'n_jobs':-1\n",
    "}\n",
    "\n",
    "model = xgb.XGBRegressor(**params)\n",
    "\n",
    "model.fit(X_train, y_train,verbose=True)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[\"Ewltp (g/km)\"] = model.predict(data_test.drop(columns=\"ID\"))\n",
    "data_test[[\"ID\",\"Ewltp (g/km)\"]].to_csv(\"results/new_lr_1_xgb_cstm_encoded.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xgboost 6\n",
    "With new encoding (encoding counter + OHE)\n",
    "- 'n_estimators':**5000**,\n",
    "- 'max_depth': 35,\n",
    "- 'learning_rate':**0.001**,\n",
    "- 'colsample_bytree':1,\n",
    "- 'subsample': 1,\n",
    "- 'gamma':30,\n",
    "- 'reg_alpha':0.71111,\n",
    "- 'reg_lambda':0.1,\n",
    "- 'objective': 'reg:squarederror',\n",
    "- 'tree_method': 'hist',\n",
    "- 'n_jobs':-1\n",
    "\n",
    "Computing time:  93min(home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 3.023181980018556\n"
     ]
    }
   ],
   "source": [
    "params = {'n_estimators':5000,\n",
    "          'max_depth': 35,\n",
    "          'learning_rate': 0.001,\n",
    "          'colsample_bytree':1,\n",
    "          'subsample': 1,\n",
    "          'gamma':30,\n",
    "          'reg_alpha':0.71111,\n",
    "          'reg_lambda':0.1,\n",
    "          'objective': 'reg:squarederror',\n",
    "          'tree_method': 'hist',\n",
    "          'n_jobs':-1\n",
    "}\n",
    "\n",
    "model = xgb.XGBRegressor(**params)\n",
    "\n",
    "model.fit(X_train, y_train,verbose=True)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[\"Ewltp (g/km)\"] = model.predict(data_test.drop(columns=\"ID\"))\n",
    "data_test[[\"ID\",\"Ewltp (g/km)\"]].to_csv(\"results/new_lr_0001_nEst_5000_xgb_cstm_encoded.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catboost 1\n",
    "\n",
    "computing time 49min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 166.0466744\ttest: 166.0927154\tbest: 166.0927154 (0)\ttotal: 329ms\tremaining: 54m 52s\n",
      "100:\tlearn: 19.9021623\ttest: 19.8023282\tbest: 19.8023282 (100)\ttotal: 30.2s\tremaining: 49m 16s\n",
      "200:\tlearn: 16.3895228\ttest: 16.3450710\tbest: 16.3450710 (200)\ttotal: 60s\tremaining: 48m 44s\n",
      "300:\tlearn: 14.6005017\ttest: 14.5744841\tbest: 14.5744841 (300)\ttotal: 1m 29s\tremaining: 48m 6s\n",
      "400:\tlearn: 13.3583934\ttest: 13.3527044\tbest: 13.3527044 (400)\ttotal: 1m 58s\tremaining: 47m 28s\n",
      "500:\tlearn: 12.4735322\ttest: 12.4949177\tbest: 12.4949177 (500)\ttotal: 2m 28s\tremaining: 46m 51s\n",
      "600:\tlearn: 11.8248216\ttest: 11.8637427\tbest: 11.8637427 (600)\ttotal: 2m 57s\tremaining: 46m 19s\n",
      "700:\tlearn: 11.3208447\ttest: 11.3706236\tbest: 11.3706236 (700)\ttotal: 3m 27s\tremaining: 45m 52s\n",
      "800:\tlearn: 10.8895149\ttest: 10.9501880\tbest: 10.9501880 (800)\ttotal: 3m 57s\tremaining: 45m 23s\n",
      "900:\tlearn: 10.5598394\ttest: 10.6378579\tbest: 10.6378579 (900)\ttotal: 4m 26s\tremaining: 44m 53s\n",
      "1000:\tlearn: 10.2236002\ttest: 10.3213196\tbest: 10.3213196 (1000)\ttotal: 4m 56s\tremaining: 44m 26s\n",
      "1100:\tlearn: 9.9249856\ttest: 10.0413769\tbest: 10.0413769 (1100)\ttotal: 5m 26s\tremaining: 43m 58s\n",
      "1200:\tlearn: 9.6838789\ttest: 9.8144088\tbest: 9.8144088 (1200)\ttotal: 5m 55s\tremaining: 43m 25s\n",
      "1300:\tlearn: 9.4749389\ttest: 9.6182998\tbest: 9.6182998 (1300)\ttotal: 6m 25s\tremaining: 42m 57s\n",
      "1400:\tlearn: 9.2613355\ttest: 9.4174017\tbest: 9.4174017 (1400)\ttotal: 6m 54s\tremaining: 42m 26s\n",
      "1500:\tlearn: 9.0757878\ttest: 9.2448582\tbest: 9.2448582 (1500)\ttotal: 7m 23s\tremaining: 41m 52s\n",
      "1600:\tlearn: 8.9131492\ttest: 9.0879916\tbest: 9.0879916 (1600)\ttotal: 7m 53s\tremaining: 41m 23s\n",
      "1700:\tlearn: 8.7650663\ttest: 8.9500690\tbest: 8.9500690 (1700)\ttotal: 8m 23s\tremaining: 40m 55s\n",
      "1800:\tlearn: 8.6260620\ttest: 8.8202797\tbest: 8.8202797 (1800)\ttotal: 8m 53s\tremaining: 40m 26s\n",
      "1900:\tlearn: 8.4965863\ttest: 8.6985953\tbest: 8.6985953 (1900)\ttotal: 9m 22s\tremaining: 39m 57s\n",
      "2000:\tlearn: 8.3863796\ttest: 8.5970480\tbest: 8.5970480 (2000)\ttotal: 9m 52s\tremaining: 39m 26s\n",
      "2100:\tlearn: 8.2697010\ttest: 8.4941648\tbest: 8.4941648 (2100)\ttotal: 10m 21s\tremaining: 38m 56s\n",
      "2200:\tlearn: 8.1624750\ttest: 8.3996657\tbest: 8.3996657 (2200)\ttotal: 10m 50s\tremaining: 38m 26s\n",
      "2300:\tlearn: 8.0658959\ttest: 8.3107307\tbest: 8.3107307 (2300)\ttotal: 11m 20s\tremaining: 37m 56s\n",
      "2400:\tlearn: 7.9717512\ttest: 8.2257280\tbest: 8.2257280 (2400)\ttotal: 11m 49s\tremaining: 37m 26s\n",
      "2500:\tlearn: 7.8757053\ttest: 8.1407682\tbest: 8.1407682 (2500)\ttotal: 12m 19s\tremaining: 36m 56s\n",
      "2600:\tlearn: 7.7969377\ttest: 8.0707225\tbest: 8.0707225 (2600)\ttotal: 12m 49s\tremaining: 36m 28s\n",
      "2700:\tlearn: 7.7114584\ttest: 7.9919318\tbest: 7.9919318 (2700)\ttotal: 13m 18s\tremaining: 35m 58s\n",
      "2800:\tlearn: 7.6438296\ttest: 7.9369904\tbest: 7.9369904 (2800)\ttotal: 13m 47s\tremaining: 35m 28s\n",
      "2900:\tlearn: 7.5751619\ttest: 7.8776189\tbest: 7.8776189 (2900)\ttotal: 14m 17s\tremaining: 34m 59s\n",
      "3000:\tlearn: 7.4986916\ttest: 7.8120378\tbest: 7.8120378 (3000)\ttotal: 14m 47s\tremaining: 34m 30s\n",
      "3100:\tlearn: 7.4385451\ttest: 7.7604479\tbest: 7.7604479 (3100)\ttotal: 15m 17s\tremaining: 34m\n",
      "3200:\tlearn: 7.3776815\ttest: 7.7099695\tbest: 7.7099695 (3200)\ttotal: 15m 46s\tremaining: 33m 30s\n",
      "3300:\tlearn: 7.3238428\ttest: 7.6651640\tbest: 7.6651640 (3300)\ttotal: 16m 16s\tremaining: 33m 1s\n",
      "3400:\tlearn: 7.2615474\ttest: 7.6121512\tbest: 7.6121512 (3400)\ttotal: 16m 46s\tremaining: 32m 32s\n",
      "3500:\tlearn: 7.2077850\ttest: 7.5640114\tbest: 7.5640114 (3500)\ttotal: 17m 15s\tremaining: 32m 1s\n",
      "3600:\tlearn: 7.1557560\ttest: 7.5202177\tbest: 7.5202177 (3600)\ttotal: 17m 44s\tremaining: 31m 31s\n",
      "3700:\tlearn: 7.1063249\ttest: 7.4781872\tbest: 7.4781872 (3700)\ttotal: 18m 13s\tremaining: 31m 1s\n",
      "3800:\tlearn: 7.0540243\ttest: 7.4364803\tbest: 7.4364803 (3800)\ttotal: 18m 43s\tremaining: 30m 31s\n",
      "3900:\tlearn: 7.0081334\ttest: 7.3969569\tbest: 7.3969569 (3900)\ttotal: 19m 12s\tremaining: 30m 2s\n",
      "4000:\tlearn: 6.9638378\ttest: 7.3594711\tbest: 7.3594711 (4000)\ttotal: 19m 42s\tremaining: 29m 32s\n",
      "4100:\tlearn: 6.9216268\ttest: 7.3253296\tbest: 7.3253296 (4100)\ttotal: 20m 11s\tremaining: 29m 2s\n",
      "4200:\tlearn: 6.8802454\ttest: 7.2884633\tbest: 7.2884633 (4200)\ttotal: 20m 40s\tremaining: 28m 32s\n",
      "4300:\tlearn: 6.8382012\ttest: 7.2529692\tbest: 7.2529692 (4300)\ttotal: 21m 10s\tremaining: 28m 2s\n",
      "4400:\tlearn: 6.8043102\ttest: 7.2244396\tbest: 7.2244396 (4400)\ttotal: 21m 39s\tremaining: 27m 33s\n",
      "4500:\tlearn: 6.7700650\ttest: 7.1967552\tbest: 7.1967552 (4500)\ttotal: 22m 9s\tremaining: 27m 3s\n",
      "4600:\tlearn: 6.7367701\ttest: 7.1690013\tbest: 7.1690013 (4600)\ttotal: 22m 39s\tremaining: 26m 34s\n",
      "4700:\tlearn: 6.7019675\ttest: 7.1432553\tbest: 7.1432553 (4700)\ttotal: 23m 8s\tremaining: 26m 5s\n",
      "4800:\tlearn: 6.6700677\ttest: 7.1182887\tbest: 7.1182887 (4800)\ttotal: 23m 37s\tremaining: 25m 35s\n",
      "4900:\tlearn: 6.6384356\ttest: 7.0911952\tbest: 7.0911952 (4900)\ttotal: 24m 7s\tremaining: 25m 5s\n",
      "5000:\tlearn: 6.6072880\ttest: 7.0666014\tbest: 7.0666014 (5000)\ttotal: 24m 36s\tremaining: 24m 35s\n",
      "5100:\tlearn: 6.5731033\ttest: 7.0388711\tbest: 7.0388711 (5100)\ttotal: 25m 5s\tremaining: 24m 6s\n",
      "5200:\tlearn: 6.5401495\ttest: 7.0123981\tbest: 7.0123981 (5200)\ttotal: 25m 35s\tremaining: 23m 36s\n",
      "5300:\tlearn: 6.5139913\ttest: 6.9906888\tbest: 6.9906888 (5300)\ttotal: 26m 4s\tremaining: 23m 6s\n",
      "5400:\tlearn: 6.4854360\ttest: 6.9682795\tbest: 6.9682795 (5400)\ttotal: 26m 34s\tremaining: 22m 37s\n",
      "5500:\tlearn: 6.4574234\ttest: 6.9464700\tbest: 6.9464700 (5500)\ttotal: 27m 3s\tremaining: 22m 7s\n",
      "5600:\tlearn: 6.4316316\ttest: 6.9256022\tbest: 6.9256022 (5600)\ttotal: 27m 32s\tremaining: 21m 38s\n",
      "5700:\tlearn: 6.4035248\ttest: 6.9033371\tbest: 6.9033371 (5700)\ttotal: 28m 2s\tremaining: 21m 8s\n",
      "5800:\tlearn: 6.3787717\ttest: 6.8831965\tbest: 6.8831965 (5800)\ttotal: 28m 31s\tremaining: 20m 39s\n",
      "5900:\tlearn: 6.3527250\ttest: 6.8633130\tbest: 6.8633130 (5900)\ttotal: 29m 1s\tremaining: 20m 9s\n",
      "6000:\tlearn: 6.3266736\ttest: 6.8448102\tbest: 6.8448102 (6000)\ttotal: 29m 31s\tremaining: 19m 40s\n",
      "6100:\tlearn: 6.3013590\ttest: 6.8236507\tbest: 6.8236507 (6100)\ttotal: 30m\tremaining: 19m 10s\n",
      "6200:\tlearn: 6.2738316\ttest: 6.8003132\tbest: 6.8003132 (6200)\ttotal: 30m 29s\tremaining: 18m 41s\n",
      "6300:\tlearn: 6.2524299\ttest: 6.7852274\tbest: 6.7852274 (6300)\ttotal: 30m 59s\tremaining: 18m 11s\n",
      "6400:\tlearn: 6.2283233\ttest: 6.7662723\tbest: 6.7662723 (6400)\ttotal: 31m 28s\tremaining: 17m 41s\n",
      "6500:\tlearn: 6.2052207\ttest: 6.7472313\tbest: 6.7472313 (6500)\ttotal: 31m 57s\tremaining: 17m 12s\n",
      "6600:\tlearn: 6.1855662\ttest: 6.7326308\tbest: 6.7326308 (6600)\ttotal: 32m 27s\tremaining: 16m 42s\n",
      "6700:\tlearn: 6.1644274\ttest: 6.7168549\tbest: 6.7168549 (6700)\ttotal: 32m 56s\tremaining: 16m 13s\n",
      "6800:\tlearn: 6.1468624\ttest: 6.7047782\tbest: 6.7047782 (6800)\ttotal: 33m 26s\tremaining: 15m 43s\n",
      "6900:\tlearn: 6.1264562\ttest: 6.6910957\tbest: 6.6910957 (6900)\ttotal: 33m 55s\tremaining: 15m 13s\n",
      "7000:\tlearn: 6.1071969\ttest: 6.6769634\tbest: 6.6769634 (7000)\ttotal: 34m 24s\tremaining: 14m 44s\n",
      "7100:\tlearn: 6.0889251\ttest: 6.6639591\tbest: 6.6639591 (7100)\ttotal: 34m 53s\tremaining: 14m 14s\n",
      "7200:\tlearn: 6.0711890\ttest: 6.6499598\tbest: 6.6499598 (7200)\ttotal: 35m 22s\tremaining: 13m 45s\n",
      "7300:\tlearn: 6.0523024\ttest: 6.6365681\tbest: 6.6365681 (7300)\ttotal: 35m 52s\tremaining: 13m 15s\n",
      "7400:\tlearn: 6.0336699\ttest: 6.6231073\tbest: 6.6228892 (7398)\ttotal: 36m 21s\tremaining: 12m 46s\n",
      "7500:\tlearn: 6.0162877\ttest: 6.6109212\tbest: 6.6109212 (7500)\ttotal: 36m 51s\tremaining: 12m 16s\n",
      "7600:\tlearn: 5.9966628\ttest: 6.5943113\tbest: 6.5943113 (7600)\ttotal: 37m 22s\tremaining: 11m 47s\n",
      "7700:\tlearn: 5.9772625\ttest: 6.5793735\tbest: 6.5793735 (7700)\ttotal: 37m 52s\tremaining: 11m 18s\n",
      "7800:\tlearn: 5.9601471\ttest: 6.5666977\tbest: 6.5666977 (7800)\ttotal: 38m 22s\tremaining: 10m 48s\n",
      "7900:\tlearn: 5.9452964\ttest: 6.5565818\tbest: 6.5565818 (7900)\ttotal: 38m 52s\tremaining: 10m 19s\n",
      "8000:\tlearn: 5.9273913\ttest: 6.5422218\tbest: 6.5422218 (8000)\ttotal: 39m 22s\tremaining: 9m 50s\n",
      "8100:\tlearn: 5.9110707\ttest: 6.5291251\tbest: 6.5288533 (8099)\ttotal: 39m 52s\tremaining: 9m 20s\n",
      "8200:\tlearn: 5.8947185\ttest: 6.5158430\tbest: 6.5158430 (8200)\ttotal: 40m 22s\tremaining: 8m 51s\n",
      "8300:\tlearn: 5.8785198\ttest: 6.5027082\tbest: 6.5027082 (8300)\ttotal: 40m 51s\tremaining: 8m 21s\n",
      "8400:\tlearn: 5.8645655\ttest: 6.4922696\tbest: 6.4922696 (8400)\ttotal: 41m 21s\tremaining: 7m 52s\n",
      "8500:\tlearn: 5.8495689\ttest: 6.4811648\tbest: 6.4811648 (8500)\ttotal: 41m 51s\tremaining: 7m 22s\n",
      "8600:\tlearn: 5.8331511\ttest: 6.4698149\tbest: 6.4698149 (8600)\ttotal: 42m 21s\tremaining: 6m 53s\n",
      "8700:\tlearn: 5.8196580\ttest: 6.4605392\tbest: 6.4605392 (8700)\ttotal: 42m 50s\tremaining: 6m 23s\n",
      "8800:\tlearn: 5.8074554\ttest: 6.4519562\tbest: 6.4519562 (8800)\ttotal: 43m 20s\tremaining: 5m 54s\n",
      "8900:\tlearn: 5.7943792\ttest: 6.4419928\tbest: 6.4419928 (8900)\ttotal: 43m 49s\tremaining: 5m 24s\n",
      "9000:\tlearn: 5.7774505\ttest: 6.4292843\tbest: 6.4292843 (9000)\ttotal: 44m 19s\tremaining: 4m 55s\n",
      "9100:\tlearn: 5.7634473\ttest: 6.4205080\tbest: 6.4205080 (9100)\ttotal: 44m 49s\tremaining: 4m 25s\n",
      "9200:\tlearn: 5.7526009\ttest: 6.4119830\tbest: 6.4119830 (9200)\ttotal: 45m 19s\tremaining: 3m 56s\n",
      "9300:\tlearn: 5.7406910\ttest: 6.4037244\tbest: 6.4037244 (9300)\ttotal: 45m 48s\tremaining: 3m 26s\n",
      "9400:\tlearn: 5.7258422\ttest: 6.3921093\tbest: 6.3921093 (9400)\ttotal: 46m 18s\tremaining: 2m 57s\n",
      "9500:\tlearn: 5.7115733\ttest: 6.3802132\tbest: 6.3802132 (9500)\ttotal: 46m 47s\tremaining: 2m 27s\n",
      "9600:\tlearn: 5.6982262\ttest: 6.3710968\tbest: 6.3710968 (9600)\ttotal: 47m 17s\tremaining: 1m 57s\n",
      "9700:\tlearn: 5.6866344\ttest: 6.3621594\tbest: 6.3621594 (9700)\ttotal: 47m 46s\tremaining: 1m 28s\n",
      "9800:\tlearn: 5.6742874\ttest: 6.3529217\tbest: 6.3529217 (9800)\ttotal: 48m 16s\tremaining: 58.8s\n",
      "9900:\tlearn: 5.6607105\ttest: 6.3442332\tbest: 6.3442332 (9900)\ttotal: 48m 45s\tremaining: 29.3s\n",
      "9999:\tlearn: 5.6496027\ttest: 6.3367792\tbest: 6.3367792 (9999)\ttotal: 49m 15s\tremaining: 0us\n",
      "\n",
      "bestTest = 6.336779233\n",
      "bestIteration = 9999\n",
      "\n",
      "Mean Absolute Error (MAE): 3.5228090019627323\n"
     ]
    }
   ],
   "source": [
    "\n",
    "catb = CatBoostRegressor(iterations=10000,  # Nombre d'itérations (peut être ajusté)\n",
    "                          depth=6,  # Profondeur de l'arbre (peut être ajusté)\n",
    "                          learning_rate=0.1,  # Taux d'apprentissage (peut être ajusté)\n",
    "                          loss_function='RMSE',  # Fonction de perte pour la régression\n",
    "                          random_seed=42,\n",
    "                          task_type='CPU',\n",
    "                          thread_count=-1)\n",
    "\n",
    "catb.fit(X_train, y_train, eval_set=(X_test, y_test), early_stopping_rounds=50, verbose=100)\n",
    "\n",
    "predictions = catb.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[\"Ewltp (g/km)\"] = catb.predict(data_test.drop(columns='ID'))\n",
    "data_test[[\"ID\",\"Ewltp (g/km)\"]].to_csv(\"results/new_simple_catboosting.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catboost 2\n",
    "\n",
    "different test:\n",
    "- iterations=10000 (+ = mieux)\n",
    "- depth=10,  (+= mieux) max = 16\n",
    "- learning_rate=1  \n",
    "- loss_function='MAE'\n",
    "\n",
    "computing time: 72min(home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 28.6058121\ttest: 28.5322570\tbest: 28.5322570 (0)\ttotal: 873ms\tremaining: 2h 25m 25s\n",
      "100:\tlearn: 5.8784600\ttest: 5.9230869\tbest: 5.9230869 (100)\ttotal: 1m 13s\tremaining: 1h 59m 39s\n",
      "200:\tlearn: 4.9300804\ttest: 5.0235295\tbest: 5.0235295 (200)\ttotal: 2m 25s\tremaining: 1h 58m 25s\n",
      "300:\tlearn: 4.4654082\ttest: 4.6011861\tbest: 4.6011861 (300)\ttotal: 3m 38s\tremaining: 1h 57m 15s\n",
      "400:\tlearn: 4.1525710\ttest: 4.3238239\tbest: 4.3238239 (400)\ttotal: 4m 50s\tremaining: 1h 56m 3s\n",
      "500:\tlearn: 3.9729140\ttest: 4.1695874\tbest: 4.1695874 (500)\ttotal: 6m 4s\tremaining: 1h 55m 3s\n",
      "600:\tlearn: 3.8476238\ttest: 4.0645906\tbest: 4.0645906 (600)\ttotal: 7m 16s\tremaining: 1h 53m 54s\n",
      "700:\tlearn: 3.7449758\ttest: 3.9826462\tbest: 3.9826462 (700)\ttotal: 8m 29s\tremaining: 1h 52m 36s\n",
      "800:\tlearn: 3.6587264\ttest: 3.9120052\tbest: 3.9120052 (800)\ttotal: 9m 42s\tremaining: 1h 51m 25s\n",
      "900:\tlearn: 3.5905199\ttest: 3.8547781\tbest: 3.8547781 (900)\ttotal: 10m 54s\tremaining: 1h 50m 14s\n",
      "1000:\tlearn: 3.5346848\ttest: 3.8087109\tbest: 3.8087109 (1000)\ttotal: 12m 7s\tremaining: 1h 49m 4s\n",
      "1100:\tlearn: 3.4887149\ttest: 3.7759716\tbest: 3.7759716 (1100)\ttotal: 13m 21s\tremaining: 1h 47m 57s\n",
      "1200:\tlearn: 3.4503491\ttest: 3.7468483\tbest: 3.7468483 (1200)\ttotal: 14m 34s\tremaining: 1h 46m 48s\n",
      "1300:\tlearn: 3.4185730\ttest: 3.7238199\tbest: 3.7238199 (1300)\ttotal: 15m 48s\tremaining: 1h 45m 39s\n",
      "1400:\tlearn: 3.3903943\ttest: 3.7033265\tbest: 3.7033265 (1400)\ttotal: 17m 1s\tremaining: 1h 44m 28s\n",
      "1500:\tlearn: 3.3684336\ttest: 3.6882153\tbest: 3.6882153 (1500)\ttotal: 18m 14s\tremaining: 1h 43m 19s\n",
      "1600:\tlearn: 3.3472045\ttest: 3.6736624\tbest: 3.6736624 (1600)\ttotal: 19m 28s\tremaining: 1h 42m 10s\n",
      "1700:\tlearn: 3.3262787\ttest: 3.6591502\tbest: 3.6591502 (1700)\ttotal: 20m 42s\tremaining: 1h 40m 59s\n",
      "1800:\tlearn: 3.3115123\ttest: 3.6487759\tbest: 3.6487759 (1800)\ttotal: 21m 55s\tremaining: 1h 39m 50s\n",
      "1900:\tlearn: 3.2959068\ttest: 3.6376043\tbest: 3.6376043 (1900)\ttotal: 23m 9s\tremaining: 1h 38m 40s\n",
      "2000:\tlearn: 3.2822549\ttest: 3.6287513\tbest: 3.6287447 (1998)\ttotal: 24m 23s\tremaining: 1h 37m 28s\n",
      "2100:\tlearn: 3.2686904\ttest: 3.6184183\tbest: 3.6184183 (2100)\ttotal: 25m 36s\tremaining: 1h 36m 17s\n",
      "2200:\tlearn: 3.2569184\ttest: 3.6095289\tbest: 3.6095289 (2200)\ttotal: 26m 50s\tremaining: 1h 35m 5s\n",
      "2300:\tlearn: 3.2464669\ttest: 3.6020472\tbest: 3.6020472 (2300)\ttotal: 28m 4s\tremaining: 1h 33m 54s\n",
      "2400:\tlearn: 3.2364954\ttest: 3.5943976\tbest: 3.5943976 (2400)\ttotal: 29m 17s\tremaining: 1h 32m 43s\n",
      "2500:\tlearn: 3.2244291\ttest: 3.5836095\tbest: 3.5836095 (2500)\ttotal: 30m 31s\tremaining: 1h 31m 31s\n",
      "2600:\tlearn: 3.2174780\ttest: 3.5796945\tbest: 3.5796945 (2600)\ttotal: 31m 45s\tremaining: 1h 30m 19s\n",
      "2700:\tlearn: 3.2086766\ttest: 3.5741718\tbest: 3.5741718 (2700)\ttotal: 32m 58s\tremaining: 1h 29m 7s\n",
      "2800:\tlearn: 3.2024196\ttest: 3.5699092\tbest: 3.5699092 (2800)\ttotal: 34m 12s\tremaining: 1h 27m 55s\n",
      "2900:\tlearn: 3.1955663\ttest: 3.5657307\tbest: 3.5657307 (2900)\ttotal: 35m 26s\tremaining: 1h 26m 43s\n",
      "3000:\tlearn: 3.1883796\ttest: 3.5605259\tbest: 3.5603828 (2999)\ttotal: 36m 41s\tremaining: 1h 25m 34s\n",
      "3100:\tlearn: 3.1801879\ttest: 3.5543445\tbest: 3.5543445 (3100)\ttotal: 37m 54s\tremaining: 1h 24m 21s\n",
      "3200:\tlearn: 3.1730717\ttest: 3.5492945\tbest: 3.5492945 (3200)\ttotal: 39m 8s\tremaining: 1h 23m 8s\n",
      "3300:\tlearn: 3.1672100\ttest: 3.5460970\tbest: 3.5460970 (3300)\ttotal: 40m 21s\tremaining: 1h 21m 55s\n",
      "3400:\tlearn: 3.1624236\ttest: 3.5433120\tbest: 3.5433120 (3400)\ttotal: 41m 35s\tremaining: 1h 20m 42s\n",
      "3500:\tlearn: 3.1565052\ttest: 3.5397139\tbest: 3.5397139 (3500)\ttotal: 42m 49s\tremaining: 1h 19m 29s\n",
      "3600:\tlearn: 3.1512287\ttest: 3.5366070\tbest: 3.5366070 (3600)\ttotal: 44m 3s\tremaining: 1h 18m 17s\n",
      "3700:\tlearn: 3.1469143\ttest: 3.5332726\tbest: 3.5332703 (3699)\ttotal: 45m 17s\tremaining: 1h 17m 4s\n",
      "3800:\tlearn: 3.1432130\ttest: 3.5313707\tbest: 3.5313707 (3800)\ttotal: 46m 30s\tremaining: 1h 15m 51s\n",
      "3900:\tlearn: 3.1399189\ttest: 3.5291993\tbest: 3.5291688 (3898)\ttotal: 47m 44s\tremaining: 1h 14m 38s\n",
      "4000:\tlearn: 3.1361407\ttest: 3.5265430\tbest: 3.5265430 (4000)\ttotal: 48m 58s\tremaining: 1h 13m 26s\n",
      "4100:\tlearn: 3.1302975\ttest: 3.5233417\tbest: 3.5233340 (4098)\ttotal: 50m 12s\tremaining: 1h 12m 13s\n",
      "4200:\tlearn: 3.1270103\ttest: 3.5215924\tbest: 3.5215924 (4200)\ttotal: 51m 26s\tremaining: 1h 11m\n",
      "4300:\tlearn: 3.1240005\ttest: 3.5195204\tbest: 3.5195181 (4297)\ttotal: 52m 40s\tremaining: 1h 9m 47s\n",
      "4400:\tlearn: 3.1214113\ttest: 3.5181794\tbest: 3.5181794 (4400)\ttotal: 53m 54s\tremaining: 1h 8m 34s\n",
      "4500:\tlearn: 3.1185588\ttest: 3.5160926\tbest: 3.5160926 (4500)\ttotal: 55m 8s\tremaining: 1h 7m 21s\n",
      "4600:\tlearn: 3.1150128\ttest: 3.5143685\tbest: 3.5142563 (4593)\ttotal: 56m 22s\tremaining: 1h 6m 8s\n",
      "4700:\tlearn: 3.1107908\ttest: 3.5113874\tbest: 3.5113874 (4700)\ttotal: 57m 35s\tremaining: 1h 4m 55s\n",
      "4800:\tlearn: 3.1076219\ttest: 3.5092093\tbest: 3.5092093 (4800)\ttotal: 58m 49s\tremaining: 1h 3m 42s\n",
      "4900:\tlearn: 3.1053087\ttest: 3.5078493\tbest: 3.5078493 (4900)\ttotal: 1h 3s\tremaining: 1h 2m 29s\n",
      "5000:\tlearn: 3.1010357\ttest: 3.5040584\tbest: 3.5040584 (5000)\ttotal: 1h 1m 17s\tremaining: 1h 1m 16s\n",
      "5100:\tlearn: 3.0974200\ttest: 3.5018003\tbest: 3.5018003 (5100)\ttotal: 1h 2m 31s\tremaining: 1h 2s\n",
      "5200:\tlearn: 3.0950249\ttest: 3.4993585\tbest: 3.4993585 (5200)\ttotal: 1h 3m 45s\tremaining: 58m 49s\n",
      "5300:\tlearn: 3.0916851\ttest: 3.4973645\tbest: 3.4973645 (5300)\ttotal: 1h 4m 58s\tremaining: 57m 36s\n",
      "5400:\tlearn: 3.0892426\ttest: 3.4960149\tbest: 3.4959706 (5397)\ttotal: 1h 6m 13s\tremaining: 56m 23s\n",
      "5500:\tlearn: 3.0865626\ttest: 3.4943547\tbest: 3.4943547 (5500)\ttotal: 1h 7m 26s\tremaining: 55m 9s\n",
      "5600:\tlearn: 3.0845733\ttest: 3.4931199\tbest: 3.4931199 (5600)\ttotal: 1h 8m 40s\tremaining: 53m 56s\n",
      "5700:\tlearn: 3.0821533\ttest: 3.4917589\tbest: 3.4917490 (5699)\ttotal: 1h 9m 54s\tremaining: 52m 43s\n",
      "5800:\tlearn: 3.0801431\ttest: 3.4910627\tbest: 3.4910548 (5786)\ttotal: 1h 11m 8s\tremaining: 51m 29s\n",
      "5900:\tlearn: 3.0780316\ttest: 3.4906685\tbest: 3.4903613 (5870)\ttotal: 1h 12m 22s\tremaining: 50m 16s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 3.490361325\n",
      "bestIteration = 5870\n",
      "\n",
      "Shrink model to first 5871 iterations.\n",
      "Mean Absolute Error (MAE): 3.490362327182753\n"
     ]
    }
   ],
   "source": [
    "\n",
    "catb = CatBoostRegressor(iterations=10000,  # Nombre d'itérations (peut être ajusté)\n",
    "                          depth=10,  # Profondeur de l'arbre (peut être ajusté)\n",
    "                          learning_rate=1,  # Taux d'apprentissage (peut être ajusté)\n",
    "                          loss_function='MAE',  # Fonction de perte pour la régression\n",
    "                          random_seed=42,\n",
    "                          task_type='CPU',\n",
    "                          thread_count=-1)\n",
    "\n",
    "catb.fit(X_train, y_train, eval_set=(X_test, y_test), early_stopping_rounds=50, verbose=100)\n",
    "\n",
    "predictions = catb.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[\"Ewltp (g/km)\"] = catb.predict(data_test.drop(columns='ID'))\n",
    "data_test[[\"ID\",\"Ewltp (g/km)\"]].to_csv(\"results/new_10k_8_1_MAE_catboosting.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catboost 3\n",
    "\n",
    "- iterations= 10000 \n",
    "- depth= 16\n",
    "- learning_rate=1  \n",
    "- loss_function='RMSE' (mieux que MAE)\n",
    "\n",
    "computing time: 109min(home)\n",
    "\n",
    "catboost encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 26.5469423\ttest: 26.5609633\tbest: 26.5609633 (0)\ttotal: 4.74s\tremaining: 13h 10m 32s\n",
      "100:\tlearn: 5.3983648\ttest: 6.9934249\tbest: 6.9934249 (100)\ttotal: 8m\tremaining: 13h 4m 54s\n",
      "200:\tlearn: 4.7077175\ttest: 6.6169532\tbest: 6.6169532 (200)\ttotal: 15m 46s\tremaining: 12h 49m 24s\n",
      "300:\tlearn: 4.4201075\ttest: 6.4774620\tbest: 6.4774620 (300)\ttotal: 23m 34s\tremaining: 12h 39m 52s\n",
      "400:\tlearn: 4.2737071\ttest: 6.4174758\tbest: 6.4174758 (400)\ttotal: 31m 21s\tremaining: 12h 30m 36s\n",
      "500:\tlearn: 4.1882630\ttest: 6.3826007\tbest: 6.3825403 (499)\ttotal: 39m 9s\tremaining: 12h 22m 33s\n",
      "600:\tlearn: 4.1312714\ttest: 6.3621676\tbest: 6.3621676 (600)\ttotal: 46m 57s\tremaining: 12h 14m 19s\n",
      "700:\tlearn: 4.0869902\ttest: 6.3518645\tbest: 6.3513306 (696)\ttotal: 54m 46s\tremaining: 12h 6m 30s\n",
      "800:\tlearn: 4.0517728\ttest: 6.3425141\tbest: 6.3425141 (800)\ttotal: 1h 2m 34s\tremaining: 11h 58m 40s\n",
      "900:\tlearn: 4.0276829\ttest: 6.3377529\tbest: 6.3377529 (900)\ttotal: 1h 10m 21s\tremaining: 11h 50m 29s\n",
      "1000:\tlearn: 4.0089038\ttest: 6.3334878\tbest: 6.3332513 (973)\ttotal: 1h 18m 9s\tremaining: 11h 42m 41s\n",
      "1100:\tlearn: 3.9906253\ttest: 6.3290929\tbest: 6.3290929 (1100)\ttotal: 1h 25m 59s\tremaining: 11h 35m 1s\n",
      "1200:\tlearn: 3.9753642\ttest: 6.3261264\tbest: 6.3261130 (1198)\ttotal: 1h 33m 44s\tremaining: 11h 26m 49s\n",
      "1300:\tlearn: 3.9623904\ttest: 6.3248195\tbest: 6.3247612 (1298)\ttotal: 1h 41m 36s\tremaining: 11h 19m 20s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 6.323595825\n",
      "bestIteration = 1346\n",
      "\n",
      "Shrink model to first 1347 iterations.\n",
      "Mean Absolute Error (MAE): 2.997643282302508\n"
     ]
    }
   ],
   "source": [
    "\n",
    "catb = CatBoostRegressor(iterations=10000,  # Nombre d'itérations (peut être ajusté)\n",
    "                          depth=16,  # Profondeur de l'arbre (peut être ajusté)\n",
    "                          learning_rate=1,  # Taux d'apprentissage (peut être ajusté)\n",
    "                          loss_function='RMSE',  # Fonction de perte pour la régression\n",
    "                          random_seed=42)\n",
    "\n",
    "catb.fit(X_train, y_train, eval_set=(X_test, y_test), early_stopping_rounds=50, verbose=100)\n",
    "\n",
    "predictions = catb.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[\"Ewltp (g/km)\"] = catb.predict(data_test.drop(columns='ID'))\n",
    "data_test[[\"ID\",\"Ewltp (g/km)\"]].to_csv(\"results/new_10k_16_1_RMSE_catboosting.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_mining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
