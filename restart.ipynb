{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data + Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor,BaggingRegressor,AdaBoostRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, OrdinalEncoder, RobustScaler, PolynomialFeatures, TargetEncoder\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from category_encoders import LeaveOneOutEncoder\n",
    "import joblib\n",
    "\n",
    "#from catboost import CatBoostRegressor, Pool\n",
    "#import lightgbm as lgb\n",
    "#import xgboost as xgb\n",
    "#from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "sns.set_theme(style=\"ticks\", palette=\"pastel\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train= pd.read_csv(\"data/train.csv\",sep=\",\",low_memory=False)\n",
    "data_test = pd.read_csv(\"data/test.csv\",sep=\",\",low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupération d'observations\n",
    "\n",
    "Lorsque la voiture est électrique: on peut se permettre de set `ec(cm3)`, `Fuel consumption `, `z (Wh/km)` à 0\n",
    "\n",
    "lorsque la voiture n'est pas hybride / électrique : on peut mettre `Electric range (km)` à 0\n",
    "\n",
    "A voir si on choisir de prendre le traitement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_fuel_types(category: str):\n",
    "    if category in ['PETROL/ELECTRIC', 'DIESEL/ELECTRIC']:\n",
    "        return \"HYBRID\"\n",
    "    elif category in ['NG-BIOMETHANE', 'HYDROGEN', 'NG','E85']:\n",
    "        return \"BIO-FUEL\"\n",
    "    elif category in ['PETROL','LPG'] :\n",
    "        return 'PETROL'\n",
    "    else:\n",
    "        return category   \n",
    "\n",
    "def recup_electric(df):\n",
    "    #  ec (cm3)\n",
    "    df.loc[(df[\"Ft\"].apply(group_fuel_types)==\"ELECTRIC\") & (df[\"ec (cm3)\"].isna()),\"ec (cm3)\"] = 0\n",
    "    #  Fm\n",
    "    df.loc[(df[\"Ft\"].apply(group_fuel_types) ==\"ELECTRIC\") & (df[\"Fm\"].isna()),\"Fm\"] = \"E\"\n",
    "    df.loc[(df[\"Ft\"].apply(group_fuel_types) ==\"HYBRID\") & (df[\"Fm\"].isna()),\"Fm\"] = \"P\"\n",
    "\n",
    "    # Electric range (km)\n",
    "    df.loc[~(df[\"Ft\"].apply(group_fuel_types).isin([\"ELECTRIC\", \"HYBRID\"])) & (df[\"Electric range (km)\"].isna()),\"Electric range (km)\"] = 0\n",
    "\n",
    "    #  Fuel consumption \n",
    "    df.loc[(df[\"Ft\"].apply(group_fuel_types) ==\"ELECTRIC\") & (df[\"Fuel consumption \"].isna()),\"Fuel consumption \"] = 0\n",
    "\n",
    "    #  z (Wh/km)\n",
    "    df.loc[~(df[\"Ft\"].apply(group_fuel_types).isin([\"ELECTRIC\", \"HYBRID\"])) & (df[\"z (Wh/km)\"].isna()),\"z (Wh/km)\"] = 0\n",
    "    pass\n",
    "\n",
    "recup_electric(data_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete columns\n",
    "\n",
    "Supprimer les colonnes avec 1 seul valeur unique (aucune info) ou 0 valeur unique (que des NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colonne supprimée: MMS\n",
      "colonne supprimée: r\n",
      "colonne supprimée: Ernedc (g/km)\n",
      "colonne supprimée: De\n",
      "colonne supprimée: Vf\n",
      "colonne supprimée: Status\n"
     ]
    }
   ],
   "source": [
    "valeurs_uniques = {}\n",
    "nombre_val_unique={}\n",
    "for col in data_train.columns:\n",
    "    valeurs_uniques[col]=data_train[col].unique().tolist()\n",
    "    nombre_val_unique[col]=data_train[col].nunique()\n",
    "\n",
    "for element in nombre_val_unique:\n",
    "    if nombre_val_unique[element]<=1:\n",
    "        print(f\"colonne supprimée: {element}\")\n",
    "        data_train.drop(columns=element, inplace=True)\n",
    "        data_test.drop(columns=element, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supprimer les colonnes avec **+ de 50%** de NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colonne supprimée: Enedc (g/km)\n",
      "colonne supprimée: z (Wh/km)\n",
      "colonne supprimée: Electric range (km)\n"
     ]
    }
   ],
   "source": [
    "for col in data_train.columns:\n",
    "    if (data_train[col].isna().sum()/data_train.shape[0] > 0.5):\n",
    "        print(f\"colonne supprimée: {col}\")\n",
    "        data_train.drop(columns=col, inplace=True)\n",
    "        data_test.drop(columns=col, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supprimer les `Date` et `ID` (seulement pour train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colonne supprimée pour data_train: Date of registration, ID\n",
      "colonne supprimée pour data_test: Date of registration\n"
     ]
    }
   ],
   "source": [
    "data_train.drop(columns=['Date of registration','ID'], inplace=True)\n",
    "data_test.drop(columns='Date of registration', inplace=True)\n",
    "\n",
    "print(f\"colonne supprimée pour data_train: Date of registration, ID\")\n",
    "print(f\"colonne supprimée pour data_test: Date of registration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_categoricals = data_test.select_dtypes(include=\"object\").columns.tolist()\n",
    "col_numericals = [col for col in data_test.columns if col not in col_categoricals]\n",
    "col_numericals.remove(\"ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer les doublons en moyenne de Y\n",
    "\n",
    "En considérant qu'on a drop: `Date of registration`,`ID` on va prendre la moyenne des Y pour lesquelles les caractéristiques sont des doublons.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1163819, 26)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colus=list(data_train.columns)\n",
    "colus.remove('Ewltp (g/km)')\n",
    "duplicates = data_train.drop(columns=\"Ewltp (g/km)\").duplicated(keep=False)\n",
    "\n",
    "mean_target = data_train[duplicates].groupby(colus)['Ewltp (g/km)'].mean()\n",
    "\n",
    "# Supprimer les lignes en double et réindexer le DataFrame\n",
    "data_train = pd.concat([data_train[~duplicates], mean_target.reset_index()], ignore_index=True)\n",
    "data_train.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers \n",
    "\n",
    "Utilisation de l'écart interquartile pour identifier les valeurs aberrantes.\n",
    "\n",
    "Imputation des outliers:\n",
    "\n",
    "Fixer les valeurs aberrantes à un certain pourcentage (par exemple, 5e et 95e percentiles).\n",
    "\n",
    "**on pourrait aussi tenter d'imputer par la médiane si cela n'aboutit pas** \n",
    "\n",
    "### windorization of outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles={}\n",
    "\n",
    "def winsorize_outliers(data, column_name, lower_percentile=5, upper_percentile=95,train=True):\n",
    "    \"\"\"\n",
    "    Detects and imputes outliers using winsorizing for a specific column in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - data: Pandas DataFrame, input data\n",
    "    - column_name: str, name of the column to be winsorized\n",
    "    - lower_percentile: int, lower percentile for winsorizing (default: 5)\n",
    "    - upper_percentile: int, upper percentile for winsorizing (default: 95)\n",
    "\n",
    "    Returns:\n",
    "    - winsorized_data: Pandas DataFrame, data with outliers winsorized for the specified column\n",
    "    \"\"\"\n",
    "\n",
    "    column_data = data[column_name]\n",
    "    if train:\n",
    "        quantiles[\"q1\"] = np.percentile(column_data, lower_percentile)\n",
    "        quantiles[\"q3\"] = np.percentile(column_data, upper_percentile)\n",
    "        iqr = quantiles[\"q3\"] - quantiles[\"q1\"]\n",
    "        quantiles[\"lower_bound\"] = quantiles[\"q1\"] - 1.5 * iqr\n",
    "        quantiles[\"upper_bound\"] = quantiles[\"q3\"] + 1.5 * iqr\n",
    "\n",
    "    data[column_name] = np.clip(column_data, quantiles[\"lower_bound\"], quantiles[\"upper_bound\"])\n",
    "\n",
    "    return data\n",
    "\n",
    "for col in col_numericals:\n",
    "    data_train=winsorize_outliers(data_train,col)\n",
    "    data_test =winsorize_outliers(data_test,col,train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Median imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles={}\n",
    "\n",
    "def replace_outliers_with_median(data, column_name,train=True):\n",
    "    \"\"\"\n",
    "    Detects and replaces outliers with the median for a specific column in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - data: Pandas DataFrame, input data\n",
    "    - column_name: str, name of the column to be processed\n",
    "    - train: bool\n",
    "\n",
    "    Returns:\n",
    "    - data_with_median: Pandas DataFrame, data with outliers replaced by the median for the specified column\n",
    "    \"\"\"\n",
    "\n",
    "    column_data = data[column_name]\n",
    "\n",
    "    if train:\n",
    "        q1 = np.percentile(column_data,25)\n",
    "        q3 = np.percentile(column_data, 75)\n",
    "        iqr = q3 - q1\n",
    "        quantiles[\"lower_bound\"] = q1 - 1.5 * iqr\n",
    "        quantiles[\"upper_bound\"] = q3 + 1.5 * iqr\n",
    "        quantiles[column_name]=column_data.median()\n",
    "\n",
    "\n",
    "    outliers_mask = (column_data < quantiles[\"lower_bound\"]) | (column_data > quantiles[\"upper_bound\"])\n",
    "\n",
    "    data.loc[outliers_mask, column_name] = quantiles[column_name]\n",
    "    return data\n",
    "\n",
    "\n",
    "for col in col_numericals:\n",
    "    data_train = replace_outliers_with_median(data_train,col)\n",
    "    data_train =replace_outliers_with_median(data_test,col,train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute NaN by median/mode\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputers={}\n",
    "_coefficient_variation= lambda series : series.std()/series.mean()\n",
    "\n",
    "def fill_missing_values(colname : str,data:pd.DataFrame) -> None:\n",
    "    \n",
    "    if data[colname].dtype in [\"float64\"]:\n",
    "        if _coefficient_variation(data[colname]) > 0.15 :\n",
    "            imputers[colname]=SimpleImputer(missing_values=np.nan,strategy=\"median\")\n",
    "        else:\n",
    "            imputers[colname]=SimpleImputer(missing_values=np.nan,strategy=\"mean\")\n",
    "    else:\n",
    "        imputers[colname]=SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "    imputers[colname].fit(data[colname].to_numpy().reshape(-1,1))\n",
    "    pass\n",
    "\n",
    "for col in data_test.columns[1:]:\n",
    "    fill_missing_values(col,data_train)\n",
    "    data_train[col]=pd.Series(imputers[col].transform(data_train[col].to_numpy().reshape(-1,1)).flatten())\n",
    "    data_test[col]=pd.Series(imputers[col].transform(data_test[col].to_numpy().reshape(-1,1)).flatten())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode categorical columns\n",
    "\n",
    "Many choices:\n",
    "- Label/Ordinal encoding\n",
    "- Target encoding\n",
    "- Impact encoding\n",
    "\n",
    "\n",
    "Label/Ordinal encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders={}\n",
    "def ordinal_encoder(colname:str,data:pd.DataFrame,train=True):\n",
    "    if train:\n",
    "        encoders[colname]=OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "        data[colname]=encoders[colname].fit_transform(data[[colname]])\n",
    "        pass\n",
    "    else:\n",
    "        data[colname]=encoders[colname].transform(data[[colname]])\n",
    "        pass\n",
    "\n",
    "for col in col_categoricals:\n",
    "    ordinal_encoder(col,data_train)\n",
    "    ordinal_encoder(col,data_test,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders={}\n",
    "def target_encoder(colname:str,data:pd.DataFrame,train=True):\n",
    "    if train:\n",
    "        encoders[colname]=TargetEncoder(target_type='continuous', smooth='auto',random_state=42)\n",
    "        data[colname]=encoders[colname].fit_transform(data[[colname]],data['Ewltp (g/km)'])\n",
    "        pass\n",
    "    else:\n",
    "        data[colname]=encoders[colname].transform(data[[colname]])\n",
    "        pass\n",
    "\n",
    "for col in col_categoricals:\n",
    "    target_encoder(col,data_train)\n",
    "    target_encoder(col,data_test,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impact Encoding\n",
    "\n",
    "Proposé par Sam B. J'ai utilisé Chat GPT pour l'implémenter honnêtement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train, test = train_test_split(data_train, test_size=0.33, random_state=42)\n",
    "\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X_train, y_train = train.drop(columns=[\"Ewltp (g/km)\"]), train[\"Ewltp (g/km)\"]\n",
    "X_test, y_test = test.drop(columns=[\"Ewltp (g/km)\"]), test[\"Ewltp (g/km)\"]\n",
    "\n",
    "encoders={}\n",
    "def impact_encoder(colname:str,data:pd.DataFrame,target=None,train=True):\n",
    "    if train:\n",
    "        encoders[colname]=LeaveOneOutEncoder(handle_unknown=\"value\")\n",
    "        data[colname]=encoders[colname].fit_transform(data[[colname]],target)\n",
    "        pass\n",
    "    else:\n",
    "        data[colname]=encoders[colname].transform(data[[colname]])\n",
    "        pass\n",
    "\n",
    "for col in col_categoricals:\n",
    "    impact_encoder(col,X_train,target=y_train)\n",
    "    impact_encoder(col,X_test,train=False)\n",
    "    impact_encoder(col,data_test,train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Doit-on utiliser ces variables ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conforme(df):\n",
    "    df['conforme'] = df['Tan'].isna()\n",
    "    df['conforme'] = df['conforme'].apply(lambda x: 1 if x==False else 0)\n",
    "    pass\n",
    "\n",
    "create_conforme(data_train)\n",
    "create_conforme(data_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_surface(obs):\n",
    "    max_largeur= max(obs['At1 (mm)'], obs['At2 (mm)'])\n",
    "    return obs['W (mm)']*obs['At1 (mm)'] if max_largeur == obs['At1 (mm)'] else obs['W (mm)'] * obs['At2 (mm)']\n",
    "\n",
    "def create_surface(df):\n",
    "    df['surface']= df.apply(compute_surface, axis=1)\n",
    "    pass\n",
    "\n",
    "create_surface(data_train)\n",
    "create_surface(data_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data_train, test_size=0.33, random_state=42)\n",
    "\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X_train, y_train = train.drop(columns=[\"Ewltp (g/km)\"]), train[\"Ewltp (g/km)\"]\n",
    "X_test, y_test = test.drop(columns=[\"Ewltp (g/km)\"]), test[\"Ewltp (g/km)\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest 1\n",
    "\n",
    "Use all columns except those dropped (MMS, r, Ernedc (g/km), De,Vf,StatusEnedc (g/km),z (Wh/km),Electric range (km),Date of registration). \n",
    "\n",
    "ordinal encoding. Strandard fill NaN. Random forest. No feature Engineering.\n",
    "\n",
    "Computation time: 41min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 2.8986731992878796\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "y_pred = random_forest.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[\"Ewltp (g/km)\"] = random_forest.predict(data_test.drop(columns='ID'))\n",
    "data_test[[\"ID\",\"Ewltp (g/km)\"]].to_csv(\"data/new_simple_rf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['random_forest_simple_model.joblib']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(random_forest, 'models/random_forest_simple_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest 2\n",
    "\n",
    "Use all columns except those dropped (MMS, r, Ernedc (g/km), De,Vf,StatusEnedc (g/km),z (Wh/km),Electric range (km),Date of registration). \n",
    "\n",
    "**target** encoding. Strandard fill NaN. Random forest. No feature Engineering.\n",
    "\n",
    "Computation time: 23min (Home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 2.986984486605863\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "y_pred = random_forest.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[\"Ewltp (g/km)\"] = random_forest.predict(data_test.drop(columns='ID'))\n",
    "data_test[[\"ID\",\"Ewltp (g/km)\"]].to_csv(\"results/new_target_rf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest 3\n",
    "\n",
    "Use all columns except those dropped (MMS, r, Ernedc (g/km), De,Vf,StatusEnedc (g/km),z (Wh/km),Electric range (km),Date of registration). \n",
    "\n",
    "**impact** encoding. Strandard fill NaN. Random forest. No feature Engineering.\n",
    "\n",
    "Computation time: 22 (Home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 26.539493069959914\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "y_pred = random_forest.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[\"Ewltp (g/km)\"] = random_forest.predict(data_test.drop(columns='ID'))\n",
    "data_test[[\"ID\",\"Ewltp (g/km)\"]].to_csv(\"results/new_impact_rf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest 5\n",
    "\n",
    "Use all columns except those dropped (MMS, r, Ernedc (g/km), De,Vf,StatusEnedc (g/km),z (Wh/km),Electric range (km),Date of registration). \n",
    "\n",
    "ordinal encoding. Strandard fill NaN. Random forest. No feature Engineering. **use mean of Y**\n",
    "\n",
    "Computation time: 1.33min (Home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 3.6239323258632914\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "y_pred = random_forest.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[\"Ewltp (g/km)\"] = random_forest.predict(data_test.drop(columns='ID'))\n",
    "data_test[[\"ID\",\"Ewltp (g/km)\"]].to_csv(\"results/new_mean_rf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest 4\n",
    "\n",
    "Use all columns except those dropped (MMS, r, Ernedc (g/km), De,Vf,StatusEnedc (g/km),z (Wh/km),Electric range (km),Date of registration). \n",
    "\n",
    "ordinal encoding. Strandard fill NaN. Random forest. No feature Engineering. **feature engineering variables (conformes, surface)**\n",
    "\n",
    "Computation time: 23min (Home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 2.8998117328283994\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "y_pred = random_forest.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[\"Ewltp (g/km)\"] = random_forest.predict(data_test.drop(columns='ID'))\n",
    "data_test[[\"ID\",\"Ewltp (g/km)\"]].to_csv(\"results/new_fe_rf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging 1\n",
    "Use all columns except those dropped (MMS, r, Ernedc (g/km), De,Vf,StatusEnedc (g/km),z (Wh/km),Electric range (km),Date of registration). \n",
    "\n",
    "ordinal encoding. Strandard fill NaN. Random forest. No feature Engineering. estimator = DecistionTreeRegressor\n",
    "\n",
    "Computation time: 2,14min (Home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Yoan Python\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:1261: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 2.929723205821804\n"
     ]
    }
   ],
   "source": [
    "# TODO: remplacer par RandomForestRegressor(), SVR()\n",
    "bag=BaggingRegressor(estimator=DecisionTreeRegressor(),n_jobs=-1,bootstrap=True,oob_score=True, random_state=42)\n",
    "\n",
    "bag.fit(X_train, y_train)\n",
    "\n",
    "y_pred = bag.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[\"Ewltp (g/km)\"] = bag.predict(data_test.drop(columns='ID'))\n",
    "data_test[[\"ID\",\"Ewltp (g/km)\"]].to_csv(\"results/new_tree_bagging.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_estimator': 'deprecated',\n",
       " 'bootstrap': True,\n",
       " 'bootstrap_features': False,\n",
       " 'estimator__ccp_alpha': 0.0,\n",
       " 'estimator__criterion': 'squared_error',\n",
       " 'estimator__max_depth': None,\n",
       " 'estimator__max_features': None,\n",
       " 'estimator__max_leaf_nodes': None,\n",
       " 'estimator__min_impurity_decrease': 0.0,\n",
       " 'estimator__min_samples_leaf': 1,\n",
       " 'estimator__min_samples_split': 2,\n",
       " 'estimator__min_weight_fraction_leaf': 0.0,\n",
       " 'estimator__random_state': None,\n",
       " 'estimator__splitter': 'best',\n",
       " 'estimator': DecisionTreeRegressor(),\n",
       " 'max_features': 1.0,\n",
       " 'max_samples': 1.0,\n",
       " 'n_estimators': 10,\n",
       " 'n_jobs': -1,\n",
       " 'oob_score': True,\n",
       " 'random_state': 42,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging 2\n",
    "Use all columns except those dropped (MMS, r, Ernedc (g/km), De,Vf,StatusEnedc (g/km),z (Wh/km),Electric range (km),Date of registration). \n",
    "\n",
    "ordinal encoding. Strandard fill NaN. Random forest. No feature Engineering. **n_estimator = 20**\n",
    "\n",
    "Computation time: 5min (Home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done   2 out of  20 | elapsed:  4.4min remaining: 39.8min\n",
      "[Parallel(n_jobs=20)]: Done   9 out of  20 | elapsed:  4.5min remaining:  5.5min\n",
      "[Parallel(n_jobs=20)]: Done  16 out of  20 | elapsed:  4.5min remaining:  1.1min\n",
      "[Parallel(n_jobs=20)]: Done  20 out of  20 | elapsed:  4.5min finished\n",
      "d:\\Yoan Python\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:1261: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\n",
      "[Parallel(n_jobs=20)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done   2 out of  20 | elapsed:    5.4s remaining:   49.1s\n",
      "[Parallel(n_jobs=20)]: Done   9 out of  20 | elapsed:    5.9s remaining:    7.2s\n",
      "[Parallel(n_jobs=20)]: Done  16 out of  20 | elapsed:    6.3s remaining:    1.5s\n",
      "[Parallel(n_jobs=20)]: Done  20 out of  20 | elapsed:    6.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 2.9137636972220546\n"
     ]
    }
   ],
   "source": [
    "bag=BaggingRegressor(estimator=DecisionTreeRegressor(),n_jobs=-1,bootstrap=True,oob_score=True, random_state=42,verbose=3,n_estimators=20)\n",
    "\n",
    "bag.fit(X_train, y_train)\n",
    "\n",
    "y_pred = bag.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done   2 out of  20 | elapsed:    1.8s remaining:   17.0s\n",
      "[Parallel(n_jobs=20)]: Done   9 out of  20 | elapsed:    2.6s remaining:    3.3s\n",
      "[Parallel(n_jobs=20)]: Done  16 out of  20 | elapsed:    3.1s remaining:    0.7s\n",
      "[Parallel(n_jobs=20)]: Done  20 out of  20 | elapsed:    3.3s finished\n"
     ]
    }
   ],
   "source": [
    "data_test[\"Ewltp (g/km)\"] = bag.predict(data_test.drop(columns='ID'))\n",
    "data_test[[\"ID\",\"Ewltp (g/km)\"]].to_csv(\"results/new_20_estim_bagging.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging 3\n",
    "Use all columns except those dropped (MMS, r, Ernedc (g/km), De,Vf,StatusEnedc (g/km),z (Wh/km),Electric range (km),Date of registration). \n",
    "\n",
    "ordinal encoding. Strandard fill NaN. Random forest. No feature Engineering. **n_estimator = 30**\n",
    "\n",
    "Computation time: 7min (Home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=24)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=24)]: Done   4 out of  24 | elapsed:  5.3min remaining: 26.7min\n",
      "[Parallel(n_jobs=24)]: Done  13 out of  24 | elapsed:  5.5min remaining:  4.7min\n",
      "[Parallel(n_jobs=24)]: Done  22 out of  24 | elapsed:  6.7min remaining:   36.2s\n",
      "[Parallel(n_jobs=24)]: Done  24 out of  24 | elapsed:  6.7min finished\n",
      "d:\\Yoan Python\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:1261: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\n",
      "[Parallel(n_jobs=24)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=24)]: Done   4 out of  24 | elapsed:    7.2s remaining:   36.2s\n",
      "[Parallel(n_jobs=24)]: Done  13 out of  24 | elapsed:    8.2s remaining:    6.9s\n",
      "[Parallel(n_jobs=24)]: Done  22 out of  24 | elapsed:    9.8s remaining:    0.8s\n",
      "[Parallel(n_jobs=24)]: Done  24 out of  24 | elapsed:    9.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 2.9082315288155627\n"
     ]
    }
   ],
   "source": [
    "bag=BaggingRegressor(estimator=DecisionTreeRegressor(),n_jobs=-1,bootstrap=True,oob_score=True, random_state=42,verbose=3,n_estimators= 30)\n",
    "\n",
    "bag.fit(X_train, y_train)\n",
    "\n",
    "y_pred = bag.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=24)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=24)]: Done   4 out of  24 | elapsed:    3.5s remaining:   18.1s\n",
      "[Parallel(n_jobs=24)]: Done  13 out of  24 | elapsed:    4.3s remaining:    3.6s\n",
      "[Parallel(n_jobs=24)]: Done  22 out of  24 | elapsed:    4.8s remaining:    0.3s\n",
      "[Parallel(n_jobs=24)]: Done  24 out of  24 | elapsed:    4.9s finished\n"
     ]
    }
   ],
   "source": [
    "data_test[\"Ewltp (g/km)\"] = bag.predict(data_test.drop(columns='ID'))\n",
    "data_test[[\"ID\",\"Ewltp (g/km)\"]].to_csv(\"results/new_30_estim_bagging.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging 3\n",
    "Use all columns except those dropped (MMS, r, Ernedc (g/km), De,Vf,StatusEnedc (g/km),z (Wh/km),Electric range (km),Date of registration). \n",
    "\n",
    "ordinal encoding. Strandard fill NaN. Random forest. No feature Engineering. **n_estimator = 50**\n",
    "\n",
    "Computation time: 13min (Home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=24)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=24)]: Done   4 out of  24 | elapsed: 10.8min remaining: 53.9min\n",
      "[Parallel(n_jobs=24)]: Done  13 out of  24 | elapsed: 11.0min remaining:  9.3min\n",
      "[Parallel(n_jobs=24)]: Done  22 out of  24 | elapsed: 11.2min remaining:  1.0min\n",
      "[Parallel(n_jobs=24)]: Done  24 out of  24 | elapsed: 11.8min finished\n",
      "[Parallel(n_jobs=24)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=24)]: Done   4 out of  24 | elapsed:   12.2s remaining:  1.0min\n",
      "[Parallel(n_jobs=24)]: Done  13 out of  24 | elapsed:   14.0s remaining:   11.9s\n",
      "[Parallel(n_jobs=24)]: Done  22 out of  24 | elapsed:   15.6s remaining:    1.3s\n",
      "[Parallel(n_jobs=24)]: Done  24 out of  24 | elapsed:   15.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 2.9028932743761215\n"
     ]
    }
   ],
   "source": [
    "bag=BaggingRegressor(estimator=DecisionTreeRegressor(),n_jobs=-1,bootstrap=True,oob_score=True, random_state=42,verbose=3,n_estimators= 50)\n",
    "\n",
    "bag.fit(X_train, y_train)\n",
    "\n",
    "y_pred = bag.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=24)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=24)]: Done   4 out of  24 | elapsed:    5.7s remaining:   28.8s\n",
      "[Parallel(n_jobs=24)]: Done  13 out of  24 | elapsed:    7.5s remaining:    6.4s\n",
      "[Parallel(n_jobs=24)]: Done  22 out of  24 | elapsed:    8.8s remaining:    0.7s\n",
      "[Parallel(n_jobs=24)]: Done  24 out of  24 | elapsed:    9.0s finished\n"
     ]
    }
   ],
   "source": [
    "data_test[\"Ewltp (g/km)\"] = bag.predict(data_test.drop(columns='ID'))\n",
    "data_test[[\"ID\",\"Ewltp (g/km)\"]].to_csv(\"results/new_50_estim_bagging.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging 4\n",
    "Use all columns except those dropped (MMS, r, Ernedc (g/km), De,Vf,StatusEnedc (g/km),z (Wh/km),Electric range (km),Date of registration). \n",
    "\n",
    "ordinal encoding. Strandard fill NaN. Random forest. No feature Engineering. **n_estimator = 10, bootstrap_features= True**\n",
    "\n",
    "Computation time: 2,15min (Home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done   3 out of  10 | elapsed:  1.7min remaining:  4.0min\n",
      "[Parallel(n_jobs=10)]: Done   7 out of  10 | elapsed:  1.8min remaining:   46.2s\n",
      "[Parallel(n_jobs=10)]: Done  10 out of  10 | elapsed:  2.0min finished\n",
      "d:\\Yoan Python\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:1261: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\n",
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done   3 out of  10 | elapsed:    2.9s remaining:    6.9s\n",
      "[Parallel(n_jobs=10)]: Done   7 out of  10 | elapsed:    3.2s remaining:    1.3s\n",
      "[Parallel(n_jobs=10)]: Done  10 out of  10 | elapsed:    3.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 2.94623303681266\n"
     ]
    }
   ],
   "source": [
    "bag=BaggingRegressor(estimator=DecisionTreeRegressor(),n_jobs=-1,bootstrap=True,oob_score=True, random_state=42,verbose=3,n_estimators= 10,bootstrap_features=True)\n",
    "\n",
    "bag.fit(X_train, y_train)\n",
    "\n",
    "y_pred = bag.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging 5\n",
    "Use all columns except those dropped (MMS, r, Ernedc (g/km), De,Vf,StatusEnedc (g/km),z (Wh/km),Electric range (km),Date of registration). \n",
    "\n",
    "ordinal encoding. Strandard fill NaN. Random forest. No feature Engineering. **n_estimator = 20, oob_score=False**\n",
    "\n",
    "Computation time: 4.35min (Home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done   2 out of  20 | elapsed:  4.4min remaining: 39.3min\n",
      "[Parallel(n_jobs=20)]: Done   9 out of  20 | elapsed:  4.4min remaining:  5.4min\n",
      "[Parallel(n_jobs=20)]: Done  16 out of  20 | elapsed:  4.4min remaining:  1.1min\n",
      "[Parallel(n_jobs=20)]: Done  20 out of  20 | elapsed:  4.5min finished\n",
      "[Parallel(n_jobs=20)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done   2 out of  20 | elapsed:    5.5s remaining:   50.5s\n",
      "[Parallel(n_jobs=20)]: Done   9 out of  20 | elapsed:    5.9s remaining:    7.2s\n",
      "[Parallel(n_jobs=20)]: Done  16 out of  20 | elapsed:    6.4s remaining:    1.5s\n",
      "[Parallel(n_jobs=20)]: Done  20 out of  20 | elapsed:    6.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 2.9137636972220546\n"
     ]
    }
   ],
   "source": [
    "bag=BaggingRegressor(estimator=DecisionTreeRegressor(),n_jobs=-1,bootstrap=True,oob_score=False, random_state=42,verbose=3,n_estimators= 20)\n",
    "\n",
    "bag.fit(X_train, y_train)\n",
    "\n",
    "y_pred = bag.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 56.538170887529546\n"
     ]
    }
   ],
   "source": [
    "boosting = AdaBoostRegressor(random_state=42,loss='exponential')\n",
    "\n",
    "boosting.fit(X_train, y_train)\n",
    "\n",
    "y_pred = boosting.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[\"Ewltp (g/km)\"] = bag.predict(data_test.drop(columns='ID'))\n",
    "data_test[[\"ID\",\"Ewltp (g/km)\"]].to_csv(\"results/new_simple_boosting.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_model = DecisionTreeRegressor()\n",
    "\n",
    "param_grid = {\n",
    "    'criterion': ['absolute_error', 'poisson', 'squared_error'],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]}\n",
    "\n",
    "grid_search = GridSearchCV(decision_tree_model, param_grid, cv=5, scoring='neg_mean_absolute_error',verbose=3)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Meilleurs hyperparamètres:\", grid_search.best_params_)\n",
    "\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error sur les données de test:\", mae)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baggin Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_params = {\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    \n",
    "}\n",
    "\n",
    "# Définir les hyperparamètres spécifiques au Bagging\n",
    "bagging_params = {\n",
    "    'n_estimators': [20,50,100],\n",
    "    'max_samples': [1.0, 0.8, 0.9],  # La fraction d'échantillons à utiliser pour chaque sac\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Créer un modèle DecisionTreeRegressor pour être utilisé comme modèle de base\n",
    "base_model = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# Créer un modèle BaggingRegressor\n",
    "bagging_model = BaggingRegressor(base_model, random_state=42)\n",
    "\n",
    "# Combiner les deux ensembles de paramètres\n",
    "param_grid = {**base_model_params, **bagging_params}\n",
    "\n",
    "# Utiliser GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=bagging_model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_absolute_error',  # Utilisez 'neg_mean_absolute_error' pour la régression\n",
    "    cv=3,\n",
    "    verbose=3)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Meilleurs paramètres : \", grid_search.best_params_)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE) sur les données de test : \", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xgboost 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catboost 1\n",
    "\n",
    "computing time 49min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 166.0466744\ttest: 166.0927154\tbest: 166.0927154 (0)\ttotal: 329ms\tremaining: 54m 52s\n",
      "100:\tlearn: 19.9021623\ttest: 19.8023282\tbest: 19.8023282 (100)\ttotal: 30.2s\tremaining: 49m 16s\n",
      "200:\tlearn: 16.3895228\ttest: 16.3450710\tbest: 16.3450710 (200)\ttotal: 60s\tremaining: 48m 44s\n",
      "300:\tlearn: 14.6005017\ttest: 14.5744841\tbest: 14.5744841 (300)\ttotal: 1m 29s\tremaining: 48m 6s\n",
      "400:\tlearn: 13.3583934\ttest: 13.3527044\tbest: 13.3527044 (400)\ttotal: 1m 58s\tremaining: 47m 28s\n",
      "500:\tlearn: 12.4735322\ttest: 12.4949177\tbest: 12.4949177 (500)\ttotal: 2m 28s\tremaining: 46m 51s\n",
      "600:\tlearn: 11.8248216\ttest: 11.8637427\tbest: 11.8637427 (600)\ttotal: 2m 57s\tremaining: 46m 19s\n",
      "700:\tlearn: 11.3208447\ttest: 11.3706236\tbest: 11.3706236 (700)\ttotal: 3m 27s\tremaining: 45m 52s\n",
      "800:\tlearn: 10.8895149\ttest: 10.9501880\tbest: 10.9501880 (800)\ttotal: 3m 57s\tremaining: 45m 23s\n",
      "900:\tlearn: 10.5598394\ttest: 10.6378579\tbest: 10.6378579 (900)\ttotal: 4m 26s\tremaining: 44m 53s\n",
      "1000:\tlearn: 10.2236002\ttest: 10.3213196\tbest: 10.3213196 (1000)\ttotal: 4m 56s\tremaining: 44m 26s\n",
      "1100:\tlearn: 9.9249856\ttest: 10.0413769\tbest: 10.0413769 (1100)\ttotal: 5m 26s\tremaining: 43m 58s\n",
      "1200:\tlearn: 9.6838789\ttest: 9.8144088\tbest: 9.8144088 (1200)\ttotal: 5m 55s\tremaining: 43m 25s\n",
      "1300:\tlearn: 9.4749389\ttest: 9.6182998\tbest: 9.6182998 (1300)\ttotal: 6m 25s\tremaining: 42m 57s\n",
      "1400:\tlearn: 9.2613355\ttest: 9.4174017\tbest: 9.4174017 (1400)\ttotal: 6m 54s\tremaining: 42m 26s\n",
      "1500:\tlearn: 9.0757878\ttest: 9.2448582\tbest: 9.2448582 (1500)\ttotal: 7m 23s\tremaining: 41m 52s\n",
      "1600:\tlearn: 8.9131492\ttest: 9.0879916\tbest: 9.0879916 (1600)\ttotal: 7m 53s\tremaining: 41m 23s\n",
      "1700:\tlearn: 8.7650663\ttest: 8.9500690\tbest: 8.9500690 (1700)\ttotal: 8m 23s\tremaining: 40m 55s\n",
      "1800:\tlearn: 8.6260620\ttest: 8.8202797\tbest: 8.8202797 (1800)\ttotal: 8m 53s\tremaining: 40m 26s\n",
      "1900:\tlearn: 8.4965863\ttest: 8.6985953\tbest: 8.6985953 (1900)\ttotal: 9m 22s\tremaining: 39m 57s\n",
      "2000:\tlearn: 8.3863796\ttest: 8.5970480\tbest: 8.5970480 (2000)\ttotal: 9m 52s\tremaining: 39m 26s\n",
      "2100:\tlearn: 8.2697010\ttest: 8.4941648\tbest: 8.4941648 (2100)\ttotal: 10m 21s\tremaining: 38m 56s\n",
      "2200:\tlearn: 8.1624750\ttest: 8.3996657\tbest: 8.3996657 (2200)\ttotal: 10m 50s\tremaining: 38m 26s\n",
      "2300:\tlearn: 8.0658959\ttest: 8.3107307\tbest: 8.3107307 (2300)\ttotal: 11m 20s\tremaining: 37m 56s\n",
      "2400:\tlearn: 7.9717512\ttest: 8.2257280\tbest: 8.2257280 (2400)\ttotal: 11m 49s\tremaining: 37m 26s\n",
      "2500:\tlearn: 7.8757053\ttest: 8.1407682\tbest: 8.1407682 (2500)\ttotal: 12m 19s\tremaining: 36m 56s\n",
      "2600:\tlearn: 7.7969377\ttest: 8.0707225\tbest: 8.0707225 (2600)\ttotal: 12m 49s\tremaining: 36m 28s\n",
      "2700:\tlearn: 7.7114584\ttest: 7.9919318\tbest: 7.9919318 (2700)\ttotal: 13m 18s\tremaining: 35m 58s\n",
      "2800:\tlearn: 7.6438296\ttest: 7.9369904\tbest: 7.9369904 (2800)\ttotal: 13m 47s\tremaining: 35m 28s\n",
      "2900:\tlearn: 7.5751619\ttest: 7.8776189\tbest: 7.8776189 (2900)\ttotal: 14m 17s\tremaining: 34m 59s\n",
      "3000:\tlearn: 7.4986916\ttest: 7.8120378\tbest: 7.8120378 (3000)\ttotal: 14m 47s\tremaining: 34m 30s\n",
      "3100:\tlearn: 7.4385451\ttest: 7.7604479\tbest: 7.7604479 (3100)\ttotal: 15m 17s\tremaining: 34m\n",
      "3200:\tlearn: 7.3776815\ttest: 7.7099695\tbest: 7.7099695 (3200)\ttotal: 15m 46s\tremaining: 33m 30s\n",
      "3300:\tlearn: 7.3238428\ttest: 7.6651640\tbest: 7.6651640 (3300)\ttotal: 16m 16s\tremaining: 33m 1s\n",
      "3400:\tlearn: 7.2615474\ttest: 7.6121512\tbest: 7.6121512 (3400)\ttotal: 16m 46s\tremaining: 32m 32s\n",
      "3500:\tlearn: 7.2077850\ttest: 7.5640114\tbest: 7.5640114 (3500)\ttotal: 17m 15s\tremaining: 32m 1s\n",
      "3600:\tlearn: 7.1557560\ttest: 7.5202177\tbest: 7.5202177 (3600)\ttotal: 17m 44s\tremaining: 31m 31s\n",
      "3700:\tlearn: 7.1063249\ttest: 7.4781872\tbest: 7.4781872 (3700)\ttotal: 18m 13s\tremaining: 31m 1s\n",
      "3800:\tlearn: 7.0540243\ttest: 7.4364803\tbest: 7.4364803 (3800)\ttotal: 18m 43s\tremaining: 30m 31s\n",
      "3900:\tlearn: 7.0081334\ttest: 7.3969569\tbest: 7.3969569 (3900)\ttotal: 19m 12s\tremaining: 30m 2s\n",
      "4000:\tlearn: 6.9638378\ttest: 7.3594711\tbest: 7.3594711 (4000)\ttotal: 19m 42s\tremaining: 29m 32s\n",
      "4100:\tlearn: 6.9216268\ttest: 7.3253296\tbest: 7.3253296 (4100)\ttotal: 20m 11s\tremaining: 29m 2s\n",
      "4200:\tlearn: 6.8802454\ttest: 7.2884633\tbest: 7.2884633 (4200)\ttotal: 20m 40s\tremaining: 28m 32s\n",
      "4300:\tlearn: 6.8382012\ttest: 7.2529692\tbest: 7.2529692 (4300)\ttotal: 21m 10s\tremaining: 28m 2s\n",
      "4400:\tlearn: 6.8043102\ttest: 7.2244396\tbest: 7.2244396 (4400)\ttotal: 21m 39s\tremaining: 27m 33s\n",
      "4500:\tlearn: 6.7700650\ttest: 7.1967552\tbest: 7.1967552 (4500)\ttotal: 22m 9s\tremaining: 27m 3s\n",
      "4600:\tlearn: 6.7367701\ttest: 7.1690013\tbest: 7.1690013 (4600)\ttotal: 22m 39s\tremaining: 26m 34s\n",
      "4700:\tlearn: 6.7019675\ttest: 7.1432553\tbest: 7.1432553 (4700)\ttotal: 23m 8s\tremaining: 26m 5s\n",
      "4800:\tlearn: 6.6700677\ttest: 7.1182887\tbest: 7.1182887 (4800)\ttotal: 23m 37s\tremaining: 25m 35s\n",
      "4900:\tlearn: 6.6384356\ttest: 7.0911952\tbest: 7.0911952 (4900)\ttotal: 24m 7s\tremaining: 25m 5s\n",
      "5000:\tlearn: 6.6072880\ttest: 7.0666014\tbest: 7.0666014 (5000)\ttotal: 24m 36s\tremaining: 24m 35s\n",
      "5100:\tlearn: 6.5731033\ttest: 7.0388711\tbest: 7.0388711 (5100)\ttotal: 25m 5s\tremaining: 24m 6s\n",
      "5200:\tlearn: 6.5401495\ttest: 7.0123981\tbest: 7.0123981 (5200)\ttotal: 25m 35s\tremaining: 23m 36s\n",
      "5300:\tlearn: 6.5139913\ttest: 6.9906888\tbest: 6.9906888 (5300)\ttotal: 26m 4s\tremaining: 23m 6s\n",
      "5400:\tlearn: 6.4854360\ttest: 6.9682795\tbest: 6.9682795 (5400)\ttotal: 26m 34s\tremaining: 22m 37s\n",
      "5500:\tlearn: 6.4574234\ttest: 6.9464700\tbest: 6.9464700 (5500)\ttotal: 27m 3s\tremaining: 22m 7s\n",
      "5600:\tlearn: 6.4316316\ttest: 6.9256022\tbest: 6.9256022 (5600)\ttotal: 27m 32s\tremaining: 21m 38s\n",
      "5700:\tlearn: 6.4035248\ttest: 6.9033371\tbest: 6.9033371 (5700)\ttotal: 28m 2s\tremaining: 21m 8s\n",
      "5800:\tlearn: 6.3787717\ttest: 6.8831965\tbest: 6.8831965 (5800)\ttotal: 28m 31s\tremaining: 20m 39s\n",
      "5900:\tlearn: 6.3527250\ttest: 6.8633130\tbest: 6.8633130 (5900)\ttotal: 29m 1s\tremaining: 20m 9s\n",
      "6000:\tlearn: 6.3266736\ttest: 6.8448102\tbest: 6.8448102 (6000)\ttotal: 29m 31s\tremaining: 19m 40s\n",
      "6100:\tlearn: 6.3013590\ttest: 6.8236507\tbest: 6.8236507 (6100)\ttotal: 30m\tremaining: 19m 10s\n",
      "6200:\tlearn: 6.2738316\ttest: 6.8003132\tbest: 6.8003132 (6200)\ttotal: 30m 29s\tremaining: 18m 41s\n",
      "6300:\tlearn: 6.2524299\ttest: 6.7852274\tbest: 6.7852274 (6300)\ttotal: 30m 59s\tremaining: 18m 11s\n",
      "6400:\tlearn: 6.2283233\ttest: 6.7662723\tbest: 6.7662723 (6400)\ttotal: 31m 28s\tremaining: 17m 41s\n",
      "6500:\tlearn: 6.2052207\ttest: 6.7472313\tbest: 6.7472313 (6500)\ttotal: 31m 57s\tremaining: 17m 12s\n",
      "6600:\tlearn: 6.1855662\ttest: 6.7326308\tbest: 6.7326308 (6600)\ttotal: 32m 27s\tremaining: 16m 42s\n",
      "6700:\tlearn: 6.1644274\ttest: 6.7168549\tbest: 6.7168549 (6700)\ttotal: 32m 56s\tremaining: 16m 13s\n",
      "6800:\tlearn: 6.1468624\ttest: 6.7047782\tbest: 6.7047782 (6800)\ttotal: 33m 26s\tremaining: 15m 43s\n",
      "6900:\tlearn: 6.1264562\ttest: 6.6910957\tbest: 6.6910957 (6900)\ttotal: 33m 55s\tremaining: 15m 13s\n",
      "7000:\tlearn: 6.1071969\ttest: 6.6769634\tbest: 6.6769634 (7000)\ttotal: 34m 24s\tremaining: 14m 44s\n",
      "7100:\tlearn: 6.0889251\ttest: 6.6639591\tbest: 6.6639591 (7100)\ttotal: 34m 53s\tremaining: 14m 14s\n",
      "7200:\tlearn: 6.0711890\ttest: 6.6499598\tbest: 6.6499598 (7200)\ttotal: 35m 22s\tremaining: 13m 45s\n",
      "7300:\tlearn: 6.0523024\ttest: 6.6365681\tbest: 6.6365681 (7300)\ttotal: 35m 52s\tremaining: 13m 15s\n",
      "7400:\tlearn: 6.0336699\ttest: 6.6231073\tbest: 6.6228892 (7398)\ttotal: 36m 21s\tremaining: 12m 46s\n",
      "7500:\tlearn: 6.0162877\ttest: 6.6109212\tbest: 6.6109212 (7500)\ttotal: 36m 51s\tremaining: 12m 16s\n",
      "7600:\tlearn: 5.9966628\ttest: 6.5943113\tbest: 6.5943113 (7600)\ttotal: 37m 22s\tremaining: 11m 47s\n",
      "7700:\tlearn: 5.9772625\ttest: 6.5793735\tbest: 6.5793735 (7700)\ttotal: 37m 52s\tremaining: 11m 18s\n",
      "7800:\tlearn: 5.9601471\ttest: 6.5666977\tbest: 6.5666977 (7800)\ttotal: 38m 22s\tremaining: 10m 48s\n",
      "7900:\tlearn: 5.9452964\ttest: 6.5565818\tbest: 6.5565818 (7900)\ttotal: 38m 52s\tremaining: 10m 19s\n",
      "8000:\tlearn: 5.9273913\ttest: 6.5422218\tbest: 6.5422218 (8000)\ttotal: 39m 22s\tremaining: 9m 50s\n",
      "8100:\tlearn: 5.9110707\ttest: 6.5291251\tbest: 6.5288533 (8099)\ttotal: 39m 52s\tremaining: 9m 20s\n",
      "8200:\tlearn: 5.8947185\ttest: 6.5158430\tbest: 6.5158430 (8200)\ttotal: 40m 22s\tremaining: 8m 51s\n",
      "8300:\tlearn: 5.8785198\ttest: 6.5027082\tbest: 6.5027082 (8300)\ttotal: 40m 51s\tremaining: 8m 21s\n",
      "8400:\tlearn: 5.8645655\ttest: 6.4922696\tbest: 6.4922696 (8400)\ttotal: 41m 21s\tremaining: 7m 52s\n",
      "8500:\tlearn: 5.8495689\ttest: 6.4811648\tbest: 6.4811648 (8500)\ttotal: 41m 51s\tremaining: 7m 22s\n",
      "8600:\tlearn: 5.8331511\ttest: 6.4698149\tbest: 6.4698149 (8600)\ttotal: 42m 21s\tremaining: 6m 53s\n",
      "8700:\tlearn: 5.8196580\ttest: 6.4605392\tbest: 6.4605392 (8700)\ttotal: 42m 50s\tremaining: 6m 23s\n",
      "8800:\tlearn: 5.8074554\ttest: 6.4519562\tbest: 6.4519562 (8800)\ttotal: 43m 20s\tremaining: 5m 54s\n",
      "8900:\tlearn: 5.7943792\ttest: 6.4419928\tbest: 6.4419928 (8900)\ttotal: 43m 49s\tremaining: 5m 24s\n",
      "9000:\tlearn: 5.7774505\ttest: 6.4292843\tbest: 6.4292843 (9000)\ttotal: 44m 19s\tremaining: 4m 55s\n",
      "9100:\tlearn: 5.7634473\ttest: 6.4205080\tbest: 6.4205080 (9100)\ttotal: 44m 49s\tremaining: 4m 25s\n",
      "9200:\tlearn: 5.7526009\ttest: 6.4119830\tbest: 6.4119830 (9200)\ttotal: 45m 19s\tremaining: 3m 56s\n",
      "9300:\tlearn: 5.7406910\ttest: 6.4037244\tbest: 6.4037244 (9300)\ttotal: 45m 48s\tremaining: 3m 26s\n",
      "9400:\tlearn: 5.7258422\ttest: 6.3921093\tbest: 6.3921093 (9400)\ttotal: 46m 18s\tremaining: 2m 57s\n",
      "9500:\tlearn: 5.7115733\ttest: 6.3802132\tbest: 6.3802132 (9500)\ttotal: 46m 47s\tremaining: 2m 27s\n",
      "9600:\tlearn: 5.6982262\ttest: 6.3710968\tbest: 6.3710968 (9600)\ttotal: 47m 17s\tremaining: 1m 57s\n",
      "9700:\tlearn: 5.6866344\ttest: 6.3621594\tbest: 6.3621594 (9700)\ttotal: 47m 46s\tremaining: 1m 28s\n",
      "9800:\tlearn: 5.6742874\ttest: 6.3529217\tbest: 6.3529217 (9800)\ttotal: 48m 16s\tremaining: 58.8s\n",
      "9900:\tlearn: 5.6607105\ttest: 6.3442332\tbest: 6.3442332 (9900)\ttotal: 48m 45s\tremaining: 29.3s\n",
      "9999:\tlearn: 5.6496027\ttest: 6.3367792\tbest: 6.3367792 (9999)\ttotal: 49m 15s\tremaining: 0us\n",
      "\n",
      "bestTest = 6.336779233\n",
      "bestIteration = 9999\n",
      "\n",
      "Mean Absolute Error (MAE): 3.5228090019627323\n"
     ]
    }
   ],
   "source": [
    "\n",
    "catb = CatBoostRegressor(iterations=10000,  # Nombre d'itérations (peut être ajusté)\n",
    "                          depth=6,  # Profondeur de l'arbre (peut être ajusté)\n",
    "                          learning_rate=0.1,  # Taux d'apprentissage (peut être ajusté)\n",
    "                          loss_function='RMSE',  # Fonction de perte pour la régression\n",
    "                          random_seed=42,\n",
    "                          task_type='CPU',\n",
    "                          thread_count=-1)\n",
    "\n",
    "catb.fit(X_train, y_train, eval_set=(X_test, y_test), early_stopping_rounds=50, verbose=100)\n",
    "\n",
    "predictions = catb.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[\"Ewltp (g/km)\"] = catb.predict(data_test.drop(columns='ID'))\n",
    "data_test[[\"ID\",\"Ewltp (g/km)\"]].to_csv(\"results/new_simple_catboosting.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catboost 2\n",
    "\n",
    "https://towardsdatascience.com/catboost-regression-in-6-minutes-3487f3e5b329\n",
    "\n",
    "### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_train.drop(columns=\"Ewltp (g/km)\"), data_train[\"Ewltp (g/km)\"], test_size = 0.2, random_state=42)\n",
    "\n",
    "train_dataset = Pool(X_train, y_train) \n",
    "test_dataset = Pool(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "catb = CatBoostRegressor(iterations=10000,  # Nombre d'itérations (peut être ajusté)\n",
    "                          depth=6,  # Profondeur de l'arbre (peut être ajusté)\n",
    "                          learning_rate=0.1,  # Taux d'apprentissage (peut être ajusté)\n",
    "                          loss_function='RMSE',  # Fonction de perte pour la régression\n",
    "                          random_seed=42,\n",
    "                          task_type='CPU',\n",
    "                          thread_count=-1)\n",
    "\n",
    "grid = {'iterations': [100, 150, 200],\n",
    "        'learning_rate': [0.03, 0.1],\n",
    "        'depth': [2, 4, 6, 8],\n",
    "        'l2_leaf_reg': [0.2, 0.5, 1, 3]}\n",
    "\n",
    "catb.grid_search(grid, train_dataset)\n",
    "\n",
    "predictions = catb.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_mining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
