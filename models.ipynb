{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.preprocessing\n",
    "from src.preprocessing import Dataset,Preprocessor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from xgboost.callback import EarlyStopping, LearningRateScheduler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement de la donnée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=Dataset(\"data/train.csv\")\n",
    "data_train=train.load_data()\n",
    "test=Dataset(\"data/test.csv\")\n",
    "data_test=test.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing de la donnée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preprocessor=Preprocessor(data_train,train=True)\n",
    "test_preprocessor=Preprocessor(data_test,train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preprocessor.recup_electric()\n",
    "test_preprocessor.recup_electric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Vf', 'Ernedc (g/km)', 'MMS', 'De', 'Enedc (g/km)', 'Status', 'r'] have been deleted on train data\n",
      "['Vf', 'Ernedc (g/km)', 'MMS', 'De', 'Enedc (g/km)', 'Status', 'r'] have been deleted on test data\n"
     ]
    }
   ],
   "source": [
    "train_preprocessor.delete_useless_columns()\n",
    "test_preprocessor.delete_useless_columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols=data_train.select_dtypes(include='number').columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols.remove('ID')\n",
    "numerical_cols.remove('Ewltp (g/km)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in numerical_cols:\n",
    "    train_preprocessor.winsorize_outliers(col)\n",
    "    test_preprocessor.winsorize_outliers(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_variables=list(filter(lambda x : x not in ['ID','Ewltp (g/km)','Date of registration'],data_train.columns.tolist())) #,'Erwltp (g/km)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in x_variables:\n",
    "    train_preprocessor.fill_missing_values(col)\n",
    "    test_preprocessor.fill_missing_values(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_categoricals=list(filter(lambda x: x not in numerical_cols,x_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding count : Country\n",
      "encoding count : VFN\n",
      "encoding OHE : Mp\n",
      "encoding count : Mh\n",
      "encoding count : Man\n",
      "encoding count : Tan\n",
      "encoding count : T\n",
      "encoding count : Va\n",
      "encoding count : Ve\n",
      "encoding count : Mk\n",
      "encoding count : Cn\n",
      "encoding OHE : Ct\n",
      "encoding OHE : Cr\n",
      "encoding OHE : Ft\n",
      "encoding OHE : Fm\n",
      "encoding count : IT\n"
     ]
    }
   ],
   "source": [
    "for col in col_categoricals:\n",
    "    if Preprocessor.nombre_val_unique[col]>=15: #eventually replace by catboost encoder but careful cuz of Target ! (do TTS first)\n",
    "        train_preprocessor.count_encoder(col)\n",
    "        test_preprocessor.count_encoder(col)\n",
    "        print(f\"encoding count : {col}\")\n",
    "    else:\n",
    "        train_preprocessor.ohe_encoder(col)\n",
    "        test_preprocessor.ohe_encoder(col)\n",
    "        print(f\"encoding OHE : {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.drop(columns=['Date of registration','ID','Erwltp (g/km)'], inplace=True)#\n",
    "data_test.drop(columns=['Date of registration','Erwltp (g/km)'], inplace=True)#,'Erwltp (g/km)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols.remove('Erwltp (g/km)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jeann\\AppData\\Local\\Temp\\ipykernel_1348\\3387644149.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_train[poly_columns]=poly.transform(data_train[numerical_cols])\n",
      "C:\\Users\\jeann\\AppData\\Local\\Temp\\ipykernel_1348\\3387644149.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_train[poly_columns]=poly.transform(data_train[numerical_cols])\n",
      "C:\\Users\\jeann\\AppData\\Local\\Temp\\ipykernel_1348\\3387644149.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_train[poly_columns]=poly.transform(data_train[numerical_cols])\n",
      "C:\\Users\\jeann\\AppData\\Local\\Temp\\ipykernel_1348\\3387644149.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_train[poly_columns]=poly.transform(data_train[numerical_cols])\n",
      "C:\\Users\\jeann\\AppData\\Local\\Temp\\ipykernel_1348\\3387644149.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_train[poly_columns]=poly.transform(data_train[numerical_cols])\n",
      "C:\\Users\\jeann\\AppData\\Local\\Temp\\ipykernel_1348\\3387644149.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_train[poly_columns]=poly.transform(data_train[numerical_cols])\n",
      "C:\\Users\\jeann\\AppData\\Local\\Temp\\ipykernel_1348\\3387644149.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_train[poly_columns]=poly.transform(data_train[numerical_cols])\n",
      "C:\\Users\\jeann\\AppData\\Local\\Temp\\ipykernel_1348\\3387644149.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_test[poly_columns]=poly.transform(data_test[numerical_cols])\n",
      "C:\\Users\\jeann\\AppData\\Local\\Temp\\ipykernel_1348\\3387644149.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_test[poly_columns]=poly.transform(data_test[numerical_cols])\n",
      "C:\\Users\\jeann\\AppData\\Local\\Temp\\ipykernel_1348\\3387644149.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_test[poly_columns]=poly.transform(data_test[numerical_cols])\n",
      "C:\\Users\\jeann\\AppData\\Local\\Temp\\ipykernel_1348\\3387644149.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_test[poly_columns]=poly.transform(data_test[numerical_cols])\n",
      "C:\\Users\\jeann\\AppData\\Local\\Temp\\ipykernel_1348\\3387644149.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_test[poly_columns]=poly.transform(data_test[numerical_cols])\n",
      "C:\\Users\\jeann\\AppData\\Local\\Temp\\ipykernel_1348\\3387644149.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_test[poly_columns]=poly.transform(data_test[numerical_cols])\n",
      "C:\\Users\\jeann\\AppData\\Local\\Temp\\ipykernel_1348\\3387644149.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_test[poly_columns]=poly.transform(data_test[numerical_cols])\n"
     ]
    }
   ],
   "source": [
    "# poly = PolynomialFeatures(degree=2,include_bias=False)\n",
    "# poly.fit(data_train[numerical_cols])\n",
    "\n",
    "# poly_columns=poly.get_feature_names_out(numerical_cols)\n",
    "# data_train[poly_columns]=poly.transform(data_train[numerical_cols])\n",
    "# data_test[poly_columns]=poly.transform(data_test[numerical_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data_train, test_size=0.33, random_state=42)\n",
    "\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X_train, y_train = train.drop(columns=[\"Ewltp (g/km)\"]).to_numpy(), train[\"Ewltp (g/km)\"].to_numpy()\n",
    "X_test, y_test = test.drop(columns=[\"Ewltp (g/km)\"]).to_numpy(), test[\"Ewltp (g/km)\"].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modélisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'n_estimators':4000,\n",
    "          'max_depth': 35,\n",
    "          'learning_rate': 0.005,\n",
    "          'colsample_bytree':0.80,\n",
    "          'gamma':10,\n",
    "          'reg_alpha':0.8,\n",
    "          'reg_lambda':0.1,\n",
    "          'objective': 'reg:squarederror',\n",
    "          'tree_method': 'hist',\n",
    "          'n_jobs':-1,\n",
    "          'device':'cuda'\n",
    "}\n",
    "\n",
    "model = xgb.XGBRegressor(**params)\n",
    "\n",
    "model.fit(X_train, y_train,verbose=True)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Mean Absolute Error (MAE): 2.817709256862628 sans Erwltp\n",
    "\n",
    "Mean Absolute Error (MAE): 2.8182787570889554 avec Erwltp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_mining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
